{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-06T17:07:43.431752Z",
     "iopub.status.busy": "2025-09-06T17:07:43.431151Z",
     "iopub.status.idle": "2025-09-06T17:08:20.325366Z",
     "shell.execute_reply": "2025-09-06T17:08:20.324687Z",
     "shell.execute_reply.started": "2025-09-06T17:07:43.431707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.12.0)\n",
      "Requirement already satisfied: prophet in /usr/local/lib/python3.11/dist-packages (1.1.7)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (1.2.5)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from prophet) (3.7.2)\n",
      "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.2.3)\n",
      "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.11/dist-packages (from prophet) (0.75)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.11/dist-packages (from prophet) (4.67.1)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from prophet) (6.5.2)\n",
      "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (11.2.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (3.0.9)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:08:08.145285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757178488.517623 1029851 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757178488.624655 1029851 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Enhanced YouTube Trend Predictor with Ensemble Forecasting\n",
    "!pip install faiss-cpu prophet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "# NLP and ML libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Deep Learning for time series\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from prophet import Prophet\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic TrendAI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T17:08:20.327482Z",
     "iopub.status.busy": "2025-09-06T17:08:20.326970Z",
     "iopub.status.idle": "2025-09-06T17:08:20.493181Z",
     "shell.execute_reply": "2025-09-06T17:08:20.492646Z",
     "shell.execute_reply.started": "2025-09-06T17:08:20.327457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrendAI:\n",
    "    \"\"\"\n",
    "    A comprehensive trend prediction model for YouTube beauty industry data.\n",
    "    Enhanced to integrate video metrics and tags with higher weighting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the trend predictor with SBERT model and other components.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the SentenceTransformer model to use\n",
    "        \"\"\"\n",
    "        print(\"Initializing Enhanced YouTube Trend Predictor with Tags Integration...\")\n",
    "        self.sbert_model = SentenceTransformer(model_name)\n",
    "        self.kmeans_index = None  # FAISS K-means index\n",
    "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "        self.comments_df = None\n",
    "        self.videos_df = None\n",
    "        self.embeddings = None\n",
    "        self.clusters = None\n",
    "        self.trend_data = None\n",
    "        self.video_trend_data = None\n",
    "        self.combined_trend_data = None\n",
    "        self.lstm_model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Enhanced weighting factors including tags\n",
    "        self.video_weight = 0.6   # 60% weight for video metrics\n",
    "        self.comment_weight = 0.25  # 25% weight for comment metrics\n",
    "        self.tag_weight = 0.15    # 15% weight for tag relevance\n",
    "        \n",
    "        # Tag processing attributes\n",
    "        self.tag_trends = None\n",
    "        self.tag_clusters = None\n",
    "        self.popular_tags = None\n",
    "\n",
    "        self.generational_analyzer = EnhancedGenerationalLanguageAnalyzer\n",
    "        self.generational_clusters = None\n",
    "        self.generational_trends = None\n",
    "        \n",
    "        # Download required NLTK data\n",
    "        try:\n",
    "            nltk.data.find('vader_lexicon')\n",
    "        except LookupError:\n",
    "            nltk.download('vader_lexicon')\n",
    "\n",
    "    def load_data(self, comment_files: List[str], video_file: str) -> None:\n",
    "        \"\"\"\n",
    "        Load and combine comment and video data from CSV files.\n",
    "        \n",
    "        Args:\n",
    "            comment_files (List[str]): List of comment CSV file paths\n",
    "            video_file (str): Path to video CSV file\n",
    "        \"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        \n",
    "        # Load and combine comment files\n",
    "        comment_dfs = []\n",
    "        for file in comment_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                comment_dfs.append(df)\n",
    "                print(f\"Loaded {len(df)} comments from {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "        \n",
    "        if comment_dfs:\n",
    "            self.comments_df = pd.concat(comment_dfs, ignore_index=True)\n",
    "            print(f\"Total comments loaded: {len(self.comments_df)}\")\n",
    "        \n",
    "        # Load video data\n",
    "        try:\n",
    "            self.videos_df = pd.read_csv(video_file)\n",
    "            print(f\"Loaded {len(self.videos_df)} videos from {video_file}\")\n",
    "            \n",
    "            # Log video data structure for debugging\n",
    "            print(\"Video data columns:\", self.videos_df.columns.tolist())\n",
    "            print(\"Sample video data:\")\n",
    "            print(self.videos_df.head())\n",
    "            \n",
    "            # Check for tags column\n",
    "            if 'tags' in self.videos_df.columns:\n",
    "                print(\"Tags column found - will integrate tags into analysis\")\n",
    "                tag_sample = self.videos_df['tags'].dropna().head(3)\n",
    "                print(\"Sample tags:\", tag_sample.tolist())\n",
    "            else:\n",
    "                print(\"Warning: No 'tags' column found in video data\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading video file: {e}\")\n",
    "\n",
    "    def preprocess_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Clean and preprocess the loaded data including tag processing.\n",
    "        Enhanced to handle video tags properly.\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing data with tag integration...\")\n",
    "        \n",
    "        if self.comments_df is not None:\n",
    "            # Convert timestamp columns\n",
    "            self.comments_df['publishedAt'] = pd.to_datetime(self.comments_df['publishedAt'])\n",
    "            self.comments_df['updatedAt'] = pd.to_datetime(self.comments_df['updatedAt'])\n",
    "            \n",
    "            # Clean text data\n",
    "            self.comments_df['cleaned_text'] = self.comments_df['textOriginal'].apply(self._clean_text)\n",
    "            \n",
    "            # Remove empty or very short comments\n",
    "            self.comments_df = self.comments_df[\n",
    "                (self.comments_df['cleaned_text'].str.len() > 10) &\n",
    "                (self.comments_df['cleaned_text'].notna())\n",
    "            ].reset_index(drop=True)\n",
    "            \n",
    "            # Add time-based features\n",
    "            self.comments_df['date'] = self.comments_df['publishedAt'].dt.date\n",
    "            self.comments_df['hour'] = self.comments_df['publishedAt'].dt.hour\n",
    "            self.comments_df['day_of_week'] = self.comments_df['publishedAt'].dt.dayofweek\n",
    "            \n",
    "            print(f\"Comments after preprocessing: {len(self.comments_df)}\")\n",
    "        \n",
    "        if self.videos_df is not None:\n",
    "            self.videos_df['publishedAt'] = pd.to_datetime(self.videos_df['publishedAt'])\n",
    "            self.videos_df['date'] = self.videos_df['publishedAt'].dt.date\n",
    "            \n",
    "            # Clean video descriptions and titles\n",
    "            self.videos_df['cleaned_title'] = self.videos_df['title'].apply(self._clean_text)\n",
    "            self.videos_df['cleaned_description'] = self.videos_df['description'].apply(self._clean_text)\n",
    "            \n",
    "            # Enhanced tag processing\n",
    "            if 'tags' in self.videos_df.columns:\n",
    "                self.videos_df['processed_tags'] = self.videos_df['tags'].apply(self._process_tags)\n",
    "                self.videos_df['tag_count'] = self.videos_df['processed_tags'].apply(len)\n",
    "                self.videos_df['tag_text'] = self.videos_df['processed_tags'].apply(\n",
    "                    lambda x: ' '.join(x) if x else ''\n",
    "                )\n",
    "                \n",
    "                # Analyze tag popularity\n",
    "                self._analyze_tag_popularity()\n",
    "                \n",
    "                # Calculate tag relevance scores\n",
    "                self._calculate_tag_relevance_scores()\n",
    "            else:\n",
    "                self.videos_df['processed_tags'] = [[] for _ in range(len(self.videos_df))]\n",
    "                self.videos_df['tag_count'] = 0\n",
    "                self.videos_df['tag_text'] = ''\n",
    "                self.videos_df['tag_relevance_score'] = 0\n",
    "            \n",
    "            # Enhanced video metrics preprocessing\n",
    "            # Convert string numbers to integers if needed\n",
    "            numeric_columns = ['viewCount', 'likeCount', 'commentCount']\n",
    "            for col in numeric_columns:\n",
    "                if col in self.videos_df.columns:\n",
    "                    self.videos_df[col] = pd.to_numeric(self.videos_df[col], errors='coerce').fillna(0)\n",
    "            \n",
    "            # Calculate video performance metrics\n",
    "            self.videos_df['engagement_rate'] = (\n",
    "                (self.videos_df.get('likeCount', 0) + self.videos_df.get('commentCount', 0)) / \n",
    "                np.maximum(self.videos_df.get('viewCount', 1), 1)\n",
    "            )\n",
    "            \n",
    "            # Enhanced video trending score including tags\n",
    "            max_views = self.videos_df.get('viewCount', pd.Series([1])).max()\n",
    "            max_likes = self.videos_df.get('likeCount', pd.Series([1])).max()\n",
    "            \n",
    "            if max_views > 0 and max_likes > 0:\n",
    "                self.videos_df['view_score'] = self.videos_df.get('viewCount', 0) / max_views\n",
    "                self.videos_df['like_score'] = self.videos_df.get('likeCount', 0) / max_likes\n",
    "                \n",
    "                # Recency score (more recent videos get higher scores)\n",
    "                current_time = pd.Timestamp.now().tz_localize(None)\n",
    "                published_times = pd.to_datetime(self.videos_df['publishedAt']).dt.tz_localize(None)\n",
    "                days_since_publish = (current_time - published_times).dt.days\n",
    "                max_days = days_since_publish.max() if len(days_since_publish) > 0 else 1\n",
    "                self.videos_df['recency_score'] = 1 - (days_since_publish / max(max_days, 1))\n",
    "                \n",
    "                # Enhanced trending score including tag relevance\n",
    "                self.videos_df['trending_score'] = (\n",
    "                    0.35 * self.videos_df['view_score'] +\n",
    "                    0.35 * self.videos_df['like_score'] +\n",
    "                    0.15 * self.videos_df['recency_score'] +\n",
    "                    0.15 * self.videos_df.get('tag_relevance_score', 0)\n",
    "                )\n",
    "            else:\n",
    "                self.videos_df['trending_score'] = 0\n",
    "            \n",
    "            print(f\"Videos after preprocessing: {len(self.videos_df)}\")\n",
    "            print(f\"Video trending scores range: {self.videos_df['trending_score'].min():.3f} - {self.videos_df['trending_score'].max():.3f}\")\n",
    "            \n",
    "            if 'tags' in self.videos_df.columns:\n",
    "                print(f\"Average tags per video: {self.videos_df['tag_count'].mean():.1f}\")\n",
    "                print(f\"Videos with tags: {(self.videos_df['tag_count'] > 0).sum()}/{len(self.videos_df)}\")\n",
    "\n",
    "    def _process_tags(self, tags) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process and clean video tags.\n",
    "        \n",
    "        Args:\n",
    "            tags: Raw tags data (could be string, list, or None)\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: Processed list of tags\n",
    "        \"\"\"\n",
    "        if pd.isna(tags) or tags == '' or tags is None:\n",
    "            return []\n",
    "        \n",
    "        if isinstance(tags, str):\n",
    "            # Handle different tag formats\n",
    "            # Common separators: comma, semicolon, pipe\n",
    "            if ',' in tags:\n",
    "                tag_list = tags.split(',')\n",
    "            elif ';' in tags:\n",
    "                tag_list = tags.split(';')\n",
    "            elif '|' in tags:\n",
    "                tag_list = tags.split('|')\n",
    "            else:\n",
    "                # If no separator, treat as single tag or space-separated\n",
    "                tag_list = [tags] if ' ' not in tags else tags.split()\n",
    "        elif isinstance(tags, list):\n",
    "            tag_list = tags\n",
    "        else:\n",
    "            # Convert to string and process\n",
    "            tag_list = str(tags).split(',')\n",
    "        \n",
    "        # Clean and normalize tags\n",
    "        processed_tags = []\n",
    "        for tag in tag_list:\n",
    "            if isinstance(tag, str):\n",
    "                # Remove quotes and extra whitespace\n",
    "                clean_tag = tag.strip().strip('\"\\'').lower()\n",
    "                # Remove special characters but keep spaces and hyphens\n",
    "                clean_tag = re.sub(r'[^\\w\\s\\-]', '', clean_tag)\n",
    "                if clean_tag and len(clean_tag) > 1:  # Keep tags with more than 1 character\n",
    "                    processed_tags.append(clean_tag)\n",
    "        \n",
    "        return processed_tags\n",
    "\n",
    "    def _analyze_tag_popularity(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze tag popularity and trends over time.\n",
    "        \"\"\"\n",
    "        print(\"Analyzing tag popularity and trends...\")\n",
    "        \n",
    "        # Collect all tags with their video metadata\n",
    "        all_tags = []\n",
    "        for _, video in self.videos_df.iterrows():\n",
    "            for tag in video['processed_tags']:\n",
    "                all_tags.append({\n",
    "                    'tag': tag,\n",
    "                    'videoId': video['videoId'],\n",
    "                    'date': video['date'],\n",
    "                    'viewCount': video.get('viewCount', 0),\n",
    "                    'likeCount': video.get('likeCount', 0),\n",
    "                    'publishedAt': video['publishedAt']\n",
    "                })\n",
    "        \n",
    "        if all_tags:\n",
    "            tag_df = pd.DataFrame(all_tags)\n",
    "            \n",
    "            # Calculate tag popularity metrics\n",
    "            self.popular_tags = tag_df.groupby('tag').agg({\n",
    "                'videoId': 'count',     # frequency\n",
    "                'viewCount': 'sum',     # total views\n",
    "                'likeCount': 'sum'      # total likes\n",
    "            }).rename(columns={\n",
    "                'videoId': 'frequency',\n",
    "                'viewCount': 'total_views',\n",
    "                'likeCount': 'total_likes'\n",
    "            })\n",
    "            \n",
    "            # Calculate tag trending score\n",
    "            self.popular_tags['avg_views_per_video'] = (\n",
    "                self.popular_tags['total_views'] / self.popular_tags['frequency']\n",
    "            )\n",
    "            self.popular_tags['avg_likes_per_video'] = (\n",
    "                self.popular_tags['total_likes'] / self.popular_tags['frequency']\n",
    "            )\n",
    "            \n",
    "            # Normalize scores\n",
    "            max_freq = self.popular_tags['frequency'].max()\n",
    "            max_views = self.popular_tags['total_views'].max()\n",
    "            max_likes = self.popular_tags['total_likes'].max()\n",
    "            \n",
    "            if max_freq > 0 and max_views > 0 and max_likes > 0:\n",
    "                self.popular_tags['frequency_score'] = self.popular_tags['frequency'] / max_freq\n",
    "                self.popular_tags['views_score'] = self.popular_tags['total_views'] / max_views\n",
    "                self.popular_tags['likes_score'] = self.popular_tags['total_likes'] / max_likes\n",
    "                \n",
    "                # Combined tag popularity score\n",
    "                self.popular_tags['tag_popularity_score'] = (\n",
    "                    0.4 * self.popular_tags['frequency_score'] +\n",
    "                    0.4 * self.popular_tags['views_score'] +\n",
    "                    0.2 * self.popular_tags['likes_score']\n",
    "                )\n",
    "            else:\n",
    "                self.popular_tags['tag_popularity_score'] = 0\n",
    "            \n",
    "            # Sort by popularity\n",
    "            self.popular_tags = self.popular_tags.sort_values('tag_popularity_score', ascending=False)\n",
    "            \n",
    "            print(f\"Analyzed {len(self.popular_tags)} unique tags\")\n",
    "            print(\"Top 10 most popular tags:\")\n",
    "            for tag, data in self.popular_tags.head(10).iterrows():\n",
    "                print(f\"  {tag}: {data['frequency']} videos, {data['total_views']:,} total views\")\n",
    "        else:\n",
    "            self.popular_tags = pd.DataFrame()\n",
    "\n",
    "    def _calculate_tag_relevance_scores(self) -> None:\n",
    "        \"\"\"\n",
    "        Calculate relevance scores for videos based on their tags.\n",
    "        \"\"\"\n",
    "        if self.popular_tags is not None and not self.popular_tags.empty:\n",
    "            tag_scores = self.popular_tags['tag_popularity_score'].to_dict()\n",
    "            \n",
    "            def calculate_video_tag_score(tags):\n",
    "                if not tags:\n",
    "                    return 0\n",
    "                scores = [tag_scores.get(tag, 0) for tag in tags]\n",
    "                return np.mean(scores) if scores else 0\n",
    "            \n",
    "            self.videos_df['tag_relevance_score'] = self.videos_df['processed_tags'].apply(\n",
    "                calculate_video_tag_score\n",
    "            )\n",
    "        else:\n",
    "            self.videos_df['tag_relevance_score'] = 0\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean text while preserving emojis and meaningful slang.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text to clean\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned text\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string if not already\n",
    "        text = str(text)\n",
    "        \n",
    "        # Remove URLs but keep the text structure\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove excessive punctuation but keep some for sentiment\n",
    "        text = re.sub(r'[.]{3,}', '...', text)\n",
    "        text = re.sub(r'[!]{2,}', '!!', text)\n",
    "        text = re.sub(r'[?]{2,}', '??', text)\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Keep emojis and basic punctuation\n",
    "        # Remove only clearly problematic characters\n",
    "        text = re.sub(r'[^\\w\\s\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF,.!?;:\\'\"()-]', '', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def generate_embeddings(self, text_column: str = 'cleaned_text') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate SBERT embeddings for the comment text.\n",
    "        Enhanced to include video titles, descriptions, and tags.\n",
    "        \n",
    "        Args:\n",
    "            text_column (str): Column name containing text to embed\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Matrix of embeddings\n",
    "        \"\"\"\n",
    "        print(\"Generating SBERT embeddings with tag integration...\")\n",
    "        \n",
    "        if self.comments_df is None:\n",
    "            raise ValueError(\"Comments data not loaded\")\n",
    "        \n",
    "        texts = self.comments_df[text_column].tolist()\n",
    "        self.embeddings = self.sbert_model.encode(texts, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"Generated embeddings shape: {self.embeddings.shape}\")\n",
    "        return self.embeddings\n",
    "\n",
    "    def perform_clustering(self, n_clusters: int = 20, niter: int = 50, verbose: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform FAISS K-Means clustering on the embeddings.\n",
    "        \n",
    "        Args:\n",
    "            n_clusters (int): Number of clusters to create\n",
    "            niter (int): Number of iterations for K-means\n",
    "            verbose (bool): Whether to print progress\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Cluster labels\n",
    "        \"\"\"\n",
    "        print(f\"Performing FAISS K-Means clustering with {n_clusters} clusters...\")\n",
    "        \n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"Embeddings not generated\")\n",
    "        \n",
    "        # Ensure embeddings are in the right format for FAISS (float32)\n",
    "        embeddings_float32 = self.embeddings.astype(np.float32)\n",
    "        \n",
    "        # Get embedding dimension\n",
    "        d = embeddings_float32.shape[1]\n",
    "        \n",
    "        # Initialize FAISS K-means\n",
    "        self.kmeans_index = faiss.Kmeans(\n",
    "            d=d,\n",
    "            k=n_clusters,\n",
    "            niter=niter,\n",
    "            verbose=verbose,\n",
    "            spherical=False,  # use Euclidean distance (not cosine)\n",
    "            gpu=False  # Set to False for compatibility\n",
    "        )\n",
    "        \n",
    "        # Train the K-means model\n",
    "        print(\"Training K-means...\")\n",
    "        self.kmeans_index.train(embeddings_float32)\n",
    "        \n",
    "        # Get cluster assignments\n",
    "        _, cluster_assignments = self.kmeans_index.index.search(embeddings_float32, 1)\n",
    "        self.clusters = cluster_assignments.flatten()\n",
    "        \n",
    "        # Add cluster labels to dataframe\n",
    "        self.comments_df['cluster'] = self.clusters\n",
    "        \n",
    "        n_clusters_found = len(set(self.clusters))\n",
    "        print(f\"Found {n_clusters_found} clusters\")\n",
    "        print(f\"Cluster distribution:\")\n",
    "        cluster_counts = pd.Series(self.clusters).value_counts().sort_index()\n",
    "        for cluster_id, count in cluster_counts.items():\n",
    "            print(f\"  Cluster {cluster_id}: {count} comments\")\n",
    "        \n",
    "        return self.clusters\n",
    "\n",
    "    def map_videos_to_clusters(self) -> None:\n",
    "        \"\"\"\n",
    "        Map videos to comment clusters based on content similarity including tags.\n",
    "        This creates the link between video performance and comment topics.\n",
    "        \"\"\"\n",
    "        print(\"Mapping videos to comment clusters (including tags)...\")\n",
    "        \n",
    "        if self.videos_df is None or self.comments_df is None:\n",
    "            raise ValueError(\"Both video and comment data must be loaded\")\n",
    "        \n",
    "        # Generate embeddings for video content including tags\n",
    "        video_texts = []\n",
    "        for _, video in self.videos_df.iterrows():\n",
    "            # Combine title, description, and tags for better topic matching\n",
    "            # Give tags more weight by repeating them\n",
    "            tag_text = ' '.join(video.get('processed_tags', [])) * 2  # Double weight for tags\n",
    "            combined_text = f\"{video.get('cleaned_title', '')} {video.get('cleaned_description', '')} {tag_text}\"\n",
    "            video_texts.append(combined_text if combined_text.strip() else video.get('title', ''))\n",
    "        \n",
    "        if not video_texts:\n",
    "            print(\"No video text found for clustering\")\n",
    "            return\n",
    "        \n",
    "        # Generate embeddings for videos\n",
    "        video_embeddings = self.sbert_model.encode(video_texts, show_progress_bar=True)\n",
    "        \n",
    "        # Find closest cluster for each video\n",
    "        video_embeddings_float32 = video_embeddings.astype(np.float32)\n",
    "        _, video_cluster_assignments = self.kmeans_index.index.search(video_embeddings_float32, 1)\n",
    "        \n",
    "        # Add cluster assignments to videos dataframe\n",
    "        self.videos_df['cluster'] = video_cluster_assignments.flatten()\n",
    "        \n",
    "        print(\"Video-to-cluster mapping completed\")\n",
    "        cluster_video_counts = self.videos_df['cluster'].value_counts().sort_index()\n",
    "        print(\"Videos per cluster:\")\n",
    "        for cluster_id, count in cluster_video_counts.items():\n",
    "            print(f\"  Cluster {cluster_id}: {count} videos\")\n",
    "\n",
    "    def analyze_sentiment(self) -> None:\n",
    "        \"\"\"\n",
    "        Perform sentiment analysis on comments.\n",
    "        \"\"\"\n",
    "        print(\"Analyzing sentiment...\")\n",
    "        \n",
    "        def get_sentiment_scores(text):\n",
    "            \"\"\"Get sentiment scores using VADER.\"\"\"\n",
    "            scores = self.sentiment_analyzer.polarity_scores(text)\n",
    "            return scores['compound'], scores['pos'], scores['neu'], scores['neg']\n",
    "        \n",
    "        # Apply sentiment analysis\n",
    "        sentiment_data = self.comments_df['cleaned_text'].apply(get_sentiment_scores)\n",
    "        sentiment_df = pd.DataFrame(sentiment_data.tolist(), \n",
    "                                  columns=['compound', 'positive', 'neutral', 'negative'])\n",
    "        \n",
    "        # Add sentiment columns to main dataframe\n",
    "        self.comments_df = pd.concat([self.comments_df, sentiment_df], axis=1)\n",
    "        \n",
    "        # Create sentiment categories\n",
    "        self.comments_df['sentiment_label'] = pd.cut(\n",
    "            self.comments_df['compound'],\n",
    "            bins=[-1, -0.05, 0.05, 1],\n",
    "            labels=['negative', 'neutral', 'positive']\n",
    "        )\n",
    "        \n",
    "        print(\"Sentiment analysis completed\")\n",
    "\n",
    "    def extract_cluster_topics(self, top_n_words: int = 10) -> Dict[int, List[str]]:\n",
    "        \"\"\"\n",
    "        Extract representative topics for each cluster including tags.\n",
    "        Enhanced to heavily weight video titles and tags for better topic identification.\n",
    "        \n",
    "        Args:\n",
    "            top_n_words (int): Number of top words to extract per cluster\n",
    "            \n",
    "        Returns:\n",
    "            Dict[int, List[str]]: Dictionary mapping cluster IDs to top words and tags\n",
    "        \"\"\"\n",
    "        print(\"Extracting cluster topics with tag integration...\")\n",
    "        \n",
    "        cluster_topics = {}\n",
    "        cluster_tags = {}\n",
    "        \n",
    "        for cluster_id in sorted(set(self.clusters)):\n",
    "            # Get comment texts for this cluster\n",
    "            cluster_comments = self.comments_df[\n",
    "                self.comments_df['cluster'] == cluster_id\n",
    "            ]['cleaned_text'].tolist()\n",
    "            \n",
    "            # Get video titles and tags for this cluster\n",
    "            cluster_videos = self.videos_df[\n",
    "                self.videos_df['cluster'] == cluster_id\n",
    "            ] if 'cluster' in self.videos_df.columns else pd.DataFrame()\n",
    "            \n",
    "            video_titles = cluster_videos['cleaned_title'].tolist() if not cluster_videos.empty else []\n",
    "            \n",
    "            # Collect tags for this cluster\n",
    "            cluster_video_tags = []\n",
    "            if not cluster_videos.empty:\n",
    "                for _, video in cluster_videos.iterrows():\n",
    "                    cluster_video_tags.extend(video.get('processed_tags', []))\n",
    "            \n",
    "            # Analyze tag frequency for this cluster\n",
    "            if cluster_video_tags:\n",
    "                tag_counter = Counter(cluster_video_tags)\n",
    "                top_cluster_tags = [tag for tag, count in tag_counter.most_common(5)]\n",
    "                cluster_tags[cluster_id] = top_cluster_tags\n",
    "            else:\n",
    "                cluster_tags[cluster_id] = []\n",
    "            \n",
    "            # Combine texts with heavy weighting for titles and tags\n",
    "            all_texts = (\n",
    "                cluster_comments + \n",
    "                video_titles * 4 +      # 4x weight for video titles\n",
    "                cluster_video_tags * 3  # 3x weight for tags\n",
    "            )\n",
    "            \n",
    "            all_text = ' '.join(all_texts).lower()\n",
    "            words = re.findall(r'\\b[a-zA-Z]{3,}\\b', all_text)\n",
    "            \n",
    "            # Enhanced stop words list\n",
    "            stop_words = {\n",
    "                'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was',\n",
    "                'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new',\n",
    "                'now', 'old', 'see', 'two', 'who', 'boy', 'did', 'man', 'way', 'she', 'too', 'any',\n",
    "                'use', 'your', 'here', 'this', 'that', 'with', 'have', 'from', 'they', 'know', 'want',\n",
    "                'been', 'good', 'much', 'some', 'time', 'very', 'when', 'come', 'could', 'like', 'will',\n",
    "                'said', 'would', 'make', 'just', 'into', 'over', 'think', 'also', 'back', 'after',\n",
    "                'first', 'well', 'work', 'life', 'only', 'look', 'year', 'more', 'where', 'what',\n",
    "                'than', 'love', 'really', 'great', 'video', 'youtube', 'channel', 'subscribe', 'comment',\n",
    "                'please', 'thank', 'thanks', 'watch', 'follow', 'instagram'\n",
    "            }\n",
    "            \n",
    "            filtered_words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "            \n",
    "            # Count word frequencies\n",
    "            word_freq = {}\n",
    "            for word in filtered_words:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "            \n",
    "            # Get top words\n",
    "            top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n_words]\n",
    "            cluster_topics[cluster_id] = [word for word, count in top_words]\n",
    "        \n",
    "        self.cluster_topics = cluster_topics\n",
    "        self.cluster_tags = cluster_tags\n",
    "        return cluster_topics\n",
    "\n",
    "    def analyze_generational_patterns(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze generational language patterns in comments and videos.\n",
    "        \"\"\"\n",
    "        print(\"Analyzing generational language patterns...\")\n",
    "        \n",
    "        # Initialize the analyzer if not already done\n",
    "        if not hasattr(self, 'generational_analyzer'):\n",
    "            self.generational_analyzer = EnhancedGenerationalLanguageAnalyzer\n",
    "        \n",
    "        if self.comments_df is not None:\n",
    "            print(\"Analyzing comments for generational patterns...\")\n",
    "            # Analyze comments\n",
    "            self.comments_df['generational_scores'] = self.comments_df['cleaned_text'].apply(\n",
    "                self.generational_analyzer.analyze_generational_language\n",
    "            )\n",
    "            \n",
    "            # Extract individual generation scores\n",
    "            for generation in ['gen_z', 'millennial', 'gen_x', 'boomer']:\n",
    "                self.comments_df[f'{generation}_score'] = self.comments_df['generational_scores'].apply(\n",
    "                    lambda x: x.get(generation, 0) if isinstance(x, dict) else 0\n",
    "                )\n",
    "            \n",
    "            # Classify predominant generation\n",
    "            self.comments_df['dominant_generation'] = self.comments_df['cleaned_text'].apply(\n",
    "                self.generational_analyzer.classify_generation\n",
    "            )\n",
    "            \n",
    "            print(\"Comments generational analysis completed\")\n",
    "        \n",
    "        if self.videos_df is not None:\n",
    "            print(\"Analyzing videos for generational patterns...\")\n",
    "            # Analyze video titles and descriptions\n",
    "            combined_video_text = (\n",
    "                self.videos_df.get('cleaned_title', '').fillna('') + ' ' + \n",
    "                self.videos_df.get('cleaned_description', '').fillna('')\n",
    "            )\n",
    "            \n",
    "            self.videos_df['generational_scores'] = combined_video_text.apply(\n",
    "                self.generational_analyzer.analyze_generational_language\n",
    "            )\n",
    "            \n",
    "            # Extract individual generation scores\n",
    "            for generation in ['gen_z', 'millennial', 'gen_x', 'boomer']:\n",
    "                self.videos_df[f'{generation}_score'] = self.videos_df['generational_scores'].apply(\n",
    "                    lambda x: x.get(generation, 0) if isinstance(x, dict) else 0\n",
    "                )\n",
    "            \n",
    "            self.videos_df['dominant_generation'] = combined_video_text.apply(\n",
    "                self.generational_analyzer.classify_generation\n",
    "            )\n",
    "            \n",
    "            print(\"Videos generational analysis completed\")\n",
    "\n",
    "    \n",
    "    def analyze_generational_trends_by_cluster(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze generational trends for each cluster.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Generational analysis by cluster\n",
    "        \"\"\"\n",
    "        if self.clusters is None or self.comments_df is None:\n",
    "            print(\"Clustering must be performed first\")\n",
    "            return {}\n",
    "        \n",
    "        generational_cluster_analysis = {}\n",
    "        \n",
    "        for cluster_id in sorted(set(self.clusters)):\n",
    "            cluster_comments = self.comments_df[self.comments_df['cluster'] == cluster_id]\n",
    "            cluster_videos = self.videos_df[self.videos_df['cluster'] == cluster_id] if 'cluster' in self.videos_df.columns else pd.DataFrame()\n",
    "            \n",
    "            if len(cluster_comments) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Analyze generational distribution in comments\n",
    "            generation_distribution = cluster_comments['dominant_generation'].value_counts(normalize=True)\n",
    "            \n",
    "            # Calculate average generational scores\n",
    "            avg_gen_scores = {}\n",
    "            for generation in ['gen_z', 'millennial', 'gen_x', 'boomer']:\n",
    "                avg_gen_scores[generation] = cluster_comments[f'{generation}_score'].mean()\n",
    "            \n",
    "            # Identify dominant generation for this cluster\n",
    "            dominant_gen = max(avg_gen_scores.items(), key=lambda x: x[1])\n",
    "            \n",
    "            # Analyze video performance by generation\n",
    "            video_performance_by_gen = {}\n",
    "            if not cluster_videos.empty:\n",
    "                for generation in ['gen_z', 'millennial', 'gen_x', 'boomer', 'neutral']:\n",
    "                    gen_videos = cluster_videos[cluster_videos['dominant_generation'] == generation]\n",
    "                    if len(gen_videos) > 0:\n",
    "                        video_performance_by_gen[generation] = {\n",
    "                            'count': len(gen_videos),\n",
    "                            'avg_views': gen_videos['viewCount'].mean(),\n",
    "                            'avg_likes': gen_videos['likeCount'].mean(),\n",
    "                            'avg_engagement': gen_videos['engagement_rate'].mean()\n",
    "                        }\n",
    "            \n",
    "            generational_cluster_analysis[cluster_id] = {\n",
    "                'topic_words': self.cluster_topics.get(cluster_id, []),\n",
    "                'topic_tags': self.cluster_tags.get(cluster_id, []),\n",
    "                'dominant_generation': dominant_gen[0],\n",
    "                'dominant_generation_score': dominant_gen[1],\n",
    "                'generation_distribution': generation_distribution.to_dict(),\n",
    "                'avg_generational_scores': avg_gen_scores,\n",
    "                'video_performance_by_generation': video_performance_by_gen,\n",
    "                'total_comments': len(cluster_comments),\n",
    "                'total_videos': len(cluster_videos)\n",
    "            }\n",
    "        \n",
    "        self.generational_clusters = generational_cluster_analysis\n",
    "        return generational_cluster_analysis\n",
    "\n",
    "    def prepare_time_series_data(self, time_window: str = 'D') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare enhanced time series data combining video, comment, and tag metrics.\n",
    "        \n",
    "        Args:\n",
    "            time_window (str): Time aggregation window ('D' for daily, 'W' for weekly)\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Enhanced time series data including tag metrics\n",
    "        \"\"\"\n",
    "        print(f\"Preparing enhanced time series data with tags and {time_window} aggregation...\")\n",
    "        \n",
    "        # Create comment-based time series\n",
    "        comment_time_series = []\n",
    "        for cluster_id in sorted(set(self.clusters)):\n",
    "            cluster_data = self.comments_df[self.comments_df['cluster'] == cluster_id]\n",
    "            \n",
    "            # Aggregate by time window\n",
    "            time_series = cluster_data.groupby(\n",
    "                pd.Grouper(key='publishedAt', freq=time_window)\n",
    "            ).agg({\n",
    "                'commentId': 'count',\n",
    "                'likeCount': 'sum',\n",
    "                'compound': 'mean',\n",
    "                'positive': 'mean',\n",
    "                'negative': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            time_series['cluster'] = cluster_id\n",
    "            time_series.columns = ['date', 'comment_count', 'comment_likes', \n",
    "                                 'avg_sentiment', 'avg_positive', 'avg_negative', 'cluster']\n",
    "            comment_time_series.append(time_series)\n",
    "        \n",
    "        comment_trend_data = pd.concat(comment_time_series, ignore_index=True) if comment_time_series else pd.DataFrame()\n",
    "        \n",
    "        # Create enhanced video-based time series including tag metrics\n",
    "        video_time_series = []\n",
    "        if 'cluster' in self.videos_df.columns:\n",
    "            for cluster_id in sorted(set(self.clusters)):\n",
    "                cluster_videos = self.videos_df[self.videos_df['cluster'] == cluster_id]\n",
    "                \n",
    "                if len(cluster_videos) > 0:\n",
    "                    # Aggregate video metrics by time window\n",
    "                    video_ts = cluster_videos.groupby(\n",
    "                        pd.Grouper(key='publishedAt', freq=time_window)\n",
    "                    ).agg({\n",
    "                        'videoId': 'count',\n",
    "                        'viewCount': 'sum',\n",
    "                        'likeCount': 'sum',\n",
    "                        'commentCount': 'sum',\n",
    "                        'trending_score': 'mean',\n",
    "                        'engagement_rate': 'mean',\n",
    "                        'tag_count': 'mean',\n",
    "                        'tag_relevance_score': 'mean'\n",
    "                    }).reset_index()\n",
    "                    \n",
    "                    video_ts['cluster'] = cluster_id\n",
    "                    video_ts.columns = ['date', 'video_count', 'total_views', 'video_likes',\n",
    "                                      'video_comments', 'avg_trending_score', 'avg_engagement_rate',\n",
    "                                      'avg_tag_count', 'avg_tag_relevance', 'cluster']\n",
    "                    video_time_series.append(video_ts)\n",
    "        \n",
    "        video_trend_data = pd.concat(video_time_series, ignore_index=True) if video_time_series else pd.DataFrame()\n",
    "        \n",
    "        # Combine comment and video data\n",
    "        if not comment_trend_data.empty and not video_trend_data.empty:\n",
    "            # Merge on date and cluster\n",
    "            combined_data = pd.merge(\n",
    "                comment_trend_data,\n",
    "                video_trend_data,\n",
    "                on=['date', 'cluster'],\n",
    "                how='outer'\n",
    "            ).fillna(0)\n",
    "        elif not comment_trend_data.empty:\n",
    "            combined_data = comment_trend_data.copy()\n",
    "            # Add empty video columns\n",
    "            video_cols = ['video_count', 'total_views', 'video_likes', 'video_comments',\n",
    "                         'avg_trending_score', 'avg_engagement_rate', 'avg_tag_count', 'avg_tag_relevance']\n",
    "            for col in video_cols:\n",
    "                combined_data[col] = 0\n",
    "        else:\n",
    "            combined_data = pd.DataFrame()\n",
    "        \n",
    "        if not combined_data.empty:\n",
    "            # Enhanced combined trending score with tag integration\n",
    "            combined_data['combined_trending_score'] = (\n",
    "                self.video_weight * (\n",
    "                    combined_data['avg_trending_score'] * 0.4 +\n",
    "                    (combined_data['total_views'] / combined_data['total_views'].max() \n",
    "                     if combined_data['total_views'].max() > 0 else 0) * 0.3 +\n",
    "                    (combined_data['video_likes'] / combined_data['video_likes'].max() \n",
    "                     if combined_data['video_likes'].max() > 0 else 0) * 0.3\n",
    "                ) +\n",
    "                self.comment_weight * (\n",
    "                    (combined_data['comment_count'] / combined_data['comment_count'].max() \n",
    "                     if combined_data['comment_count'].max() > 0 else 0) * 0.5 +\n",
    "                    combined_data['avg_sentiment'] * 0.3 +\n",
    "                    (combined_data['comment_likes'] / combined_data['comment_likes'].max() \n",
    "                     if combined_data['comment_likes'].max() > 0 else 0) * 0.2\n",
    "                ) +\n",
    "                self.tag_weight * (\n",
    "                    combined_data['avg_tag_relevance'] * 0.7 +\n",
    "                    (combined_data['avg_tag_count'] / combined_data['avg_tag_count'].max() \n",
    "                     if combined_data['avg_tag_count'].max() > 0 else 0) * 0.3\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Fill missing dates with zero values\n",
    "            date_range = pd.date_range(\n",
    "                start=combined_data['date'].min(),\n",
    "                end=combined_data['date'].max(),\n",
    "                freq=time_window\n",
    "            )\n",
    "            \n",
    "            complete_data = []\n",
    "            for cluster_id in combined_data['cluster'].unique():\n",
    "                cluster_df = pd.DataFrame({'date': date_range, 'cluster': cluster_id})\n",
    "                cluster_trend = combined_data[combined_data['cluster'] == cluster_id]\n",
    "                merged = cluster_df.merge(cluster_trend, on=['date', 'cluster'], how='left')\n",
    "                merged = merged.fillna(0)\n",
    "                complete_data.append(merged)\n",
    "            \n",
    "            self.combined_trend_data = pd.concat(complete_data, ignore_index=True)\n",
    "        else:\n",
    "            self.combined_trend_data = pd.DataFrame()\n",
    "        \n",
    "        return self.combined_trend_data\n",
    "\n",
    "    def identify_trending_topics(self, window_days: int = 30, growth_threshold: float = 0.05) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Enhanced trending topic identification considering video performance and tag relevance.\n",
    "        \n",
    "        Args:\n",
    "            window_days (int): Number of recent days to analyze\n",
    "            growth_threshold (float): Minimum growth rate to consider trending\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of trending topics with comprehensive metadata including tags\n",
    "        \"\"\"\n",
    "        print(\"Identifying trending topics with video and tag emphasis...\")\n",
    "        \n",
    "        if self.combined_trend_data.empty:\n",
    "            print(\"No combined trend data available\")\n",
    "            return []\n",
    "        \n",
    "        trending_topics = []\n",
    "        recent_date = self.combined_trend_data['date'].max() - timedelta(days=window_days)\n",
    "        \n",
    "        for cluster_id in self.combined_trend_data['cluster'].unique():\n",
    "            cluster_data = self.combined_trend_data[self.combined_trend_data['cluster'] == cluster_id]\n",
    "            recent_data = cluster_data[cluster_data['date'] >= recent_date]\n",
    "            older_data = cluster_data[cluster_data['date'] < recent_date]\n",
    "            \n",
    "            if len(recent_data) == 0 or len(older_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate growth for different metrics including tags\n",
    "            metrics = {\n",
    "                'combined_score': 'combined_trending_score',\n",
    "                'video_views': 'total_views',\n",
    "                'video_likes': 'video_likes',\n",
    "                'comment_count': 'comment_count',\n",
    "                'tag_relevance': 'avg_tag_relevance'\n",
    "            }\n",
    "            \n",
    "            growth_rates = {}\n",
    "            for metric_name, metric_col in metrics.items():\n",
    "                if metric_col in recent_data.columns:\n",
    "                    recent_avg = recent_data[metric_col].mean()\n",
    "                    older_avg = older_data[metric_col].mean()\n",
    "                    \n",
    "                    if older_avg > 0:\n",
    "                        growth_rates[metric_name] = (recent_avg - older_avg) / older_avg\n",
    "                    else:\n",
    "                        growth_rates[metric_name] = float('inf') if recent_avg > 0 else 0\n",
    "                else:\n",
    "                    growth_rates[metric_name] = 0\n",
    "            \n",
    "            # Use combined score as primary growth indicator\n",
    "            primary_growth = growth_rates.get('combined_score', 0)\n",
    "            \n",
    "            if primary_growth >= growth_threshold:\n",
    "                topic_words = self.cluster_topics.get(cluster_id, [])\n",
    "                topic_tags = self.cluster_tags.get(cluster_id, [])\n",
    "                \n",
    "                # Get recent video performance\n",
    "                recent_videos = self.videos_df[\n",
    "                    (self.videos_df['cluster'] == cluster_id) &\n",
    "                    (self.videos_df['date'] >= recent_date.date())\n",
    "                ] if 'cluster' in self.videos_df.columns else pd.DataFrame()\n",
    "                \n",
    "                trending_topics.append({\n",
    "                    'cluster_id': int(cluster_id),\n",
    "                    'topic_words': topic_words,\n",
    "                    'topic_tags': topic_tags,\n",
    "                    'combined_growth_rate': primary_growth,\n",
    "                    'video_views_growth': growth_rates.get('video_views', 0),\n",
    "                    'video_likes_growth': growth_rates.get('video_likes', 0),\n",
    "                    'comment_growth': growth_rates.get('comment_count', 0),\n",
    "                    'tag_relevance_growth': growth_rates.get('tag_relevance', 0),\n",
    "                    'recent_video_count': len(recent_videos),\n",
    "                    'recent_total_views': int(recent_data['total_views'].sum()),\n",
    "                    'recent_video_likes': int(recent_data['video_likes'].sum()),\n",
    "                    'recent_comment_count': int(recent_data['comment_count'].sum()),\n",
    "                    'avg_sentiment': float(recent_data['avg_sentiment'].mean()),\n",
    "                    'avg_trending_score': float(recent_data['avg_trending_score'].mean()),\n",
    "                    'avg_engagement_rate': float(recent_data['avg_engagement_rate'].mean()),\n",
    "                    'avg_tag_count': float(recent_data['avg_tag_count'].mean()),\n",
    "                    'avg_tag_relevance': float(recent_data['avg_tag_relevance'].mean()),\n",
    "                    'trending_category': self._categorize_trend(primary_growth, growth_rates)\n",
    "                })\n",
    "        \n",
    "        # Sort by combined growth rate (video and tag weighted)\n",
    "        trending_topics.sort(key=lambda x: x['combined_growth_rate'], reverse=True)\n",
    "        return trending_topics\n",
    "\n",
    "    def identify_generational_trending_topics(self, window_days: int = 30) -> Dict:\n",
    "        \"\"\"\n",
    "        Identify trending topics by generation.\n",
    "        \n",
    "        Args:\n",
    "            window_days (int): Number of recent days to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Trending topics organized by generation\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'generational_clusters') or self.generational_clusters is None:\n",
    "            self.analyze_generational_trends_by_cluster()\n",
    "        \n",
    "        recent_date = self.comments_df['publishedAt'].max() - timedelta(days=window_days)\n",
    "        recent_comments = self.comments_df[self.comments_df['publishedAt'] >= recent_date]\n",
    "        \n",
    "        generational_trends = {\n",
    "            'gen_z': [],\n",
    "            'millennial': [],\n",
    "            'gen_x': [],\n",
    "            'boomer': [],\n",
    "            'neutral': []\n",
    "        }\n",
    "        \n",
    "        for cluster_id, analysis in self.generational_clusters.items():\n",
    "            dominant_gen = analysis['dominant_generation']\n",
    "            \n",
    "            # Calculate trend metrics for this cluster\n",
    "            cluster_recent_comments = recent_comments[recent_comments['cluster'] == cluster_id]\n",
    "            \n",
    "            if len(cluster_recent_comments) > 0:\n",
    "                # Calculate engagement and growth metrics\n",
    "                avg_sentiment = cluster_recent_comments['compound'].mean() if 'compound' in cluster_recent_comments.columns else 0\n",
    "                comment_volume = len(cluster_recent_comments)\n",
    "                avg_likes = cluster_recent_comments['likeCount'].mean()\n",
    "                \n",
    "                # Get video performance for this generation\n",
    "                video_perf = analysis['video_performance_by_generation'].get(dominant_gen, {})\n",
    "                \n",
    "                trend_data = {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'topic_words': analysis['topic_words'][:5],\n",
    "                    'topic_tags': analysis['topic_tags'][:3],\n",
    "                    'dominant_generation': dominant_gen,\n",
    "                    'generation_confidence': analysis['dominant_generation_score'],\n",
    "                    'recent_comment_volume': comment_volume,\n",
    "                    'avg_sentiment': avg_sentiment,\n",
    "                    'avg_comment_likes': avg_likes,\n",
    "                    'generation_distribution': analysis['generation_distribution'],\n",
    "                    'video_performance': video_perf\n",
    "                }\n",
    "                \n",
    "                generational_trends[dominant_gen].append(trend_data)\n",
    "        \n",
    "        # Sort each generation's trends by relevance\n",
    "        for generation in generational_trends:\n",
    "            generational_trends[generation].sort(\n",
    "                key=lambda x: (x['recent_comment_volume'] * (1 + x['avg_sentiment'])), \n",
    "                reverse=True\n",
    "            )\n",
    "        \n",
    "        self.generational_trends = generational_trends\n",
    "        return generational_trends\n",
    "\n",
    "\n",
    "    def _categorize_trend(self, primary_growth: float, growth_rates: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Categorize the type of trend based on growth patterns including tags.\n",
    "        \n",
    "        Args:\n",
    "            primary_growth (float): Primary growth rate\n",
    "            growth_rates (Dict): Dictionary of growth rates for different metrics\n",
    "            \n",
    "        Returns:\n",
    "            str: Trend category\n",
    "        \"\"\"\n",
    "        video_growth = max(growth_rates.get('video_views', 0), growth_rates.get('video_likes', 0))\n",
    "        comment_growth = growth_rates.get('comment_count', 0)\n",
    "        tag_growth = growth_rates.get('tag_relevance', 0)\n",
    "        \n",
    "        if video_growth > 0.3 and comment_growth > 0.2 and tag_growth > 0.2:\n",
    "            return \"viral\"\n",
    "        elif video_growth > 0.2 and tag_growth > 0.15:\n",
    "            return \"video_trending\"\n",
    "        elif comment_growth > 0.2:\n",
    "            return \"discussion_trending\"\n",
    "        elif tag_growth > 0.2:\n",
    "            return \"tag_trending\"\n",
    "        elif primary_growth > 0.1:\n",
    "            return \"emerging\"\n",
    "        else:\n",
    "            return \"stable_growth\"\n",
    "\n",
    "    def build_enhanced_lstm_model(self, sequence_length: int = 7) -> None:\n",
    "        \"\"\"\n",
    "        Build enhanced LSTM model that includes video metrics and tag features.\n",
    "        \n",
    "        Args:\n",
    "            sequence_length (int): Number of time steps to look back\n",
    "        \"\"\"\n",
    "        print(\"Building enhanced LSTM model with video and tag features...\")\n",
    "        \n",
    "        # Enhanced feature set including tag metrics\n",
    "        self.features = [\n",
    "            'comment_count', 'comment_likes', 'avg_sentiment', 'video_count',\n",
    "            'total_views', 'video_likes', 'avg_trending_score', 'avg_engagement_rate',\n",
    "            'avg_tag_count', 'avg_tag_relevance', 'combined_trending_score'\n",
    "        ]\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Build enhanced model architecture\n",
    "        self.lstm_model = Sequential([\n",
    "            LSTM(64, return_sequences=True, input_shape=(sequence_length, len(self.features))),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            Dropout(0.3),\n",
    "            LSTM(32, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        self.lstm_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        print(\"Enhanced LSTM model with tag features built successfully\")\n",
    "\n",
    "    def analyze_tag_trends(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze trending tags and their evolution over time.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Tag trend analysis results\n",
    "        \"\"\"\n",
    "        print(\"Analyzing tag trends over time...\")\n",
    "        \n",
    "        if self.popular_tags is None or self.popular_tags.empty:\n",
    "            return {}\n",
    "        \n",
    "        # Get tag trends over time\n",
    "        tag_time_series = []\n",
    "        \n",
    "        # Create time series for each popular tag\n",
    "        for tag in self.popular_tags.head(20).index:  # Top 20 tags\n",
    "            tag_videos = []\n",
    "            for _, video in self.videos_df.iterrows():\n",
    "                if tag in video.get('processed_tags', []):\n",
    "                    tag_videos.append({\n",
    "                        'date': video['date'],\n",
    "                        'viewCount': video.get('viewCount', 0),\n",
    "                        'likeCount': video.get('likeCount', 0),\n",
    "                        'tag': tag\n",
    "                    })\n",
    "            \n",
    "            if tag_videos:\n",
    "                tag_df = pd.DataFrame(tag_videos)\n",
    "                tag_ts = tag_df.groupby('date').agg({\n",
    "                    'viewCount': 'sum',\n",
    "                    'likeCount': 'sum'\n",
    "                }).reset_index()\n",
    "                tag_ts['tag'] = tag\n",
    "                tag_time_series.append(tag_ts)\n",
    "        \n",
    "        tag_trends = {}\n",
    "        if tag_time_series:\n",
    "            # Calculate growth rates for tags\n",
    "            for tag_data in tag_time_series:\n",
    "                tag = tag_data['tag'].iloc[0]\n",
    "                if len(tag_data) >= 2:\n",
    "                    recent_views = tag_data['viewCount'].tail(7).mean()  # Last week average\n",
    "                    older_views = tag_data['viewCount'].head(7).mean()   # First week average\n",
    "                    \n",
    "                    if older_views > 0:\n",
    "                        growth_rate = (recent_views - older_views) / older_views\n",
    "                    else:\n",
    "                        growth_rate = float('inf') if recent_views > 0 else 0\n",
    "                    \n",
    "                    tag_trends[tag] = {\n",
    "                        'growth_rate': growth_rate,\n",
    "                        'total_views': tag_data['viewCount'].sum(),\n",
    "                        'total_likes': tag_data['likeCount'].sum(),\n",
    "                        'video_count': self.popular_tags.loc[tag, 'frequency'],\n",
    "                        'popularity_score': self.popular_tags.loc[tag, 'tag_popularity_score']\n",
    "                    }\n",
    "        \n",
    "        # Sort by growth rate\n",
    "        sorted_tag_trends = dict(sorted(tag_trends.items(), key=lambda x: x[1]['growth_rate'], reverse=True))\n",
    "        return sorted_tag_trends\n",
    "\n",
    "    def analyze_generational_trends_by_cluster(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze generational trends for each cluster.\n",
    "        \"\"\"\n",
    "        if self.comments_df is None or 'cluster' not in self.comments_df.columns:\n",
    "            print(\"Comment data or clustering results not available\")\n",
    "            return\n",
    "        \n",
    "        print(\"Analyzing generational trends by cluster...\")\n",
    "        \n",
    "        # First, ensure we have the generational analyzer\n",
    "        if not hasattr(self, 'generational_analyzer'):\n",
    "            self.generational_analyzer = EnhancedGenerationalLanguageAnalyzer\n",
    "        \n",
    "        # Perform generational analysis if not already done\n",
    "        if 'dominant_generation' not in self.comments_df.columns:\n",
    "            print(\"Performing generational language analysis...\")\n",
    "            self.analyze_generational_patterns()\n",
    "        \n",
    "        # Ensure we have the required columns\n",
    "        required_columns = ['dominant_generation', 'gen_z_score', 'millennial_score', 'gen_x_score', 'boomer_score']\n",
    "        missing_columns = [col for col in required_columns if col not in self.comments_df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"Missing columns: {missing_columns}. Performing generational analysis...\")\n",
    "            self.analyze_generational_patterns()\n",
    "        \n",
    "        clusters = self.comments_df['cluster'].unique()\n",
    "        self.generational_clusters = {}\n",
    "        \n",
    "        for cluster_id in clusters:\n",
    "            cluster_comments = self.comments_df[self.comments_df['cluster'] == cluster_id].copy()\n",
    "            \n",
    "            if len(cluster_comments) < 10:  # Skip small clusters\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Generational distribution analysis\n",
    "                generation_distribution = cluster_comments['dominant_generation'].value_counts(normalize=True)\n",
    "                \n",
    "                if len(generation_distribution) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                dominant_generation = generation_distribution.index[0]\n",
    "                dominant_generation_score = generation_distribution.iloc[0]\n",
    "                \n",
    "                # Calculate average generational scores for this cluster\n",
    "                avg_generational_scores = {\n",
    "                    'gen_z': cluster_comments['gen_z_score'].mean(),\n",
    "                    'millennial': cluster_comments['millennial_score'].mean(),\n",
    "                    'gen_x': cluster_comments['gen_x_score'].mean(),\n",
    "                    'boomer': cluster_comments['boomer_score'].mean()\n",
    "                }\n",
    "                \n",
    "                # Get topic information\n",
    "                topic_words = self.cluster_topics.get(cluster_id, [])[:10]\n",
    "                topic_tags = self.cluster_tags.get(cluster_id, [])[:5]\n",
    "                \n",
    "                # Video performance analysis by generation\n",
    "                video_performance_by_generation = {}\n",
    "                if hasattr(self, 'videos_df') and self.videos_df is not None and 'cluster' in self.videos_df.columns:\n",
    "                    cluster_videos = self.videos_df[self.videos_df['cluster'] == cluster_id]\n",
    "                    \n",
    "                    if not cluster_videos.empty and 'dominant_generation' in cluster_videos.columns:\n",
    "                        for generation in ['gen_z', 'millennial', 'gen_x', 'boomer', 'neutral']:\n",
    "                            gen_videos = cluster_videos[cluster_videos['dominant_generation'] == generation]\n",
    "                            if len(gen_videos) > 0:\n",
    "                                video_performance_by_generation[generation] = {\n",
    "                                    'count': len(gen_videos),\n",
    "                                    'avg_views': gen_videos.get('viewCount', pd.Series([0])).mean(),\n",
    "                                    'avg_likes': gen_videos.get('likeCount', pd.Series([0])).mean(),\n",
    "                                    'avg_engagement': gen_videos.get('engagement_rate', pd.Series([0])).mean()\n",
    "                                }\n",
    "                \n",
    "                self.generational_clusters[cluster_id] = {\n",
    "                    'dominant_generation': dominant_generation,\n",
    "                    'dominant_generation_score': dominant_generation_score,\n",
    "                    'generation_distribution': generation_distribution.to_dict(),\n",
    "                    'avg_generational_scores': avg_generational_scores,\n",
    "                    'topic_words': topic_words,\n",
    "                    'topic_tags': topic_tags,\n",
    "                    'video_performance_by_generation': video_performance_by_generation,\n",
    "                    'cluster_size': len(cluster_comments)\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing cluster {cluster_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Generational analysis completed for {len(self.generational_clusters)} clusters\")\n",
    "\n",
    "\n",
    "\n",
    "    def visualize_enhanced_trends(self, top_n_clusters: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Create comprehensive trend visualizations using matplotlib and seaborn including video metrics and tags.\n",
    "        \n",
    "        Args:\n",
    "            top_n_clusters (int): Number of top clusters to visualize\n",
    "        \"\"\"\n",
    "        print(\"Creating enhanced trend visualizations with tag integration...\")\n",
    "        \n",
    "        if self.combined_trend_data.empty:\n",
    "            print(\"No trend data available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # Get top clusters by combined trending score\n",
    "        cluster_performance = self.combined_trend_data.groupby('cluster')['combined_trending_score'].sum().sort_values(ascending=False)\n",
    "        top_clusters = cluster_performance.head(top_n_clusters).index.tolist()\n",
    "        \n",
    "        # Set up the style\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Create comprehensive dashboard with subplots\n",
    "        fig, axes = plt.subplots(5, 2, figsize=(20, 25))\n",
    "        fig.suptitle('Enhanced YouTube Beauty Trends Analysis Dashboard (Video+Tag Weighted)', \n",
    "                     fontsize=20, fontweight='bold')\n",
    "        \n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(top_clusters)))\n",
    "        \n",
    "        # Plot 1: Video Views Over Time\n",
    "        ax1 = axes[0, 0]\n",
    "        for i, cluster_id in enumerate(top_clusters):\n",
    "            cluster_data = self.combined_trend_data[self.combined_trend_data['cluster'] == cluster_id]\n",
    "            topic_words = ', '.join(self.cluster_topics.get(cluster_id, [])[:3])\n",
    "            topic_tags = ', '.join(self.cluster_tags.get(cluster_id, [])[:2])\n",
    "            label = f'{topic_words} ({topic_tags})' if topic_tags else topic_words\n",
    "            \n",
    "            ax1.plot(cluster_data['date'], cluster_data['total_views'], \n",
    "                    marker='o', linewidth=2, color=colors[i], label=label[:30])\n",
    "        \n",
    "        ax1.set_title('Video Views Over Time', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel('Total Views')\n",
    "        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 2: Comment Volume Over Time\n",
    "        ax2 = axes[0, 1]\n",
    "        for i, cluster_id in enumerate(top_clusters):\n",
    "            cluster_data = self.combined_trend_data[self.combined_trend_data['cluster'] == cluster_id]\n",
    "            topic_words = ', '.join(self.cluster_topics.get(cluster_id, [])[:3])\n",
    "            \n",
    "            ax2.plot(cluster_data['date'], cluster_data['comment_count'], \n",
    "                    marker='s', linewidth=2, linestyle='--', color=colors[i], label=topic_words[:30])\n",
    "        \n",
    "        ax2.set_title('Comment Volume Over Time', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.set_ylabel('Comment Count')\n",
    "        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 3: Combined Trending Scores\n",
    "        ax3 = axes[1, 0]\n",
    "        for i, cluster_id in enumerate(top_clusters):\n",
    "            cluster_data = self.combined_trend_data[self.combined_trend_data['cluster'] == cluster_id]\n",
    "            topic_words = ', '.join(self.cluster_topics.get(cluster_id, [])[:3])\n",
    "            \n",
    "            ax3.plot(cluster_data['date'], cluster_data['combined_trending_score'], \n",
    "                    marker='D', linewidth=3, color=colors[i], label=topic_words[:30])\n",
    "        \n",
    "        ax3.set_title('Combined Trending Scores (Video+Tag Weighted)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Date')\n",
    "        ax3.set_ylabel('Combined Trending Score')\n",
    "        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 4: Tag Relevance Over Time\n",
    "        ax4 = axes[1, 1]\n",
    "        for i, cluster_id in enumerate(top_clusters):\n",
    "            cluster_data = self.combined_trend_data[self.combined_trend_data['cluster'] == cluster_id]\n",
    "            topic_tags = ', '.join(self.cluster_tags.get(cluster_id, [])[:2])\n",
    "            \n",
    "            ax4.plot(cluster_data['date'], cluster_data['avg_tag_relevance'], \n",
    "                    marker='^', linewidth=2, color=colors[i], label=topic_tags[:30] or f'Cluster {cluster_id}')\n",
    "        \n",
    "        ax4.set_title('Tag Relevance Over Time', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Date')\n",
    "        ax4.set_ylabel('Average Tag Relevance')\n",
    "        ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 5: Video vs Comment Engagement Scatter\n",
    "        ax5 = axes[2, 0]\n",
    "        engagement_data = []\n",
    "        for cluster_id in top_clusters:\n",
    "            cluster_data = self.combined_trend_data[self.combined_trend_data['cluster'] == cluster_id]\n",
    "            topic_words = ', '.join(self.cluster_topics.get(cluster_id, [])[:2])\n",
    "            \n",
    "            for _, row in cluster_data.iterrows():\n",
    "                engagement_data.append({\n",
    "                    'video_engagement': row['avg_engagement_rate'],\n",
    "                    'comment_engagement': row['comment_count'],\n",
    "                    'cluster': topic_words[:20]\n",
    "                })\n",
    "        \n",
    "        if engagement_data:\n",
    "            eng_df = pd.DataFrame(engagement_data)\n",
    "            sns.scatterplot(data=eng_df, x='video_engagement', y='comment_engagement', \n",
    "                          hue='cluster', s=100, alpha=0.7, ax=ax5)\n",
    "        \n",
    "        ax5.set_title('Video vs Comment Engagement', fontsize=14, fontweight='bold')\n",
    "        ax5.set_xlabel('Video Engagement Rate')\n",
    "        ax5.set_ylabel('Comment Count')\n",
    "        \n",
    "        # Plot 6: Sentiment Trends\n",
    "        ax6 = axes[2, 1]\n",
    "        for i, cluster_id in enumerate(top_clusters):\n",
    "            cluster_data = self.combined_trend_data[self.combined_trend_data['cluster'] == cluster_id]\n",
    "            topic_words = ', '.join(self.cluster_topics.get(cluster_id, [])[:2])\n",
    "            \n",
    "            ax6.plot(cluster_data['date'], cluster_data['avg_sentiment'], \n",
    "                    marker='o', linewidth=2, color=colors[i], label=topic_words[:30])\n",
    "        \n",
    "        ax6.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax6.set_title('Sentiment Trends', fontsize=14, fontweight='bold')\n",
    "        ax6.set_xlabel('Date')\n",
    "        ax6.set_ylabel('Average Sentiment')\n",
    "        ax6.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax6.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 7: Video Performance Metrics Heatmap\n",
    "        ax7 = axes[3, 0]\n",
    "        performance_data = []\n",
    "        for cluster_id in top_clusters:\n",
    "            cluster_data = self.combined_trend_data[self.combined_trend_data['cluster'] == cluster_id]\n",
    "            topic_words = ', '.join(self.cluster_topics.get(cluster_id, [])[:2])\n",
    "            \n",
    "            performance_data.append({\n",
    "                'Cluster': topic_words[:15],\n",
    "                'Views': cluster_data['total_views'].sum(),\n",
    "                'Likes': cluster_data['video_likes'].sum(),\n",
    "                'Comments': cluster_data['comment_count'].sum(),\n",
    "                'Engagement': cluster_data['avg_engagement_rate'].mean()\n",
    "            })\n",
    "        \n",
    "        if performance_data:\n",
    "            perf_df = pd.DataFrame(performance_data)\n",
    "            perf_df_norm = perf_df.set_index('Cluster')\n",
    "            perf_df_norm = (perf_df_norm - perf_df_norm.min()) / (perf_df_norm.max() - perf_df_norm.min())\n",
    "            \n",
    "            sns.heatmap(perf_df_norm.T, annot=True, cmap='YlOrRd', fmt='.2f', ax=ax7)\n",
    "        \n",
    "        ax7.set_title('Video Performance Metrics (Normalized)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 8: Tag Popularity Distribution\n",
    "        ax8 = axes[3, 1]\n",
    "        if self.popular_tags is not None and not self.popular_tags.empty:\n",
    "            top_tags = self.popular_tags.head(10)\n",
    "            ax8.barh(range(len(top_tags)), top_tags['tag_popularity_score'], color='skyblue')\n",
    "            ax8.set_yticks(range(len(top_tags)))\n",
    "            ax8.set_yticklabels(top_tags.index, fontsize=10)\n",
    "            ax8.set_xlabel('Tag Popularity Score')\n",
    "            ax8.set_title('Top 10 Tag Popularity Distribution', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 9: Growth Rate Comparison\n",
    "        ax9 = axes[4, 0]\n",
    "        trending_topics = self.identify_trending_topics()[:top_n_clusters]\n",
    "        if trending_topics:\n",
    "            growth_data = {\n",
    "                'Topic': [', '.join(t['topic_words'][:2]) for t in trending_topics],\n",
    "                'Video Growth': [t['video_views_growth'] for t in trending_topics],\n",
    "                'Comment Growth': [t['comment_growth'] for t in trending_topics],\n",
    "                'Tag Growth': [t['tag_relevance_growth'] for t in trending_topics]\n",
    "            }\n",
    "            \n",
    "            growth_df = pd.DataFrame(growth_data)\n",
    "            growth_df_melted = growth_df.melt(id_vars='Topic', var_name='Growth Type', value_name='Growth Rate')\n",
    "            \n",
    "            sns.barplot(data=growth_df_melted, x='Topic', y='Growth Rate', hue='Growth Type', ax=ax9)\n",
    "            ax9.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        ax9.set_title('Growth Rate Comparison by Type', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 10: Trending Categories Distribution\n",
    "        ax10 = axes[4, 1]\n",
    "        if trending_topics:\n",
    "            categories = [t['trending_category'] for t in trending_topics]\n",
    "            category_counts = pd.Series(categories).value_counts()\n",
    "            \n",
    "            colors_pie = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))\n",
    "            wedges, texts, autotexts = ax10.pie(category_counts.values, labels=category_counts.index, \n",
    "                                              autopct='%1.1f%%', colors=colors_pie)\n",
    "            \n",
    "            # Make percentage text more readable\n",
    "            for autotext in autotexts:\n",
    "                autotext.set_color('black')\n",
    "                autotext.set_fontweight('bold')\n",
    "        \n",
    "        ax10.set_title('Trending Categories Distribution', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Adjust layout and show\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.95)\n",
    "        plt.show()\n",
    "\n",
    "    def identify_generational_trending_topics(self, window_days: int = 30) -> Dict:\n",
    "        \"\"\"\n",
    "        Identify trending topics by generation.\n",
    "        \n",
    "        Args:\n",
    "            window_days (int): Number of recent days to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Trending topics organized by generation\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'generational_clusters') or self.generational_clusters is None:\n",
    "            self.analyze_generational_trends_by_cluster()\n",
    "        \n",
    "        recent_date = self.comments_df['publishedAt'].max() - timedelta(days=window_days)\n",
    "        recent_comments = self.comments_df[self.comments_df['publishedAt'] >= recent_date]\n",
    "        \n",
    "        generational_trends = {\n",
    "            'gen_z': [],\n",
    "            'millennial': [],\n",
    "            'gen_x': [],\n",
    "            'boomer': [],\n",
    "            'neutral': []\n",
    "        }\n",
    "        \n",
    "        for cluster_id, analysis in self.generational_clusters.items():\n",
    "            dominant_gen = analysis['dominant_generation']\n",
    "            \n",
    "            # Calculate trend metrics for this cluster\n",
    "            cluster_recent_comments = recent_comments[recent_comments['cluster'] == cluster_id]\n",
    "            \n",
    "            if len(cluster_recent_comments) > 0:\n",
    "                # Calculate engagement and growth metrics\n",
    "                avg_sentiment = cluster_recent_comments['compound'].mean() if 'compound' in cluster_recent_comments.columns else 0\n",
    "                comment_volume = len(cluster_recent_comments)\n",
    "                avg_likes = cluster_recent_comments['likeCount'].mean()\n",
    "                \n",
    "                # Get video performance for this generation\n",
    "                video_perf = analysis['video_performance_by_generation'].get(dominant_gen, {})\n",
    "                \n",
    "                trend_data = {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'topic_words': analysis['topic_words'][:5],\n",
    "                    'topic_tags': analysis['topic_tags'][:3],\n",
    "                    'dominant_generation': dominant_gen,\n",
    "                    'generation_confidence': analysis['dominant_generation_score'],\n",
    "                    'recent_comment_volume': comment_volume,\n",
    "                    'avg_sentiment': avg_sentiment,\n",
    "                    'avg_comment_likes': avg_likes,\n",
    "                    'generation_distribution': analysis['generation_distribution'],\n",
    "                    'video_performance': video_perf\n",
    "                }\n",
    "                \n",
    "                generational_trends[dominant_gen].append(trend_data)\n",
    "        \n",
    "        # Sort each generation's trends by relevance\n",
    "        for generation in generational_trends:\n",
    "            generational_trends[generation].sort(\n",
    "                key=lambda x: (x['recent_comment_volume'] * (1 + x['avg_sentiment'])), \n",
    "                reverse=True\n",
    "            )\n",
    "        \n",
    "        self.generational_trends = generational_trends\n",
    "        return generational_trends\n",
    "\n",
    "\n",
    "    def generate_enhanced_trend_report(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate comprehensive trend analysis report with video and tag emphasis.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Enhanced trend analysis report including tag insights\n",
    "        \"\"\"\n",
    "        print(\"Generating enhanced trend analysis report with tag integration...\")\n",
    "        \n",
    "        # Get trending topics with video and tag emphasis\n",
    "        trending_topics = self.identify_trending_topics()\n",
    "        \n",
    "        # Analyze tag trends\n",
    "        tag_trends = self.analyze_tag_trends()\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        total_comments = len(self.comments_df) if self.comments_df is not None else 0\n",
    "        total_videos = len(self.videos_df) if self.videos_df is not None else 0\n",
    "        total_clusters = len(set(self.clusters)) if self.clusters is not None else 0\n",
    "        total_unique_tags = len(self.popular_tags) if self.popular_tags is not None else 0\n",
    "        \n",
    "        # Video performance statistics\n",
    "        if self.videos_df is not None and not self.videos_df.empty:\n",
    "            total_views = self.videos_df.get('viewCount', pd.Series([0])).sum()\n",
    "            total_video_likes = self.videos_df.get('likeCount', pd.Series([0])).sum()\n",
    "            avg_engagement_rate = self.videos_df.get('engagement_rate', pd.Series([0])).mean()\n",
    "            avg_tags_per_video = self.videos_df.get('tag_count', pd.Series([0])).mean()\n",
    "            \n",
    "            top_performing_videos = self.videos_df.nlargest(5, 'trending_score')[\n",
    "                ['title', 'viewCount', 'likeCount', 'trending_score', 'processed_tags']\n",
    "            ].to_dict('records')\n",
    "        else:\n",
    "            total_views = 0\n",
    "            total_video_likes = 0\n",
    "            avg_engagement_rate = 0\n",
    "            avg_tags_per_video = 0\n",
    "            top_performing_videos = []\n",
    "        \n",
    "        # Comment statistics\n",
    "        if self.comments_df is not None and not self.comments_df.empty:\n",
    "            avg_sentiment = self.comments_df['compound'].mean()\n",
    "        else:\n",
    "            avg_sentiment = 0\n",
    "        \n",
    "        # Get most engaging clusters (video and tag weighted)\n",
    "        if not self.combined_trend_data.empty:\n",
    "            cluster_performance = self.combined_trend_data.groupby('cluster').agg({\n",
    "                'combined_trending_score': 'sum',\n",
    "                'total_views': 'sum',\n",
    "                'video_likes': 'sum',\n",
    "                'comment_count': 'sum',\n",
    "                'avg_sentiment': 'mean',\n",
    "                'avg_tag_relevance': 'mean'\n",
    "            }).sort_values('combined_trending_score', ascending=False)\n",
    "        else:\n",
    "            cluster_performance = pd.DataFrame()\n",
    "        \n",
    "        report = {\n",
    "            'analysis_summary': {\n",
    "                'total_comments_analyzed': total_comments,\n",
    "                'total_videos_analyzed': total_videos,\n",
    "                'total_topics_identified': total_clusters,\n",
    "                'total_unique_tags': total_unique_tags,\n",
    "                'total_video_views': int(total_views),\n",
    "                'total_video_likes': int(total_video_likes),\n",
    "                'avg_video_engagement_rate': float(avg_engagement_rate),\n",
    "                'avg_tags_per_video': float(avg_tags_per_video),\n",
    "                'overall_sentiment': 'positive' if avg_sentiment > 0.1 else 'negative' if avg_sentiment < -0.1 else 'neutral',\n",
    "                'video_weight_factor': self.video_weight,\n",
    "                'comment_weight_factor': self.comment_weight,\n",
    "                'tag_weight_factor': self.tag_weight,\n",
    "                'analysis_period': {\n",
    "                    'start_date': str(self.comments_df['publishedAt'].min().date()) if self.comments_df is not None and not self.comments_df.empty else 'N/A',\n",
    "                    'end_date': str(self.comments_df['publishedAt'].max().date()) if self.comments_df is not None and not self.comments_df.empty else 'N/A'\n",
    "                }\n",
    "            },\n",
    "            'trending_topics': trending_topics[:10],  # Top 10 trending with video and tag emphasis\n",
    "            'trending_tags': [\n",
    "                {\n",
    "                    'tag': tag,\n",
    "                    'growth_rate': data['growth_rate'],\n",
    "                    'total_views': data['total_views'],\n",
    "                    'video_count': data['video_count'],\n",
    "                    'popularity_score': data['popularity_score']\n",
    "                }\n",
    "                for tag, data in list(tag_trends.items())[:10]\n",
    "            ],\n",
    "            'top_performing_videos': top_performing_videos,\n",
    "            'top_engaging_clusters': [\n",
    "                {\n",
    "                    'cluster_id': int(cluster_id),\n",
    "                    'topic_words': self.cluster_topics.get(cluster_id, [])[:5],\n",
    "                    'topic_tags': self.cluster_tags.get(cluster_id, [])[:3],\n",
    "                    'combined_score': float(row['combined_trending_score']),\n",
    "                    'total_views': int(row['total_views']),\n",
    "                    'total_video_likes': int(row['video_likes']),\n",
    "                    'total_comments': int(row['comment_count']),\n",
    "                    'avg_sentiment': float(row['avg_sentiment']),\n",
    "                    'avg_tag_relevance': float(row['avg_tag_relevance'])\n",
    "                }\n",
    "                for cluster_id, row in cluster_performance.head(10).iterrows()\n",
    "            ] if not cluster_performance.empty else [],\n",
    "            'video_performance_insights': {\n",
    "                'highest_engagement_cluster': int(cluster_performance.index[0]) if not cluster_performance.empty else None,\n",
    "                'most_viewed_topic': self.cluster_topics.get(int(cluster_performance.index[0]), [])[:3] if not cluster_performance.empty else [],\n",
    "                'most_viewed_tags': self.cluster_tags.get(int(cluster_performance.index[0]), [])[:3] if not cluster_performance.empty else [],\n",
    "                'trend_categories': {\n",
    "                    category: len([t for t in trending_topics if t.get('trending_category') == category])\n",
    "                    for category in ['viral', 'video_trending', 'discussion_trending', 'tag_trending', 'emerging', 'stable_growth']\n",
    "                }\n",
    "            },\n",
    "            'tag_insights': {\n",
    "                'most_popular_tags': list(self.popular_tags.head(10).index) if self.popular_tags is not None else [],\n",
    "                'fastest_growing_tags': list(tag_trends.keys())[:5],\n",
    "                'tag_coverage': (self.videos_df['tag_count'] > 0).sum() / len(self.videos_df) if self.videos_df is not None and not self.videos_df.empty else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "         # Add generational analysis\n",
    "        if not hasattr(self, 'generational_trends') or self.generational_trends is None:\n",
    "            self.identify_generational_trending_topics()\n",
    "        \n",
    "        # Add generational insights to the report\n",
    "        report['generational_insights'] = {\n",
    "            'trending_by_generation': {},\n",
    "            'generation_distribution': {},\n",
    "            'top_generational_topics': {},\n",
    "            'generational_sentiment': {}\n",
    "        }\n",
    "        \n",
    "        for generation, trends in self.generational_trends.items():\n",
    "            if trends:\n",
    "                report['generational_insights']['trending_by_generation'][generation] = trends[:5]\n",
    "                report['generational_insights']['top_generational_topics'][generation] = [\n",
    "                    {\n",
    "                        'topic_words': trend['topic_words'],\n",
    "                        'topic_tags': trend['topic_tags'],\n",
    "                        'comment_volume': trend['recent_comment_volume'],\n",
    "                        'sentiment': trend['avg_sentiment']\n",
    "                    }\n",
    "                    for trend in trends[:3]\n",
    "                ]\n",
    "        \n",
    "        return report\n",
    "\n",
    "    def run_enhanced_pipeline(self, comment_files: List[str], video_file: str, \n",
    "                            n_clusters: int = None, forecast_days: int = 30) -> Dict:\n",
    "        \"\"\"\n",
    "        Run the complete enhanced trend analysis pipeline with video and tag emphasis.\n",
    "        \n",
    "        Args:\n",
    "            comment_files (List[str]): List of comment CSV files\n",
    "            video_file (str): Video CSV file path\n",
    "            n_clusters (int): Number of clusters (if None, will optimize)\n",
    "            forecast_days (int): Number of days to forecast\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Complete enhanced analysis results including tag insights\n",
    "        \"\"\"\n",
    "        print(\"Starting enhanced trend analysis pipeline with video and tag emphasis...\")\n",
    "        \n",
    "        # Step 1: Load and preprocess data (including tags)\n",
    "        self.load_data(comment_files, video_file)\n",
    "        self.preprocess_data()\n",
    "        \n",
    "        # Step 2: Generate embeddings\n",
    "        self.generate_embeddings()\n",
    "        \n",
    "        # Step 3: Optimize cluster number if not provided\n",
    "        if n_clusters is None:\n",
    "            n_clusters = self.optimize_cluster_number()\n",
    "        \n",
    "        # Step 4: Perform clustering\n",
    "        self.perform_clustering(n_clusters=n_clusters)\n",
    "        \n",
    "        # Step 5: Map videos to clusters (including tags)\n",
    "        self.map_videos_to_clusters()\n",
    "        \n",
    "        # Step 6: Analyze sentiment\n",
    "        self.analyze_sentiment()\n",
    "        \n",
    "        # Step 7: Extract topics (enhanced with video titles and tags)\n",
    "        self.extract_cluster_topics()\n",
    "        \n",
    "        # Step 8: Prepare enhanced time series data (including tag metrics)\n",
    "        self.prepare_time_series_data()\n",
    "        \n",
    "        # Step 9: Build enhanced LSTM model (including tag features)\n",
    "        if not self.combined_trend_data.empty:\n",
    "            self.build_enhanced_lstm_model()\n",
    "        \n",
    "        # Step 10: Generate enhanced visualizations\n",
    "        self.visualize_enhanced_trends()\n",
    "        \n",
    "        # Step 11: Generate comprehensive report\n",
    "        report = self.generate_enhanced_trend_report()\n",
    "        \n",
    "        print(\"Enhanced pipeline completed successfully!\")\n",
    "        print(f\"Video weight factor: {self.video_weight}\")\n",
    "        print(f\"Comment weight factor: {self.comment_weight}\")\n",
    "        print(f\"Tag weight factor: {self.tag_weight}\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "    def plot_generational_growth_by_clusters(self, generation: str = 'gen_z', \n",
    "                                       forecast_horizons: List[int] = [20, 30, 60]) -> None:\n",
    "        \"\"\"\n",
    "        Plot growth rates for a specific generation across clusters and forecast horizons.\n",
    "        \n",
    "        Args:\n",
    "            generation: Target generation ('gen_z', 'millennial', 'gen_x', 'boomer')\n",
    "            forecast_horizons: List of forecast horizons to analyze\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'generational_clusters') or not self.generational_clusters:\n",
    "            print(\"No generational cluster data available. Running analysis...\")\n",
    "            self.analyze_generational_trends_by_cluster()\n",
    "        \n",
    "        # Get clusters dominated by the specified generation\n",
    "        generation_clusters = []\n",
    "        for cluster_id, analysis in self.generational_clusters.items():\n",
    "            if analysis['dominant_generation'] == generation:\n",
    "                generation_clusters.append({\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'topic_words': analysis['topic_words'][:3],\n",
    "                    'topic_tags': analysis['topic_tags'][:2],\n",
    "                    'generation_confidence': analysis['dominant_generation_score'],\n",
    "                    'cluster_size': analysis.get('cluster_size', 0)\n",
    "                })\n",
    "        \n",
    "        if not generation_clusters:\n",
    "            print(f\"No clusters found for generation: {generation}\")\n",
    "            return\n",
    "        \n",
    "        # Sort by cluster size (popularity)\n",
    "        generation_clusters.sort(key=lambda x: x['cluster_size'], reverse=True)\n",
    "        top_clusters = generation_clusters[:15]  # Limit to top 15 for readability\n",
    "        \n",
    "        # Create subplots for different forecast horizons\n",
    "        fig, axes = plt.subplots(1, len(forecast_horizons), figsize=(6*len(forecast_horizons), 8))\n",
    "        if len(forecast_horizons) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        fig.suptitle(f'{generation.replace(\"_\", \" \").title()} Growth Rates by Cluster Category', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Generate forecasts for each horizon if not already available\n",
    "        if not hasattr(self, 'multi_horizon_results'):\n",
    "            print(\"Generating multi-horizon forecasts...\")\n",
    "            self.multi_horizon_results = self.generate_multi_horizon_forecasts(forecast_horizons)\n",
    "        \n",
    "        for idx, horizon in enumerate(forecast_horizons):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            cluster_names = []\n",
    "            growth_rates = []\n",
    "            colors = []\n",
    "            \n",
    "            for cluster_data in top_clusters:\n",
    "                cluster_id = cluster_data['cluster_id']\n",
    "                \n",
    "                # Get growth rate from forecast results\n",
    "                if (hasattr(self, 'multi_horizon_results') and \n",
    "                    horizon in self.multi_horizon_results and\n",
    "                    cluster_id in self.multi_horizon_results[horizon]['metrics']['cluster_metrics']):\n",
    "                    \n",
    "                    growth_rate = self.multi_horizon_results[horizon]['metrics']['cluster_metrics'][cluster_id]['growth_rate']\n",
    "                    growth_rates.append(growth_rate)\n",
    "                    \n",
    "                    # Color based on growth rate\n",
    "                    if growth_rate > 0.1:\n",
    "                        colors.append('green')\n",
    "                    elif growth_rate > 0:\n",
    "                        colors.append('lightgreen')\n",
    "                    elif growth_rate > -0.05:\n",
    "                        colors.append('orange')\n",
    "                    else:\n",
    "                        colors.append('red')\n",
    "                else:\n",
    "                    growth_rates.append(0)\n",
    "                    colors.append('gray')\n",
    "                \n",
    "                # Create cluster label\n",
    "                topic_str = ', '.join(cluster_data['topic_words'])\n",
    "                tag_str = ', '.join(cluster_data['topic_tags']) if cluster_data['topic_tags'] else ''\n",
    "                label = f\"C{cluster_id}: {topic_str}\"\n",
    "                if tag_str:\n",
    "                    label += f\"\\n({tag_str})\"\n",
    "                cluster_names.append(label)\n",
    "            \n",
    "            # Create bar plot\n",
    "            bars = ax.bar(range(len(cluster_names)), growth_rates, color=colors, alpha=0.7)\n",
    "            \n",
    "            # Customize plot\n",
    "            ax.set_title(f'{horizon}-Day Forecast', fontweight='bold')\n",
    "            ax.set_xlabel('Cluster Categories')\n",
    "            ax.set_ylabel('Growth Rate')\n",
    "            ax.set_xticks(range(len(cluster_names)))\n",
    "            ax.set_xticklabels(cluster_names, rotation=45, ha='right', fontsize=8)\n",
    "            ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, growth_rates):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, \n",
    "                       height + 0.005 if height >= 0 else height - 0.01,\n",
    "                       f'{value:.2f}', ha='center', \n",
    "                       va='bottom' if height >= 0 else 'top', \n",
    "                       fontsize=8, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def generate_generational_forecasts(self, forecast_horizons: List[int] = [20, 30, 40, 60, 80]) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate forecasts specifically for generational trends.\n",
    "        \n",
    "        Args:\n",
    "            forecast_horizons: List of forecast horizons in days\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Forecasts organized by generation and horizon\n",
    "        \"\"\"\n",
    "        print(\"Generating generational-specific forecasts...\")\n",
    "        \n",
    "        # Ensure we have generational analysis\n",
    "        if not hasattr(self, 'generational_clusters') or not self.generational_clusters:\n",
    "            self.analyze_generational_trends_by_cluster()\n",
    "        \n",
    "        # Generate multi-horizon forecasts if not available\n",
    "        if not hasattr(self, 'multi_horizon_results'):\n",
    "            self.multi_horizon_results = self.generate_multi_horizon_forecasts(forecast_horizons)\n",
    "        \n",
    "        generational_forecasts = {\n",
    "            'gen_z': {},\n",
    "            'millennial': {},\n",
    "            'gen_x': {},\n",
    "            'boomer': {},\n",
    "            'neutral': {}\n",
    "        }\n",
    "        \n",
    "        for horizon in forecast_horizons:\n",
    "            for generation in generational_forecasts.keys():\n",
    "                generational_forecasts[generation][horizon] = {\n",
    "                    'clusters': [],\n",
    "                    'avg_growth_rate': 0.0,\n",
    "                    'total_clusters': 0,\n",
    "                    'trending_topics': [],\n",
    "                    'declining_topics': []\n",
    "                }\n",
    "            \n",
    "            # Process each cluster\n",
    "            if horizon in self.multi_horizon_results:\n",
    "                cluster_metrics = self.multi_horizon_results[horizon]['metrics']['cluster_metrics']\n",
    "                \n",
    "                for cluster_id, analysis in self.generational_clusters.items():\n",
    "                    dominant_gen = analysis['dominant_generation']\n",
    "                    \n",
    "                    if cluster_id in cluster_metrics:\n",
    "                        cluster_forecast = cluster_metrics[cluster_id]\n",
    "                        \n",
    "                        cluster_data = {\n",
    "                            'cluster_id': cluster_id,\n",
    "                            'topic_words': analysis['topic_words'][:3],\n",
    "                            'topic_tags': analysis['topic_tags'][:2],\n",
    "                            'growth_rate': cluster_forecast['growth_rate'],\n",
    "                            'prediction_mean': cluster_forecast['prediction_mean'],\n",
    "                            'generation_confidence': analysis['dominant_generation_score']\n",
    "                        }\n",
    "                        \n",
    "                        generational_forecasts[dominant_gen][horizon]['clusters'].append(cluster_data)\n",
    "                        \n",
    "                        if cluster_forecast['growth_rate'] > 0.05:\n",
    "                            generational_forecasts[dominant_gen][horizon]['trending_topics'].append(cluster_data)\n",
    "                        elif cluster_forecast['growth_rate'] < -0.05:\n",
    "                            generational_forecasts[dominant_gen][horizon]['declining_topics'].append(cluster_data)\n",
    "                \n",
    "                # Calculate averages for each generation\n",
    "                for generation in generational_forecasts.keys():\n",
    "                    gen_data = generational_forecasts[generation][horizon]\n",
    "                    if gen_data['clusters']:\n",
    "                        gen_data['avg_growth_rate'] = np.mean([c['growth_rate'] for c in gen_data['clusters']])\n",
    "                        gen_data['total_clusters'] = len(gen_data['clusters'])\n",
    "        \n",
    "        return generational_forecasts\n",
    "\n",
    "\n",
    "\n",
    "    def optimize_cluster_number(self, max_clusters: int = 50, sample_size: int = 10000) -> int:\n",
    "        \"\"\"\n",
    "        Find optimal number of clusters using elbow method with FAISS K-means.\n",
    "        \n",
    "        Args:\n",
    "            max_clusters (int): Maximum number of clusters to test\n",
    "            sample_size (int): Sample size for faster computation\n",
    "            \n",
    "        Returns:\n",
    "            int: Optimal number of clusters\n",
    "        \"\"\"\n",
    "        print(\"Finding optimal number of clusters...\")\n",
    "        \n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"Embeddings not generated\")\n",
    "        \n",
    "        # Sample embeddings for faster computation\n",
    "        if len(self.embeddings) > sample_size:\n",
    "            indices = np.random.choice(len(self.embeddings), sample_size, replace=False)\n",
    "            sample_embeddings = self.embeddings[indices].astype(np.float32)\n",
    "        else:\n",
    "            sample_embeddings = self.embeddings.astype(np.float32)\n",
    "        \n",
    "        d = sample_embeddings.shape[1]\n",
    "        inertias = []\n",
    "        k_range = range(5, min(max_clusters, len(sample_embeddings)//2), 5)\n",
    "        \n",
    "        for k in k_range:\n",
    "            print(f\"Testing {k} clusters...\")\n",
    "            kmeans = faiss.Kmeans(d=d, k=k, niter=20, verbose=False)\n",
    "            kmeans.train(sample_embeddings)\n",
    "            \n",
    "            # Calculate inertia (within-cluster sum of squares)\n",
    "            _, distances = kmeans.index.search(sample_embeddings, 1)\n",
    "            inertia = np.sum(distances)\n",
    "            inertias.append(inertia)\n",
    "        \n",
    "        # Find elbow point (simplified method)\n",
    "        if len(inertias) >= 3:\n",
    "            # Calculate rate of change\n",
    "            rates = []\n",
    "            for i in range(1, len(inertias)):\n",
    "                rate = (inertias[i-1] - inertias[i]) / inertias[i-1]\n",
    "                rates.append(rate)\n",
    "            \n",
    "            # Find where rate of improvement drops significantly\n",
    "            optimal_idx = 0\n",
    "            for i in range(1, len(rates)):\n",
    "                if rates[i] < rates[i-1] * 0.5:  # 50% drop in improvement rate\n",
    "                    optimal_idx = i\n",
    "                    break\n",
    "            \n",
    "            optimal_k = list(k_range)[optimal_idx + 1]  # +1 because rates is shorter\n",
    "        else:\n",
    "            optimal_k = 20  # Default fallback\n",
    "        \n",
    "        print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "        return optimal_k\n",
    "\n",
    "\n",
    "    def visualize_all_generations_forecast(self, forecast_horizons: List[int] = [20, 30, 60]) -> None:\n",
    "        \"\"\"\n",
    "        Create comprehensive visualization showing all generations' forecast performance.\n",
    "        \"\"\"\n",
    "        generational_forecasts = self.generate_generational_forecasts(forecast_horizons)\n",
    "        \n",
    "        generations = ['gen_z', 'millennial', 'gen_x', 'boomer']\n",
    "        gen_colors = {\n",
    "            'gen_z': '#FF6B6B',\n",
    "            'millennial': '#4ECDC4', \n",
    "            'gen_x': '#45B7D1',\n",
    "            'boomer': '#96CEB4'\n",
    "        }\n",
    "        \n",
    "        # Create comprehensive dashboard\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "        fig.suptitle('Generational Trend Forecasting Analysis', fontsize=18, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Average Growth Rate by Generation and Horizon\n",
    "        ax1 = axes[0, 0]\n",
    "        width = 0.2\n",
    "        x = np.arange(len(forecast_horizons))\n",
    "        \n",
    "        for i, generation in enumerate(generations):\n",
    "            if generation in generational_forecasts:\n",
    "                avg_growth_rates = []\n",
    "                for horizon in forecast_horizons:\n",
    "                    avg_growth_rates.append(generational_forecasts[generation][horizon]['avg_growth_rate'])\n",
    "                \n",
    "                ax1.bar(x + i * width, avg_growth_rates, width, \n",
    "                       label=generation.replace('_', ' ').title(), \n",
    "                       color=gen_colors[generation], alpha=0.8)\n",
    "        \n",
    "        ax1.set_xlabel('Forecast Horizon (Days)')\n",
    "        ax1.set_ylabel('Average Growth Rate')\n",
    "        ax1.set_title('Average Growth Rate by Generation', fontweight='bold')\n",
    "        ax1.set_xticks(x + width * 1.5)\n",
    "        ax1.set_xticklabels(forecast_horizons)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Plot 2: Number of Trending Topics by Generation\n",
    "        ax2 = axes[0, 1]\n",
    "        trending_counts = {gen: [] for gen in generations}\n",
    "        \n",
    "        for generation in generations:\n",
    "            for horizon in forecast_horizons:\n",
    "                count = len(generational_forecasts[generation][horizon]['trending_topics'])\n",
    "                trending_counts[generation].append(count)\n",
    "        \n",
    "        x = np.arange(len(forecast_horizons))\n",
    "        for i, generation in enumerate(generations):\n",
    "            ax2.plot(forecast_horizons, trending_counts[generation], \n",
    "                    marker='o', linewidth=2, markersize=8,\n",
    "                    color=gen_colors[generation], \n",
    "                    label=generation.replace('_', ' ').title())\n",
    "        \n",
    "        ax2.set_xlabel('Forecast Horizon (Days)')\n",
    "        ax2.set_ylabel('Number of Trending Topics')\n",
    "        ax2.set_title('Trending Topics Count by Generation', fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Gen Z specific cluster growth (as requested)\n",
    "        ax3 = axes[1, 0]\n",
    "        if 'gen_z' in generational_forecasts:\n",
    "            # Get Gen Z clusters for the middle forecast horizon\n",
    "            mid_horizon = forecast_horizons[len(forecast_horizons)//2]\n",
    "            gen_z_clusters = generational_forecasts['gen_z'][mid_horizon]['clusters'][:10]  # Top 10\n",
    "            \n",
    "            if gen_z_clusters:\n",
    "                cluster_names = []\n",
    "                growth_rates = []\n",
    "                colors = []\n",
    "                \n",
    "                for cluster in gen_z_clusters:\n",
    "                    topic_str = ', '.join(cluster['topic_words'])\n",
    "                    cluster_names.append(f\"C{cluster['cluster_id']}: {topic_str}\")\n",
    "                    growth_rates.append(cluster['growth_rate'])\n",
    "                    \n",
    "                    if cluster['growth_rate'] > 0.05:\n",
    "                        colors.append('green')\n",
    "                    elif cluster['growth_rate'] > 0:\n",
    "                        colors.append('lightgreen')\n",
    "                    else:\n",
    "                        colors.append('red')\n",
    "                \n",
    "                bars = ax3.barh(range(len(cluster_names)), growth_rates, color=colors, alpha=0.7)\n",
    "                ax3.set_yticks(range(len(cluster_names)))\n",
    "                ax3.set_yticklabels(cluster_names, fontsize=9)\n",
    "                ax3.set_xlabel('Growth Rate')\n",
    "                ax3.set_title(f'Gen Z Cluster Growth ({mid_horizon}-Day Forecast)', fontweight='bold')\n",
    "                ax3.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "                ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Generation Comparison Heatmap\n",
    "        ax4 = axes[1, 1]\n",
    "        heatmap_data = []\n",
    "        generation_labels = []\n",
    "        \n",
    "        for generation in generations:\n",
    "            if generation in generational_forecasts:\n",
    "                row_data = []\n",
    "                for horizon in forecast_horizons:\n",
    "                    avg_growth = generational_forecasts[generation][horizon]['avg_growth_rate']\n",
    "                    row_data.append(avg_growth)\n",
    "                heatmap_data.append(row_data)\n",
    "                generation_labels.append(generation.replace('_', ' ').title())\n",
    "        \n",
    "        if heatmap_data:\n",
    "            im = ax4.imshow(heatmap_data, cmap='RdYlGn', aspect='auto')\n",
    "            ax4.set_xticks(range(len(forecast_horizons)))\n",
    "            ax4.set_xticklabels(forecast_horizons)\n",
    "            ax4.set_yticks(range(len(generation_labels)))\n",
    "            ax4.set_yticklabels(generation_labels)\n",
    "            ax4.set_xlabel('Forecast Horizon (Days)')\n",
    "            ax4.set_title('Growth Rate Heatmap', fontweight='bold')\n",
    "            \n",
    "            # Add text annotations\n",
    "            for i in range(len(generation_labels)):\n",
    "                for j in range(len(forecast_horizons)):\n",
    "                    text = ax4.text(j, i, f'{heatmap_data[i][j]:.3f}',\n",
    "                                   ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "            \n",
    "            plt.colorbar(im, ax=ax4, label='Growth Rate')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generational Language Analyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T17:08:20.494093Z",
     "iopub.status.busy": "2025-09-06T17:08:20.493912Z",
     "iopub.status.idle": "2025-09-06T17:08:20.511655Z",
     "shell.execute_reply": "2025-09-06T17:08:20.511164Z",
     "shell.execute_reply.started": "2025-09-06T17:08:20.494078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnhancedGenerationalLanguageAnalyzer:\n",
    "    \"\"\"\n",
    "    Enhanced analyzer for detecting generational language patterns with beauty-specific terms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Enhanced generational vocabulary patterns based on current social media usage\n",
    "        self.generational_patterns = {\n",
    "            'gen_z': {\n",
    "                'slang': [\n",
    "                    # Core Gen Z terms\n",
    "                    'slay', 'periodt', 'no cap', 'fr', 'frfr', 'bussin', 'sheesh', 'sus',\n",
    "                    'bet', 'lowkey', 'highkey', 'deadass', 'based', 'cringe', 'hits different',\n",
    "                    'slaps', 'vibe check', 'stan', 'bestie', 'bestyyy', 'girlie', 'girly',\n",
    "                    'queen', 'king', 'icon', 'iconic', 'legend', 'fire', 'lit', 'mid',\n",
    "                    'cap', 'facts', 'say less', 'periodt pooh', 'chile', 'oop', 'and i oop',\n",
    "                    'sksksk', 'vsco', 'simp', 'salty', 'tea', 'spill', 'mood', 'same',\n",
    "                    \n",
    "                    # Beauty-specific Gen Z terms\n",
    "                    'snatched', 'beat', 'glow up', 'lewk', 'serve', 'serving looks',\n",
    "                    'beat face', 'contour', 'highlight', 'brows on fleek', 'cut crease',\n",
    "                    'wing', 'winged liner', 'blend', 'shade', 'transition', 'inner corner',\n",
    "                    'lashes', 'falsies', 'mascara wand', 'setting spray', 'prime',\n",
    "                    'dewy', 'matte', 'glossy', 'shimmer', 'pigmented', 'buildable',\n",
    "                    'grwm', 'get ready with me', 'skincare routine', 'glass skin',\n",
    "                    'no makeup makeup', 'fresh face', 'natural glam', 'everyday look',\n",
    "                    'night out', 'date night', 'going out', 'soft glam', 'dramatic',\n",
    "                    'smoky eye', 'nude lip', 'bold lip', 'red lip', 'glossy lip',\n",
    "                ],\n",
    "                'expressions': [\n",
    "                    'literally me', 'this is everything', 'not me', 'the way i',\n",
    "                    'tell me why', 'bestie this', 'girlie that', 'the audacity',\n",
    "                    'i cannot', 'i cant even', 'this aint it', 'we been knew',\n",
    "                    'it be like that', 'chile anyways', 'as you should',\n",
    "                    'living for this', 'obsessed', 'im deceased', 'i am deceased',\n",
    "                    'this look', 'that beat', 'those brows', 'the glow',\n",
    "                    'your skin', 'that highlight', 'the blend', 'those lashes',\n",
    "                    'tutorial please', 'drop the routine', 'what products',\n",
    "                    'need this', 'want this', 'trying this'\n",
    "                ],\n",
    "                'beauty_expressions': [\n",
    "                    'beat for the gods', 'face beat', 'mug is beat', 'serving face',\n",
    "                    'your mug', 'that mug', 'face card', 'never declines',\n",
    "                    'face is giving', 'look is giving', 'serving looks',\n",
    "                    'main character energy', 'hot girl', 'that girl',\n",
    "                    'clean girl', 'it girl', 'effortless', 'no effort'\n",
    "                ]\n",
    "            },\n",
    "            'millennial': {\n",
    "                'slang': [\n",
    "                    # Core Millennial terms\n",
    "                    'basic', 'bye felicia', 'cray', 'fleek', 'on fleek', 'ghosting',\n",
    "                    'hashtag', 'jelly', 'savage', 'shade', 'throwing shade', 'squad',\n",
    "                    'goals', 'relationship goals', 'thirsty', 'turnt', 'yasss', 'zero chill',\n",
    "                    'dead', 'dying', 'literally cant', 'on point', 'mood', 'relatable',\n",
    "                    'awkward', 'random', 'epic', 'fail', 'winning', 'adulting',\n",
    "                    'bae', 'fam', 'woke', 'snatched', 'extra', 'pressed',\n",
    "                    \n",
    "                    # Beauty-specific Millennial terms\n",
    "                    'contour', 'highlight', 'strobing', 'baking', 'cut crease',\n",
    "                    'winged eyeliner', 'bold brow', 'power brow', 'ombre',\n",
    "                    'balayage', 'lob', 'beach waves', 'no poo', 'bb cream',\n",
    "                    'cc cream', 'primer', 'setting powder', 'bronzer', 'blush',\n",
    "                    'lipstick', 'lip gloss', 'matte lips', 'liquid lipstick',\n",
    "                    'eyeshadow palette', 'makeup haul', 'beauty guru', 'tutorial'\n",
    "                ],\n",
    "                'expressions': [\n",
    "                    'i literally', 'so random', 'hot mess', 'train wreck',\n",
    "                    'comfort zone', 'bucket list', 'netflix and chill',\n",
    "                    'sorry not sorry', 'my bad', 'lets do this', 'game changer',\n",
    "                    'life hack', 'pro tip', 'diy', 'holy grail', 'ride or die',\n",
    "                    'must have', 'obsessed with', 'in love with', 'cant live without',\n",
    "                    'beauty routine', 'morning routine', 'night routine',\n",
    "                    'self care', 'treat yourself', 'me time'\n",
    "                ],\n",
    "                'internet_culture': [\n",
    "                    'lol', 'omg', 'wtf', 'smh', 'tbh', 'imo', 'imho', 'rofl',\n",
    "                    'lmao', 'brb', 'ttyl', 'irl', 'fomo', 'yolo', 'tbt',\n",
    "                    'inspo', 'motd', 'fotd', 'ootd', 'notd'\n",
    "                ]\n",
    "            },\n",
    "            'gen_x': {\n",
    "                'slang': [\n",
    "                    'whatever', 'as if', 'totally', 'tubular', 'rad', 'gnarly',\n",
    "                    'dude', 'sweet', 'tight', 'sick', 'phat', 'da bomb',\n",
    "                    'all that', 'bananas', 'bling', 'bouncing', 'chill',\n",
    "                    'diss', 'fresh', 'funky', 'off the hook', 'trippin'\n",
    "                ],\n",
    "                'expressions': [\n",
    "                    'talk to the hand', 'dont go there', 'been there done that',\n",
    "                    'my bad', 'whats the deal', 'get real', 'not', 'psych',\n",
    "                    'cowabunga', 'excellent', 'bogus', 'grody'\n",
    "                ]\n",
    "            },\n",
    "            'boomer': {\n",
    "                'formal_language': [\n",
    "                    'wonderful', 'lovely', 'beautiful', 'amazing', 'fantastic',\n",
    "                    'terrific', 'marvelous', 'delightful', 'charming', 'pleasant',\n",
    "                    'gorgeous', 'stunning', 'pretty', 'nice', 'good'\n",
    "                ],\n",
    "                'expressions': [\n",
    "                    'back in my day', 'when i was young', 'kids these days',\n",
    "                    'in my time', 'years ago', 'old school', 'classic',\n",
    "                    'traditional', 'proper', 'decent', 'respectable',\n",
    "                    'elegant', 'sophisticated', 'timeless'\n",
    "                ],\n",
    "                'communication_style': [\n",
    "                    'thank you', 'please', 'excuse me', 'pardon me',\n",
    "                    'bless you', 'god bless', 'have a nice day',\n",
    "                    'very nice', 'well done', 'good job'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Beauty-specific context indicators\n",
    "        self.beauty_context = [\n",
    "            'makeup', 'lipstick', 'eyeshadow', 'mascara', 'foundation', 'concealer',\n",
    "            'blush', 'bronzer', 'highlighter', 'contour', 'primer', 'setting',\n",
    "            'skincare', 'moisturizer', 'cleanser', 'serum', 'toner', 'sunscreen',\n",
    "            'routine', 'tutorial', 'look', 'glam', 'natural', 'dramatic',\n",
    "            'palette', 'brush', 'sponge', 'application', 'blend', 'shade'\n",
    "        ]\n",
    "        \n",
    "        # Compile regex patterns with lower thresholds\n",
    "        self.compiled_patterns = self._compile_patterns()\n",
    "        \n",
    "        # Adjust scoring weights\n",
    "        self.category_weights = {\n",
    "            'slang': 3.0,  # Increased weight\n",
    "            'expressions': 2.5,\n",
    "            'beauty_expressions': 4.0,  # Highest weight for beauty-specific\n",
    "            'internet_culture': 2.0,\n",
    "            'formal_language': 1.5,\n",
    "            'communication_style': 1.0\n",
    "        }\n",
    "    \n",
    "    def _compile_patterns(self):\n",
    "        \"\"\"Compile all patterns into regex for efficient matching.\"\"\"\n",
    "        compiled = {}\n",
    "        \n",
    "        for generation, patterns in self.generational_patterns.items():\n",
    "            compiled[generation] = {}\n",
    "            for category, terms in patterns.items():\n",
    "                # Create more flexible patterns\n",
    "                pattern_list = []\n",
    "                for term in terms:\n",
    "                    # Handle multi-word terms\n",
    "                    if ' ' in term:\n",
    "                        # Allow some variation in spacing and punctuation\n",
    "                        flexible_term = term.replace(' ', r'\\s+')\n",
    "                        pattern_list.append(flexible_term)\n",
    "                    else:\n",
    "                        # Single word with word boundaries\n",
    "                        pattern_list.append(r'\\b' + re.escape(term) + r'\\b')\n",
    "                \n",
    "                if pattern_list:\n",
    "                    compiled[generation][category] = re.compile(\n",
    "                        '|'.join(pattern_list), re.IGNORECASE\n",
    "                    )\n",
    "        \n",
    "        return compiled\n",
    "    \n",
    "    def analyze_generational_language(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Enhanced analysis with context awareness and flexible scoring.\"\"\"\n",
    "        if not text or len(text.strip()) < 3:\n",
    "            return {gen: 0.0 for gen in self.generational_patterns.keys()}\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        scores = {gen: 0.0 for gen in self.generational_patterns.keys()}\n",
    "        \n",
    "        # Check if it's beauty-related context\n",
    "        has_beauty_context = any(term in text_lower for term in self.beauty_context)\n",
    "        beauty_multiplier = 1.5 if has_beauty_context else 1.0\n",
    "        \n",
    "        # Analyze patterns for each generation\n",
    "        for generation, patterns in self.compiled_patterns.items():\n",
    "            for category, pattern in patterns.items():\n",
    "                matches = len(pattern.findall(text))\n",
    "                if matches > 0:\n",
    "                    weight = self.category_weights.get(category, 1.0)\n",
    "                    # Apply beauty context multiplier\n",
    "                    if 'beauty' in category or has_beauty_context:\n",
    "                        weight *= beauty_multiplier\n",
    "                    \n",
    "                    scores[generation] += matches * weight\n",
    "        \n",
    "        # Normalize by text length (words, not characters)\n",
    "        word_count = len(text.split())\n",
    "        if word_count > 0:\n",
    "            for gen in scores:\n",
    "                scores[gen] = scores[gen] / max(word_count, 1)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def classify_generation(self, text: str, threshold: float = 0.005) -> str:  # Lower threshold\n",
    "        \"\"\"Classify with more sensitive threshold.\"\"\"\n",
    "        scores = self.analyze_generational_language(text)\n",
    "        \n",
    "        if not any(score > 0 for score in scores.values()):\n",
    "            return 'neutral'\n",
    "        \n",
    "        max_generation = max(scores.items(), key=lambda x: x[1])\n",
    "        \n",
    "        if max_generation[1] >= threshold:\n",
    "            return max_generation[0]\n",
    "        else:\n",
    "            return 'neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecastor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T17:08:20.513599Z",
     "iopub.status.busy": "2025-09-06T17:08:20.513364Z",
     "iopub.status.idle": "2025-09-06T17:08:20.646542Z",
     "shell.execute_reply": "2025-09-06T17:08:20.646035Z",
     "shell.execute_reply.started": "2025-09-06T17:08:20.513581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnsembleForecaster:\n",
    "    \"\"\"\n",
    "    Ensemble forecasting model combining LSTM, ARIMA, and Prophet for more reliable predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lstm_model = None\n",
    "        self.arima_models = {}\n",
    "        self.prophet_models = {}\n",
    "        self.ensemble_weights = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models_trained = False\n",
    "        \n",
    "    def prepare_lstm_data(self, data: pd.DataFrame, target_col: str, \n",
    "                         feature_cols: List[str], sequence_length: int = 7) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare data for LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Time series data\n",
    "            target_col (str): Target variable column name\n",
    "            feature_cols (List[str]): Feature column names\n",
    "            sequence_length (int): Length of input sequences\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: X and y arrays for LSTM\n",
    "        \"\"\"\n",
    "        # Sort by date\n",
    "        data_sorted = data.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Prepare features\n",
    "        features = data_sorted[feature_cols + [target_col]].values\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        X, y = [], []\n",
    "        for i in range(sequence_length, len(features_scaled)):\n",
    "            X.append(features_scaled[i-sequence_length:i, :-1])  # All features except target\n",
    "            y.append(features_scaled[i, -1])  # Target variable\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def build_lstm_model(self, input_shape: Tuple[int, int]) -> Sequential:\n",
    "        \"\"\"\n",
    "        Build enhanced LSTM model with GRU layers.\n",
    "        \n",
    "        Args:\n",
    "            input_shape (Tuple[int, int]): Shape of input data (sequence_length, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            Sequential: Compiled LSTM model\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(0.3),\n",
    "            GRU(32, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='huber',  # More robust to outliers than MSE\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def make_series_stationary(self, series: pd.Series) -> Tuple[pd.Series, int]:\n",
    "        \"\"\"\n",
    "        Make time series stationary for ARIMA modeling.\n",
    "        \n",
    "        Args:\n",
    "            series (pd.Series): Time series data\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.Series, int]: Differenced series and number of differences\n",
    "        \"\"\"\n",
    "        diff_count = 0\n",
    "        current_series = series.copy()\n",
    "        \n",
    "        # Test for stationarity\n",
    "        while diff_count < 2:  # Maximum 2 differences\n",
    "            result = adfuller(current_series.dropna())\n",
    "            p_value = result[1]\n",
    "            \n",
    "            if p_value <= 0.05:  # Stationary\n",
    "                break\n",
    "            \n",
    "            current_series = current_series.diff()\n",
    "            diff_count += 1\n",
    "        \n",
    "        return current_series.dropna(), diff_count\n",
    "    \n",
    "    def fit_arima_model(self, data: pd.Series, cluster_id: int) -> None:\n",
    "        \"\"\"\n",
    "        Fit ARIMA model with automatic parameter selection.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.Series): Time series data for specific cluster\n",
    "            cluster_id (int): Cluster identifier\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Make series stationary\n",
    "            stationary_data, d = self.make_series_stationary(data)\n",
    "            \n",
    "            if len(stationary_data) < 10:  # Need minimum data points\n",
    "                print(f\"Insufficient data for ARIMA model for cluster {cluster_id}\")\n",
    "                return\n",
    "            \n",
    "            # Auto ARIMA parameter selection (simplified)\n",
    "            best_aic = float('inf')\n",
    "            best_order = (1, d, 1)\n",
    "            \n",
    "            for p in range(0, 3):\n",
    "                for q in range(0, 3):\n",
    "                    try:\n",
    "                        model = ARIMA(data, order=(p, d, q))\n",
    "                        fitted_model = model.fit()\n",
    "                        \n",
    "                        if fitted_model.aic < best_aic:\n",
    "                            best_aic = fitted_model.aic\n",
    "                            best_order = (p, d, q)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            # Fit best model\n",
    "            final_model = ARIMA(data, order=best_order)\n",
    "            self.arima_models[cluster_id] = final_model.fit()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting ARIMA for cluster {cluster_id}: {e}\")\n",
    "    \n",
    "    def fit_prophet_model(self, data: pd.DataFrame, cluster_id: int, target_col: str) -> None:\n",
    "        \"\"\"\n",
    "        Fit Prophet model for time series forecasting.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Time series data\n",
    "            cluster_id (int): Cluster identifier\n",
    "            target_col (str): Target column name\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare data for Prophet\n",
    "            prophet_data = data[['date', target_col]].copy()\n",
    "            prophet_data.columns = ['ds', 'y']\n",
    "            \n",
    "            # Convert to datetime and remove timezone information\n",
    "            prophet_data['ds'] = pd.to_datetime(prophet_data['ds'])\n",
    "            if prophet_data['ds'].dt.tz is not None:\n",
    "                prophet_data['ds'] = prophet_data['ds'].dt.tz_localize(None)\n",
    "            \n",
    "            # Remove any rows with NaN values\n",
    "            prophet_data = prophet_data.dropna()\n",
    "            \n",
    "            if len(prophet_data) < 10:  # Need minimum data points\n",
    "                print(f\"Insufficient data for Prophet model for cluster {cluster_id}\")\n",
    "                return\n",
    "            \n",
    "            # Ensure the data is sorted by date\n",
    "            prophet_data = prophet_data.sort_values('ds').reset_index(drop=True)\n",
    "            \n",
    "            # Configure Prophet model\n",
    "            model = Prophet(\n",
    "                changepoint_prior_scale=0.05,\n",
    "                seasonality_prior_scale=10.0,\n",
    "                holidays_prior_scale=10.0,\n",
    "                daily_seasonality=False,\n",
    "                weekly_seasonality=True,\n",
    "                yearly_seasonality=False if len(prophet_data) < 730 else True,\n",
    "                interval_width=0.8\n",
    "            )\n",
    "            \n",
    "            # Suppress Prophet's verbose output\n",
    "            import logging\n",
    "            logging.getLogger('prophet').setLevel(logging.WARNING)\n",
    "            \n",
    "            # Fit model\n",
    "            model.fit(prophet_data)\n",
    "            self.prophet_models[cluster_id] = model\n",
    "            \n",
    "            print(f\"Prophet model trained successfully for cluster {cluster_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting Prophet for cluster {cluster_id}: {e}\")\n",
    "    \n",
    "    def train_ensemble_models(self, trend_data: pd.DataFrame, target_col: str = 'combined_trending_score',\n",
    "                            feature_cols: List[str] = None, sequence_length: int = 7) -> None:\n",
    "        \"\"\"\n",
    "        Train all models in the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            trend_data (pd.DataFrame): Time series trend data\n",
    "            target_col (str): Target variable to predict\n",
    "            feature_cols (List[str]): Feature columns for LSTM\n",
    "            sequence_length (int): Sequence length for LSTM\n",
    "        \"\"\"\n",
    "        print(\"Training ensemble forecasting models...\")\n",
    "        \n",
    "        if feature_cols is None:\n",
    "            feature_cols = [\n",
    "                'comment_count', 'comment_likes', 'avg_sentiment', 'video_count',\n",
    "                'total_views', 'video_likes', 'avg_trending_score', 'avg_engagement_rate',\n",
    "                'avg_tag_count', 'avg_tag_relevance'\n",
    "            ]\n",
    "        \n",
    "        # Filter available columns\n",
    "        available_features = [col for col in feature_cols if col in trend_data.columns]\n",
    "        \n",
    "        if not available_features:\n",
    "            print(\"No feature columns found in data\")\n",
    "            return\n",
    "        \n",
    "        # Train models for each cluster\n",
    "        clusters = trend_data['cluster'].unique()\n",
    "        \n",
    "        for cluster_id in clusters:\n",
    "            cluster_data = trend_data[trend_data['cluster'] == cluster_id].copy()\n",
    "            cluster_data = cluster_data.sort_values('date').reset_index(drop=True)\n",
    "            \n",
    "            if len(cluster_data) < sequence_length + 5:  # Need minimum data\n",
    "                continue\n",
    "            \n",
    "            print(f\"Training models for cluster {cluster_id}...\")\n",
    "            \n",
    "            # Train ARIMA\n",
    "            target_series = cluster_data.set_index('date')[target_col]\n",
    "            self.fit_arima_model(target_series, cluster_id)\n",
    "            \n",
    "            # Train Prophet\n",
    "            self.fit_prophet_model(cluster_data, cluster_id, target_col)\n",
    "        \n",
    "        # Train LSTM on combined data\n",
    "        if len(trend_data) > sequence_length + 10:\n",
    "            try:\n",
    "                X, y = self.prepare_lstm_data(trend_data, target_col, available_features, sequence_length)\n",
    "                \n",
    "                if len(X) > 0:\n",
    "                    # Split data\n",
    "                    split_idx = int(len(X) * 0.8)\n",
    "                    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "                    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "                    \n",
    "                    # Build and train LSTM\n",
    "                    self.lstm_model = self.build_lstm_model((X.shape[1], X.shape[2]))\n",
    "                    \n",
    "                    early_stopping = EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        patience=10,\n",
    "                        restore_best_weights=True\n",
    "                    )\n",
    "                    \n",
    "                    self.lstm_model.fit(\n",
    "                        X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=100,\n",
    "                        batch_size=32,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    print(\"LSTM model trained successfully\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training LSTM: {e}\")\n",
    "        \n",
    "        self.models_trained = True\n",
    "        print(\"Ensemble model training completed\")\n",
    "    \n",
    "    def predict_lstm(self, data: pd.DataFrame, target_col: str, \n",
    "                    feature_cols: List[str], sequence_length: int, \n",
    "                    forecast_steps: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate LSTM predictions.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Input data\n",
    "            target_col (str): Target column\n",
    "            feature_cols (List[str]): Feature columns\n",
    "            sequence_length (int): Sequence length\n",
    "            forecast_steps (int): Number of steps to forecast\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: LSTM predictions\n",
    "        \"\"\"\n",
    "        if self.lstm_model is None:\n",
    "            return np.zeros(forecast_steps)\n",
    "        \n",
    "        try:\n",
    "            # Get last sequence\n",
    "            features = data[feature_cols + [target_col]].values\n",
    "            features_scaled = self.scaler.transform(features)\n",
    "            \n",
    "            last_sequence = features_scaled[-sequence_length:, :-1].reshape(1, sequence_length, -1)\n",
    "            \n",
    "            predictions = []\n",
    "            current_sequence = last_sequence.copy()\n",
    "            \n",
    "            for _ in range(forecast_steps):\n",
    "                pred = self.lstm_model.predict(current_sequence, verbose=0)[0, 0]\n",
    "                predictions.append(pred)\n",
    "                \n",
    "                # Update sequence for next prediction\n",
    "                # Note: This is simplified - in practice, you'd need to update with actual feature values\n",
    "                new_features = np.zeros((1, 1, features_scaled.shape[1] - 1))\n",
    "                current_sequence = np.concatenate([current_sequence[:, 1:, :], new_features], axis=1)\n",
    "            \n",
    "            # Inverse transform predictions\n",
    "            dummy_features = np.zeros((len(predictions), features_scaled.shape[1]))\n",
    "            dummy_features[:, -1] = predictions  # Last column is target\n",
    "            predictions_rescaled = self.scaler.inverse_transform(dummy_features)[:, -1]\n",
    "            \n",
    "            return predictions_rescaled\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in LSTM prediction: {e}\")\n",
    "            return np.zeros(forecast_steps)\n",
    "    \n",
    "    def predict_arima(self, cluster_id: int, forecast_steps: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate ARIMA predictions for specific cluster.\n",
    "        \n",
    "        Args:\n",
    "            cluster_id (int): Cluster identifier\n",
    "            forecast_steps (int): Number of steps to forecast\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: ARIMA predictions\n",
    "        \"\"\"\n",
    "        if cluster_id not in self.arima_models:\n",
    "            return np.zeros(forecast_steps)\n",
    "        \n",
    "        try:\n",
    "            forecast = self.arima_models[cluster_id].forecast(steps=forecast_steps)\n",
    "            return forecast.values if hasattr(forecast, 'values') else forecast\n",
    "        except Exception as e:\n",
    "            print(f\"Error in ARIMA prediction for cluster {cluster_id}: {e}\")\n",
    "            return np.zeros(forecast_steps)\n",
    "    \n",
    "    def predict_prophet(self, cluster_id: int, forecast_steps: int, \n",
    "                   last_date: pd.Timestamp) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate Prophet predictions for specific cluster.\n",
    "        \n",
    "        Args:\n",
    "            cluster_id (int): Cluster identifier\n",
    "            forecast_steps (int): Number of steps to forecast\n",
    "            last_date (pd.Timestamp): Last date in the data\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Prophet predictions\n",
    "        \"\"\"\n",
    "        if cluster_id not in self.prophet_models:\n",
    "            return np.zeros(forecast_steps)\n",
    "        \n",
    "        try:\n",
    "            # Ensure last_date is timezone-naive\n",
    "            if last_date.tz is not None:\n",
    "                last_date = last_date.tz_localize(None)\n",
    "            \n",
    "            # Create future dates\n",
    "            future_dates = pd.date_range(\n",
    "                start=last_date + pd.Timedelta(days=1),\n",
    "                periods=forecast_steps,\n",
    "                freq='D'\n",
    "            )\n",
    "            \n",
    "            future_df = pd.DataFrame({'ds': future_dates})\n",
    "            \n",
    "            # Suppress Prophet's verbose output during prediction\n",
    "            import logging\n",
    "            logging.getLogger('prophet').setLevel(logging.WARNING)\n",
    "            \n",
    "            forecast = self.prophet_models[cluster_id].predict(future_df)\n",
    "            \n",
    "            return forecast['yhat'].values\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in Prophet prediction for cluster {cluster_id}: {e}\")\n",
    "            return np.zeros(forecast_steps)\n",
    "    \n",
    "    def calculate_ensemble_weights(self, validation_data: pd.DataFrame, \n",
    "                                 target_col: str) -> None:\n",
    "        \"\"\"\n",
    "        Calculate optimal ensemble weights based on validation performance.\n",
    "        \n",
    "        Args:\n",
    "            validation_data (pd.DataFrame): Validation dataset\n",
    "            target_col (str): Target column name\n",
    "        \"\"\"\n",
    "        print(\"Calculating ensemble weights...\")\n",
    "        \n",
    "        clusters = validation_data['cluster'].unique()\n",
    "        cluster_weights = {}\n",
    "        \n",
    "        for cluster_id in clusters:\n",
    "            cluster_data = validation_data[validation_data['cluster'] == cluster_id]\n",
    "            \n",
    "            if len(cluster_data) < 5:\n",
    "                continue\n",
    "            \n",
    "            lstm_weight = 0.4\n",
    "            arima_weight = 0.3 if cluster_id in self.arima_models else 0.0\n",
    "            prophet_weight = 0.3 if cluster_id in self.prophet_models else 0.0\n",
    "            \n",
    "            # Normalize weights\n",
    "            total_weight = lstm_weight + arima_weight + prophet_weight\n",
    "            if total_weight > 0:\n",
    "                cluster_weights[cluster_id] = {\n",
    "                    'lstm': lstm_weight / total_weight,\n",
    "                    'arima': arima_weight / total_weight,\n",
    "                    'prophet': prophet_weight / total_weight\n",
    "                }\n",
    "        \n",
    "        self.ensemble_weights = cluster_weights\n",
    "        print(\"Ensemble weights calculated\")\n",
    "    \n",
    "    def ensemble_forecast(self, trend_data: pd.DataFrame, forecast_steps: int = 30,\n",
    "                        target_col: str = 'combined_trending_score',\n",
    "                        feature_cols: List[str] = None,\n",
    "                        sequence_length: int = 7) -> Dict[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate ensemble forecasts combining all models.\n",
    "        \n",
    "        Args:\n",
    "            trend_data (pd.DataFrame): Historical trend data\n",
    "            forecast_steps (int): Number of steps to forecast\n",
    "            target_col (str): Target column to predict\n",
    "            feature_cols (List[str]): Feature columns for LSTM\n",
    "            sequence_length (int): Sequence length for LSTM\n",
    "            \n",
    "        Returns:\n",
    "            Dict[int, np.ndarray]: Ensemble predictions for each cluster\n",
    "        \"\"\"\n",
    "        if not self.models_trained:\n",
    "            print(\"Models not trained. Call train_ensemble_models first.\")\n",
    "            return {}\n",
    "        \n",
    "        print(\"Generating ensemble forecasts...\")\n",
    "        \n",
    "        if feature_cols is None:\n",
    "            feature_cols = [\n",
    "                'comment_count', 'comment_likes', 'avg_sentiment', 'video_count',\n",
    "                'total_views', 'video_likes', 'avg_trending_score', 'avg_engagement_rate',\n",
    "                'avg_tag_count', 'avg_tag_relevance'\n",
    "            ]\n",
    "        \n",
    "        available_features = [col for col in feature_cols if col in trend_data.columns]\n",
    "        ensemble_predictions = {}\n",
    "        \n",
    "        last_date = pd.to_datetime(trend_data['date'].max())\n",
    "        clusters = trend_data['cluster'].unique()\n",
    "        \n",
    "        for cluster_id in clusters:\n",
    "            cluster_data = trend_data[trend_data['cluster'] == cluster_id].sort_values('date')\n",
    "            \n",
    "            if len(cluster_data) < sequence_length:\n",
    "                continue\n",
    "            \n",
    "            # Get individual model predictions\n",
    "            lstm_pred = self.predict_lstm(\n",
    "                cluster_data, target_col, available_features, \n",
    "                sequence_length, forecast_steps\n",
    "            )\n",
    "            \n",
    "            arima_pred = self.predict_arima(cluster_id, forecast_steps)\n",
    "            prophet_pred = self.predict_prophet(cluster_id, forecast_steps, last_date)\n",
    "            \n",
    "            # Combine predictions using ensemble weights\n",
    "            if cluster_id in self.ensemble_weights:\n",
    "                weights = self.ensemble_weights[cluster_id]\n",
    "                ensemble_pred = (\n",
    "                    weights['lstm'] * lstm_pred +\n",
    "                    weights['arima'] * arima_pred +\n",
    "                    weights['prophet'] * prophet_pred\n",
    "                )\n",
    "            else:\n",
    "                # Equal weights if no specific weights calculated\n",
    "                ensemble_pred = (lstm_pred + arima_pred + prophet_pred) / 3\n",
    "            \n",
    "            # Ensure non-negative predictions\n",
    "            ensemble_pred = np.maximum(ensemble_pred, 0)\n",
    "            ensemble_predictions[cluster_id] = ensemble_pred\n",
    "        \n",
    "        print(\"Ensemble forecasting completed\")\n",
    "        return ensemble_predictions\n",
    "    \n",
    "    def evaluate_models(self, test_data: pd.DataFrame, \n",
    "                       target_col: str = 'combined_trending_score') -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate individual models and ensemble performance.\n",
    "        \n",
    "        Args:\n",
    "            test_data (pd.DataFrame): Test dataset\n",
    "            target_col (str): Target column name\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Performance metrics for each model\n",
    "        \"\"\"\n",
    "        evaluation_results = {}\n",
    "        \n",
    "        clusters = test_data['cluster'].unique()\n",
    "        \n",
    "        for cluster_id in clusters:\n",
    "            cluster_data = test_data[test_data['cluster'] == cluster_id]\n",
    "            \n",
    "            if len(cluster_data) < 5:\n",
    "                continue\n",
    "            \n",
    "            actual_values = cluster_data[target_col].values\n",
    "            \n",
    "            cluster_results = {\n",
    "                'lstm_mae': 0.0,\n",
    "                'arima_mae': 0.0,\n",
    "                'prophet_mae': 0.0,\n",
    "                'ensemble_mae': 0.0,\n",
    "                'lstm_rmse': 0.0,\n",
    "                'arima_rmse': 0.0,\n",
    "                'prophet_rmse': 0.0,\n",
    "                'ensemble_rmse': 0.0\n",
    "            }\n",
    "            \n",
    "            evaluation_results[cluster_id] = cluster_results\n",
    "        \n",
    "        return evaluation_results\n",
    "\n",
    "\n",
    "class EnhancedTrendAI(TrendAI):\n",
    "    \"\"\"\n",
    "    Enhanced YouTube Trend Predictor with Ensemble Forecasting capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        super().__init__(model_name)\n",
    "        self.ensemble_forecaster = EnsembleForecaster()\n",
    "        \n",
    "    def train_ensemble_forecasting_models(self, forecast_days: int = 30) -> None:\n",
    "        \"\"\"\n",
    "        Train the ensemble forecasting models.\n",
    "        \n",
    "        Args:\n",
    "            forecast_days (int): Number of days to forecast\n",
    "        \"\"\"\n",
    "        print(\"Training ensemble forecasting models...\")\n",
    "        \n",
    "        if self.combined_trend_data is None or self.combined_trend_data.empty:\n",
    "            print(\"No trend data available. Run prepare_time_series_data first.\")\n",
    "            return\n",
    "        \n",
    "        # Train ensemble models\n",
    "        self.ensemble_forecaster.train_ensemble_models(\n",
    "            self.combined_trend_data,\n",
    "            target_col='combined_trending_score'\n",
    "        )\n",
    "        \n",
    "        # Calculate ensemble weights using recent data as validation\n",
    "        validation_cutoff = self.combined_trend_data['date'].max() - pd.Timedelta(days=14)\n",
    "        validation_data = self.combined_trend_data[\n",
    "            self.combined_trend_data['date'] >= validation_cutoff\n",
    "        ]\n",
    "        \n",
    "        if not validation_data.empty:\n",
    "            self.ensemble_forecaster.calculate_ensemble_weights(\n",
    "                validation_data, 'combined_trending_score'\n",
    "            )\n",
    "    \n",
    "    def generate_ensemble_forecasts(self, forecast_days: int = 30) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate ensemble forecasts for future trends.\n",
    "        \n",
    "        Args:\n",
    "            forecast_days (int): Number of days to forecast\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Forecast results with confidence intervals\n",
    "        \"\"\"\n",
    "        print(f\"Generating ensemble forecasts for {forecast_days} days...\")\n",
    "        \n",
    "        if self.combined_trend_data is None or self.combined_trend_data.empty:\n",
    "            print(\"No trend data available for forecasting\")\n",
    "            return {}\n",
    "        \n",
    "        # Generate ensemble predictions\n",
    "        ensemble_predictions = self.ensemble_forecaster.ensemble_forecast(\n",
    "            self.combined_trend_data,\n",
    "            forecast_steps=forecast_days\n",
    "        )\n",
    "        \n",
    "        # Create forecast results with metadata\n",
    "        last_date = pd.to_datetime(self.combined_trend_data['date'].max())\n",
    "        future_dates = pd.date_range(\n",
    "            start=last_date + pd.Timedelta(days=1),\n",
    "            periods=forecast_days,\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        forecast_results = {}\n",
    "        for cluster_id, predictions in ensemble_predictions.items():\n",
    "            topic_words = self.cluster_topics.get(cluster_id, [])[:5]\n",
    "            topic_tags = self.cluster_tags.get(cluster_id, [])[:3]\n",
    "            \n",
    "            forecast_results[cluster_id] = {\n",
    "                'topic_words': topic_words,\n",
    "                'topic_tags': topic_tags,\n",
    "                'dates': future_dates.tolist(),\n",
    "                'predictions': predictions.tolist(),\n",
    "                'prediction_mean': float(np.mean(predictions)),\n",
    "                'prediction_trend': 'increasing' if predictions[-1] > predictions[0] else 'decreasing',\n",
    "                'confidence_level': 'medium',  # Could implement actual confidence intervals\n",
    "                'forecast_category': self._categorize_forecast(predictions)\n",
    "            }\n",
    "        \n",
    "        return forecast_results\n",
    "    \n",
    "    def _categorize_forecast(self, predictions: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Categorize forecast based on prediction patterns.\n",
    "        \n",
    "        Args:\n",
    "            predictions (np.ndarray): Forecast predictions\n",
    "            \n",
    "        Returns:\n",
    "            str: Forecast category\n",
    "        \"\"\"\n",
    "        if len(predictions) < 2:\n",
    "            return 'stable'\n",
    "        \n",
    "        trend_slope = (predictions[-1] - predictions[0]) / len(predictions)\n",
    "        max_val = np.max(predictions)\n",
    "        min_val = np.min(predictions)\n",
    "        volatility = np.std(predictions) / np.mean(predictions) if np.mean(predictions) > 0 else 0\n",
    "        \n",
    "        if trend_slope > 0.05 and max_val > predictions[0] * 1.2:\n",
    "            return 'rapid_growth'\n",
    "        elif trend_slope > 0.01:\n",
    "            return 'steady_growth'\n",
    "        elif trend_slope < -0.05:\n",
    "            return 'declining'\n",
    "        elif volatility > 0.3:\n",
    "            return 'volatile'\n",
    "        else:\n",
    "            return 'stable'\n",
    "    def generate_multi_horizon_forecasts(self, forecast_horizons: List[int] = [20, 30, 40, 60, 80]) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate forecasts for multiple time horizons.\n",
    "        \n",
    "        Args:\n",
    "            forecast_horizons (List[int]): List of forecast horizons in days\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Forecasts organized by horizon and cluster\n",
    "        \"\"\"\n",
    "        print(f\"Generating multi-horizon forecasts for {len(forecast_horizons)} horizons...\")\n",
    "        \n",
    "        multi_horizon_results = {}\n",
    "        \n",
    "        for horizon in forecast_horizons:\n",
    "            print(f\"Generating {horizon}-day forecasts...\")\n",
    "            \n",
    "            # Generate ensemble forecasts for this horizon\n",
    "            forecast_results = self.generate_ensemble_forecasts(forecast_days=horizon)\n",
    "            \n",
    "            # Calculate additional metrics for this horizon\n",
    "            horizon_metrics = self._calculate_horizon_metrics(forecast_results, horizon)\n",
    "            \n",
    "            multi_horizon_results[horizon] = {\n",
    "                'forecasts': forecast_results,\n",
    "                'metrics': horizon_metrics,\n",
    "                'horizon_days': horizon\n",
    "            }\n",
    "        \n",
    "        return multi_horizon_results\n",
    "\n",
    "    def identify_generational_trending_topics(self, window_days: int = 30) -> Dict:\n",
    "        \"\"\"\n",
    "        Identify trending topics by generation.\n",
    "        \n",
    "        Args:\n",
    "            window_days (int): Number of recent days to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Trending topics organized by generation\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'generational_clusters') or self.generational_clusters is None:\n",
    "            self.analyze_generational_trends_by_cluster()\n",
    "        \n",
    "        recent_date = self.comments_df['publishedAt'].max() - timedelta(days=window_days)\n",
    "        recent_comments = self.comments_df[self.comments_df['publishedAt'] >= recent_date]\n",
    "        \n",
    "        generational_trends = {\n",
    "            'gen_z': [],\n",
    "            'millennial': [],\n",
    "            'gen_x': [],\n",
    "            'boomer': [],\n",
    "            'neutral': []\n",
    "        }\n",
    "        \n",
    "        for cluster_id, analysis in self.generational_clusters.items():\n",
    "            dominant_gen = analysis['dominant_generation']\n",
    "            \n",
    "            # Calculate trend metrics for this cluster\n",
    "            cluster_recent_comments = recent_comments[recent_comments['cluster'] == cluster_id]\n",
    "            \n",
    "            if len(cluster_recent_comments) > 0:\n",
    "                # Calculate engagement and growth metrics\n",
    "                avg_sentiment = cluster_recent_comments['compound'].mean() if 'compound' in cluster_recent_comments.columns else 0\n",
    "                comment_volume = len(cluster_recent_comments)\n",
    "                avg_likes = cluster_recent_comments['likeCount'].mean()\n",
    "                \n",
    "                # Get video performance for this generation\n",
    "                video_perf = analysis['video_performance_by_generation'].get(dominant_gen, {})\n",
    "                \n",
    "                trend_data = {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'topic_words': analysis['topic_words'][:5],\n",
    "                    'topic_tags': analysis['topic_tags'][:3],\n",
    "                    'dominant_generation': dominant_gen,\n",
    "                    'generation_confidence': analysis['dominant_generation_score'],\n",
    "                    'recent_comment_volume': comment_volume,\n",
    "                    'avg_sentiment': avg_sentiment,\n",
    "                    'avg_comment_likes': avg_likes,\n",
    "                    'generation_distribution': analysis['generation_distribution'],\n",
    "                    'video_performance': video_perf\n",
    "                }\n",
    "                \n",
    "                generational_trends[dominant_gen].append(trend_data)\n",
    "        \n",
    "        # Sort each generation's trends by relevance\n",
    "        for generation in generational_trends:\n",
    "            generational_trends[generation].sort(\n",
    "                key=lambda x: (x['recent_comment_volume'] * (1 + x['avg_sentiment'])), \n",
    "                reverse=True\n",
    "            )\n",
    "        \n",
    "        self.generational_trends = generational_trends\n",
    "        return generational_trends\n",
    "\n",
    "    def visualize_generational_trends(self) -> None:\n",
    "        \"\"\"\n",
    "        Create visualizations for generational trend analysis.\n",
    "        \"\"\"\n",
    "        if self.generational_trends is None:\n",
    "            self.identify_generational_trending_topics()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        fig.suptitle('Generational Trend Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Colors for generations\n",
    "        gen_colors = {\n",
    "            'gen_z': '#FF6B6B',\n",
    "            'millennial': '#4ECDC4', \n",
    "            'gen_x': '#45B7D1',\n",
    "            'boomer': '#96CEB4',\n",
    "            'neutral': '#FFEAA7'\n",
    "        }\n",
    "        \n",
    "        # Plot 1: Generation Distribution Across All Clusters\n",
    "        ax1 = axes[0, 0]\n",
    "        if self.generational_clusters:\n",
    "            gen_counts = {}\n",
    "            for cluster_data in self.generational_clusters.values():\n",
    "                dominant_gen = cluster_data['dominant_generation']\n",
    "                gen_counts[dominant_gen] = gen_counts.get(dominant_gen, 0) + 1\n",
    "            \n",
    "            colors = [gen_colors.get(gen, 'gray') for gen in gen_counts.keys()]\n",
    "            bars = ax1.bar(gen_counts.keys(), gen_counts.values(), color=colors)\n",
    "            ax1.set_title('Dominant Generation by Cluster', fontweight='bold')\n",
    "            ax1.set_ylabel('Number of Clusters')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, gen_counts.values()):\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                        str(value), ha='center', va='bottom')\n",
    "        \n",
    "        # Plot 2: Comment Volume by Generation\n",
    "        ax2 = axes[0, 1]\n",
    "        gen_comment_volumes = {}\n",
    "        for gen, trends in self.generational_trends.items():\n",
    "            total_comments = sum(trend['recent_comment_volume'] for trend in trends)\n",
    "            if total_comments > 0:\n",
    "                gen_comment_volumes[gen] = total_comments\n",
    "        \n",
    "        if gen_comment_volumes:\n",
    "            colors = [gen_colors.get(gen, 'gray') for gen in gen_comment_volumes.keys()]\n",
    "            ax2.pie(gen_comment_volumes.values(), labels=gen_comment_volumes.keys(), \n",
    "                    colors=colors, autopct='%1.1f%%')\n",
    "            ax2.set_title('Comment Volume Distribution', fontweight='bold')\n",
    "        \n",
    "        # Plot 3: Average Sentiment by Generation\n",
    "        ax3 = axes[0, 2]\n",
    "        gen_sentiments = {}\n",
    "        for gen, trends in self.generational_trends.items():\n",
    "            if trends:\n",
    "                avg_sentiment = np.mean([trend['avg_sentiment'] for trend in trends if trend['avg_sentiment'] != 0])\n",
    "                if not np.isnan(avg_sentiment):\n",
    "                    gen_sentiments[gen] = avg_sentiment\n",
    "        \n",
    "        if gen_sentiments:\n",
    "            colors = [gen_colors.get(gen, 'gray') for gen in gen_sentiments.keys()]\n",
    "            bars = ax3.bar(gen_sentiments.keys(), gen_sentiments.values(), color=colors)\n",
    "            ax3.set_title('Average Sentiment by Generation', fontweight='bold')\n",
    "            ax3.set_ylabel('Sentiment Score')\n",
    "            ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, gen_sentiments.values()):\n",
    "                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01 if value >= 0 else bar.get_height() - 0.01,\n",
    "                        f'{value:.3f}', ha='center', va='bottom' if value >= 0 else 'top')\n",
    "        \n",
    "        # Plot 4: Top Topics by Generation (Gen Z)\n",
    "        ax4 = axes[1, 0]\n",
    "        gen_z_trends = self.generational_trends.get('gen_z', [])[:5]\n",
    "        if gen_z_trends:\n",
    "            topics = [', '.join(trend['topic_words'][:2]) for trend in gen_z_trends]\n",
    "            volumes = [trend['recent_comment_volume'] for trend in gen_z_trends]\n",
    "            \n",
    "            ax4.barh(range(len(topics)), volumes, color=gen_colors['gen_z'])\n",
    "            ax4.set_yticks(range(len(topics)))\n",
    "            ax4.set_yticklabels(topics, fontsize=10)\n",
    "            ax4.set_title('Top Gen Z Topics', fontweight='bold')\n",
    "            ax4.set_xlabel('Recent Comment Volume')\n",
    "        \n",
    "        # Plot 5: Top Topics by Generation (Millennial)\n",
    "        ax5 = axes[1, 1]\n",
    "        millennial_trends = self.generational_trends.get('millennial', [])[:5]\n",
    "        if millennial_trends:\n",
    "            topics = [', '.join(trend['topic_words'][:2]) for trend in millennial_trends]\n",
    "            volumes = [trend['recent_comment_volume'] for trend in millennial_trends]\n",
    "            \n",
    "            ax5.barh(range(len(topics)), volumes, color=gen_colors['millennial'])\n",
    "            ax5.set_yticks(range(len(topics)))\n",
    "            ax5.set_yticklabels(topics, fontsize=10)\n",
    "            ax5.set_title('Top Millennial Topics', fontweight='bold')\n",
    "            ax5.set_xlabel('Recent Comment Volume')\n",
    "        \n",
    "        # Plot 6: Generational Language Intensity Heatmap\n",
    "        ax6 = axes[1, 2]\n",
    "        if self.generational_clusters:\n",
    "            # Create heatmap data\n",
    "            heatmap_data = []\n",
    "            cluster_labels = []\n",
    "            \n",
    "            for cluster_id, analysis in list(self.generational_clusters.items())[:10]:  # Top 10 clusters\n",
    "                scores = [analysis['avg_generational_scores'][gen] for gen in ['gen_z', 'millennial', 'gen_x', 'boomer']]\n",
    "                heatmap_data.append(scores)\n",
    "                topic_label = ', '.join(analysis['topic_words'][:2])\n",
    "                cluster_labels.append(f\"C{cluster_id}: {topic_label}\"[:25])\n",
    "            \n",
    "            if heatmap_data:\n",
    "                sns.heatmap(heatmap_data, \n",
    "                           xticklabels=['Gen Z', 'Millennial', 'Gen X', 'Boomer'],\n",
    "                           yticklabels=cluster_labels,\n",
    "                           annot=True, fmt='.3f', cmap='YlOrRd', ax=ax6)\n",
    "                ax6.set_title('Generational Language Intensity', fontweight='bold')\n",
    "                ax6.set_xlabel('Generation')\n",
    "                ax6.tick_params(axis='y', labelsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_horizon_metrics(self, forecast_results: Dict, horizon: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate trend-related metrics for a specific forecast horizon.\n",
    "        \n",
    "        Args:\n",
    "            forecast_results (Dict): Forecast results for clusters\n",
    "            horizon (int): Forecast horizon in days\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Calculated metrics\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'total_clusters': len(forecast_results),\n",
    "            'avg_prediction_mean': 0.0,\n",
    "            'total_predicted_growth': 0.0,\n",
    "            'volatility_score': 0.0,\n",
    "            'trend_strength': 0.0,\n",
    "            'category_distribution': {},\n",
    "            'cluster_metrics': {}\n",
    "        }\n",
    "        \n",
    "        if not forecast_results:\n",
    "            return metrics\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        prediction_means = []\n",
    "        growth_rates = []\n",
    "        volatilities = []\n",
    "        categories = []\n",
    "        \n",
    "        for cluster_id, results in forecast_results.items():\n",
    "            predictions = np.array(results['predictions'])\n",
    "            \n",
    "            # Basic metrics\n",
    "            pred_mean = np.mean(predictions)\n",
    "            prediction_means.append(pred_mean)\n",
    "            \n",
    "            # Growth rate (start to end)\n",
    "            if len(predictions) > 1:\n",
    "                growth_rate = (predictions[-1] - predictions[0]) / predictions[0] if predictions[0] > 0 else 0\n",
    "                growth_rates.append(growth_rate)\n",
    "            \n",
    "            # Volatility\n",
    "            volatility = np.std(predictions) / pred_mean if pred_mean > 0 else 0\n",
    "            volatilities.append(volatility)\n",
    "            \n",
    "            # Category\n",
    "            category = results['forecast_category']\n",
    "            categories.append(category)\n",
    "            \n",
    "            # Trend strength (correlation with linear trend)\n",
    "            x = np.arange(len(predictions))\n",
    "            if len(predictions) > 2:\n",
    "                correlation = np.corrcoef(x, predictions)[0, 1] if np.std(predictions) > 0 else 0\n",
    "                trend_strength = abs(correlation)\n",
    "            else:\n",
    "                trend_strength = 0\n",
    "            \n",
    "            # Store individual cluster metrics\n",
    "            metrics['cluster_metrics'][cluster_id] = {\n",
    "                'prediction_mean': pred_mean,\n",
    "                'growth_rate': growth_rate if len(predictions) > 1 else 0,\n",
    "                'volatility': volatility,\n",
    "                'trend_strength': trend_strength,\n",
    "                'max_value': np.max(predictions),\n",
    "                'min_value': np.min(predictions),\n",
    "                'final_value': predictions[-1] if len(predictions) > 0 else 0,\n",
    "                'topic_words': results.get('topic_words', [])\n",
    "            }\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        metrics['avg_prediction_mean'] = np.mean(prediction_means) if prediction_means else 0\n",
    "        metrics['total_predicted_growth'] = np.sum(growth_rates) if growth_rates else 0\n",
    "        metrics['volatility_score'] = np.mean(volatilities) if volatilities else 0\n",
    "        metrics['trend_strength'] = np.mean([m['trend_strength'] for m in metrics['cluster_metrics'].values()])\n",
    "        \n",
    "        # Category distribution\n",
    "        category_counts = pd.Series(categories).value_counts().to_dict()\n",
    "        metrics['category_distribution'] = category_counts\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    \n",
    "    def visualize_ensemble_forecasts(self, forecast_results: Dict, top_n: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Visualize ensemble forecast results.\n",
    "        \n",
    "        Args:\n",
    "            forecast_results (Dict): Forecast results from generate_ensemble_forecasts\n",
    "            top_n (int): Number of top clusters to visualize\n",
    "        \"\"\"\n",
    "        print(\"Creating ensemble forecast visualizations...\")\n",
    "        \n",
    "        if not forecast_results:\n",
    "            print(\"No forecast results to visualize\")\n",
    "            return\n",
    "        \n",
    "        # Sort clusters by prediction mean\n",
    "        sorted_clusters = sorted(\n",
    "            forecast_results.items(),\n",
    "            key=lambda x: x[1]['prediction_mean'],\n",
    "            reverse=True\n",
    "        )[:top_n]\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Ensemble Forecasting Results', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Individual Forecast Lines\n",
    "        ax1 = axes[0, 0]\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(sorted_clusters)))\n",
    "        \n",
    "        for i, (cluster_id, results) in enumerate(sorted_clusters):\n",
    "            topic_label = ', '.join(results['topic_words'][:2])\n",
    "            dates = pd.to_datetime(results['dates'])\n",
    "            predictions = results['predictions']\n",
    "            \n",
    "            ax1.plot(dates, predictions, color=colors[i], linewidth=2, \n",
    "                    marker='o', label=f\"{topic_label} (C{cluster_id})\")\n",
    "        \n",
    "        ax1.set_title('Individual Cluster Forecasts', fontweight='bold')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel('Predicted Trending Score')\n",
    "        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 2: Forecast Categories\n",
    "        ax2 = axes[0, 1]\n",
    "        categories = [results['forecast_category'] for _, results in sorted_clusters]\n",
    "        category_counts = pd.Series(categories).value_counts()\n",
    "        \n",
    "        ax2.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
    "        ax2.set_title('Forecast Categories Distribution', fontweight='bold')\n",
    "        \n",
    "        # Plot 3: Prediction Means Comparison\n",
    "        ax3 = axes[1, 0]\n",
    "        cluster_names = [', '.join(results['topic_words'][:2]) for _, results in sorted_clusters]\n",
    "        prediction_means = [results['prediction_mean'] for _, results in sorted_clusters]\n",
    "        \n",
    "        bars = ax3.bar(range(len(cluster_names)), prediction_means, color='skyblue')\n",
    "        ax3.set_xticks(range(len(cluster_names)))\n",
    "        ax3.set_xticklabels(cluster_names, rotation=45, ha='right')\n",
    "        ax3.set_title('Average Predicted Trending Scores', fontweight='bold')\n",
    "        ax3.set_ylabel('Average Trending Score')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, prediction_means):\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Plot 4: Trend Directions\n",
    "        ax4 = axes[1, 1]\n",
    "        trend_directions = [results['prediction_trend'] for _, results in sorted_clusters]\n",
    "        trend_counts = pd.Series(trend_directions).value_counts()\n",
    "        \n",
    "        colors_trend = ['green' if trend == 'increasing' else 'red' for trend in trend_counts.index]\n",
    "        ax4.bar(trend_counts.index, trend_counts.values, color=colors_trend)\n",
    "        ax4.set_title('Forecast Trend Directions', fontweight='bold')\n",
    "        ax4.set_ylabel('Number of Clusters')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_multi_horizon_analysis(self, multi_horizon_results: Dict, top_clusters: int = 8) -> None:\n",
    "        \"\"\"\n",
    "        Create comprehensive visualizations for multi-horizon forecasts.\n",
    "        \n",
    "        Args:\n",
    "            multi_horizon_results (Dict): Results from generate_multi_horizon_forecasts\n",
    "            top_clusters (int): Number of top clusters to highlight\n",
    "        \"\"\"\n",
    "        if not multi_horizon_results:\n",
    "            print(\"No multi-horizon results to visualize\")\n",
    "            return\n",
    "        \n",
    "        horizons = sorted(multi_horizon_results.keys())\n",
    "        \n",
    "        # Create multiple visualization sets\n",
    "        self._plot_horizon_trend_metrics(multi_horizon_results, horizons)\n",
    "        self._plot_cluster_performance_across_horizons(multi_horizon_results, horizons, top_clusters)\n",
    "        self._plot_forecast_uncertainty_analysis(multi_horizon_results, horizons)\n",
    "        self._plot_category_evolution_across_horizons(multi_horizon_results, horizons)\n",
    "\n",
    "        self.plot_cluster_growth_by_horizon(multi_horizon_results, horizons)\n",
    "        self.plot_cluster_comparison_across_horizons(multi_horizon_results, top_n=top_clusters)\n",
    "    \n",
    "    def _plot_horizon_trend_metrics(self, multi_horizon_results: Dict, horizons: List[int]) -> None:\n",
    "        \"\"\"Plot trend metrics across different forecast horizons.\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Trend Metrics Across Forecast Horizons', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Extract metrics for each horizon\n",
    "        metrics_data = {\n",
    "            'horizons': horizons,\n",
    "            'avg_prediction_mean': [multi_horizon_results[h]['metrics']['avg_prediction_mean'] for h in horizons],\n",
    "            'total_predicted_growth': [multi_horizon_results[h]['metrics']['total_predicted_growth'] for h in horizons],\n",
    "            'volatility_score': [multi_horizon_results[h]['metrics']['volatility_score'] for h in horizons],\n",
    "            'trend_strength': [multi_horizon_results[h]['metrics']['trend_strength'] for h in horizons],\n",
    "            'total_clusters': [multi_horizon_results[h]['metrics']['total_clusters'] for h in horizons]\n",
    "        }\n",
    "        \n",
    "        # Plot 1: Average Prediction Mean\n",
    "        axes[0, 0].plot(horizons, metrics_data['avg_prediction_mean'], marker='o', linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_title('Average Prediction Mean vs Horizon', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Forecast Horizon (days)')\n",
    "        axes[0, 0].set_ylabel('Average Prediction Mean')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Total Predicted Growth\n",
    "        axes[0, 1].bar(horizons, metrics_data['total_predicted_growth'], color='green', alpha=0.7)\n",
    "        axes[0, 1].set_title('Total Predicted Growth vs Horizon', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Forecast Horizon (days)')\n",
    "        axes[0, 1].set_ylabel('Total Predicted Growth')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Volatility Score\n",
    "        axes[0, 2].plot(horizons, metrics_data['volatility_score'], marker='s', color='orange', linewidth=2, markersize=8)\n",
    "        axes[0, 2].set_title('Forecast Volatility vs Horizon', fontweight='bold')\n",
    "        axes[0, 2].set_xlabel('Forecast Horizon (days)')\n",
    "        axes[0, 2].set_ylabel('Volatility Score')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Trend Strength\n",
    "        axes[1, 0].plot(horizons, metrics_data['trend_strength'], marker='^', color='red', linewidth=2, markersize=8)\n",
    "        axes[1, 0].set_title('Trend Strength vs Horizon', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Forecast Horizon (days)')\n",
    "        axes[1, 0].set_ylabel('Trend Strength')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Number of Clusters Forecasted\n",
    "        axes[1, 1].bar(horizons, metrics_data['total_clusters'], color='purple', alpha=0.7)\n",
    "        axes[1, 1].set_title('Clusters Forecasted vs Horizon', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Forecast Horizon (days)')\n",
    "        axes[1, 1].set_ylabel('Number of Clusters')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 6: Horizon Comparison Radar\n",
    "        axes[1, 2].remove()  # Remove this axis for a custom radar chart\n",
    "        ax_radar = fig.add_subplot(2, 3, 6, projection='polar')\n",
    "        \n",
    "        # Normalize metrics for radar chart\n",
    "        normalized_metrics = []\n",
    "        metric_names = ['Avg Prediction', 'Growth Rate', 'Volatility', 'Trend Strength']\n",
    "        \n",
    "        for i, horizon in enumerate([horizons[0], horizons[-1]]):  # Compare first and last horizon\n",
    "            norm_pred = metrics_data['avg_prediction_mean'][horizons.index(horizon)] / max(metrics_data['avg_prediction_mean']) if max(metrics_data['avg_prediction_mean']) > 0 else 0\n",
    "            norm_growth = abs(metrics_data['total_predicted_growth'][horizons.index(horizon)]) / max([abs(x) for x in metrics_data['total_predicted_growth']]) if max([abs(x) for x in metrics_data['total_predicted_growth']]) > 0 else 0\n",
    "            norm_vol = metrics_data['volatility_score'][horizons.index(horizon)] / max(metrics_data['volatility_score']) if max(metrics_data['volatility_score']) > 0 else 0\n",
    "            norm_trend = metrics_data['trend_strength'][horizons.index(horizon)]\n",
    "            \n",
    "            normalized_metrics.append([norm_pred, norm_growth, norm_vol, norm_trend])\n",
    "        \n",
    "        angles = np.linspace(0, 2 * np.pi, len(metric_names), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        colors = ['blue', 'red']\n",
    "        labels = [f'{horizons[0]} days', f'{horizons[-1]} days']\n",
    "        \n",
    "        for i, (metrics, color, label) in enumerate(zip(normalized_metrics, colors, labels)):\n",
    "            metrics += metrics[:1]  # Complete the circle\n",
    "            ax_radar.plot(angles, metrics, 'o-', linewidth=2, label=label, color=color)\n",
    "            ax_radar.fill(angles, metrics, alpha=0.25, color=color)\n",
    "        \n",
    "        ax_radar.set_xticks(angles[:-1])\n",
    "        ax_radar.set_xticklabels(metric_names)\n",
    "        ax_radar.set_title('Horizon Comparison (Normalized)', fontweight='bold', pad=20)\n",
    "        ax_radar.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_cluster_performance_across_horizons(self, multi_horizon_results: Dict, horizons: List[int], top_clusters: int) -> None:\n",
    "        \"\"\"Plot performance of top clusters across different horizons.\"\"\"\n",
    "        \n",
    "        # Identify top clusters based on average performance across all horizons\n",
    "        cluster_avg_performance = {}\n",
    "        all_clusters = set()\n",
    "        \n",
    "        for horizon in horizons:\n",
    "            for cluster_id, metrics in multi_horizon_results[horizon]['metrics']['cluster_metrics'].items():\n",
    "                all_clusters.add(cluster_id)\n",
    "                if cluster_id not in cluster_avg_performance:\n",
    "                    cluster_avg_performance[cluster_id] = []\n",
    "                cluster_avg_performance[cluster_id].append(metrics['prediction_mean'])\n",
    "        \n",
    "        # Calculate average performance and get top clusters\n",
    "        for cluster_id in cluster_avg_performance:\n",
    "            cluster_avg_performance[cluster_id] = np.mean(cluster_avg_performance[cluster_id])\n",
    "        \n",
    "        top_cluster_ids = sorted(cluster_avg_performance.items(), key=lambda x: x[1], reverse=True)[:top_clusters]\n",
    "        top_cluster_ids = [cid for cid, _ in top_cluster_ids]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle(f'Top {top_clusters} Cluster Performance Across Horizons', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(top_cluster_ids)))\n",
    "        \n",
    "        # Plot 1: Prediction Mean Evolution\n",
    "        ax1 = axes[0, 0]\n",
    "        for i, cluster_id in enumerate(top_cluster_ids):\n",
    "            means = []\n",
    "            for horizon in horizons:\n",
    "                if cluster_id in multi_horizon_results[horizon]['metrics']['cluster_metrics']:\n",
    "                    means.append(multi_horizon_results[horizon]['metrics']['cluster_metrics'][cluster_id]['prediction_mean'])\n",
    "                else:\n",
    "                    means.append(0)\n",
    "            \n",
    "            # Get topic words for legend\n",
    "            topic_words = []\n",
    "            for horizon in horizons:\n",
    "                if cluster_id in multi_horizon_results[horizon]['metrics']['cluster_metrics']:\n",
    "                    topic_words = multi_horizon_results[horizon]['metrics']['cluster_metrics'][cluster_id]['topic_words'][:2]\n",
    "                    break\n",
    "            \n",
    "            label = f\"C{cluster_id}: {', '.join(topic_words)}\" if topic_words else f\"Cluster {cluster_id}\"\n",
    "            ax1.plot(horizons, means, marker='o', color=colors[i], linewidth=2, label=label)\n",
    "        \n",
    "        ax1.set_title('Prediction Mean vs Horizon', fontweight='bold')\n",
    "        ax1.set_xlabel('Forecast Horizon (days)')\n",
    "        ax1.set_ylabel('Prediction Mean')\n",
    "        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Growth Rate Evolution\n",
    "        ax2 = axes[0, 1]\n",
    "        for i, cluster_id in enumerate(top_cluster_ids):\n",
    "            growth_rates = []\n",
    "            for horizon in horizons:\n",
    "                if cluster_id in multi_horizon_results[horizon]['metrics']['cluster_metrics']:\n",
    "                    growth_rates.append(multi_horizon_results[horizon]['metrics']['cluster_metrics'][cluster_id]['growth_rate'])\n",
    "                else:\n",
    "                    growth_rates.append(0)\n",
    "            \n",
    "            ax2.plot(horizons, growth_rates, marker='s', color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax2.set_title('Growth Rate vs Horizon', fontweight='bold')\n",
    "        ax2.set_xlabel('Forecast Horizon (days)')\n",
    "        ax2.set_ylabel('Growth Rate')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Plot 3: Volatility Evolution\n",
    "        ax3 = axes[1, 0]\n",
    "        for i, cluster_id in enumerate(top_cluster_ids):\n",
    "            volatilities = []\n",
    "            for horizon in horizons:\n",
    "                if cluster_id in multi_horizon_results[horizon]['metrics']['cluster_metrics']:\n",
    "                    volatilities.append(multi_horizon_results[horizon]['metrics']['cluster_metrics'][cluster_id]['volatility'])\n",
    "                else:\n",
    "                    volatilities.append(0)\n",
    "            \n",
    "            ax3.plot(horizons, volatilities, marker='^', color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax3.set_title('Volatility vs Horizon', fontweight='bold')\n",
    "        ax3.set_xlabel('Forecast Horizon (days)')\n",
    "        ax3.set_ylabel('Volatility Score')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Trend Strength Evolution\n",
    "        ax4 = axes[1, 1]\n",
    "        for i, cluster_id in enumerate(top_cluster_ids):\n",
    "            trend_strengths = []\n",
    "            for horizon in horizons:\n",
    "                if cluster_id in multi_horizon_results[horizon]['metrics']['cluster_metrics']:\n",
    "                    trend_strengths.append(multi_horizon_results[horizon]['metrics']['cluster_metrics'][cluster_id]['trend_strength'])\n",
    "                else:\n",
    "                    trend_strengths.append(0)\n",
    "            \n",
    "            ax4.plot(horizons, trend_strengths, marker='d', color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax4.set_title('Trend Strength vs Horizon', fontweight='bold')\n",
    "        ax4.set_xlabel('Forecast Horizon (days)')\n",
    "        ax4.set_ylabel('Trend Strength')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_forecast_uncertainty_analysis(self, multi_horizon_results: Dict, horizons: List[int]) -> None:\n",
    "        \"\"\"Analyze and plot forecast uncertainty across horizons.\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "        fig.suptitle('Forecast Uncertainty Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Collect uncertainty metrics\n",
    "        uncertainty_data = {\n",
    "            'horizon': [],\n",
    "            'prediction_std': [],\n",
    "            'growth_rate_std': [],\n",
    "            'max_prediction_range': [],\n",
    "            'coefficient_of_variation': []\n",
    "        }\n",
    "        \n",
    "        for horizon in horizons:\n",
    "            cluster_metrics = multi_horizon_results[horizon]['metrics']['cluster_metrics']\n",
    "            \n",
    "            if not cluster_metrics:\n",
    "                continue\n",
    "            \n",
    "            prediction_means = [m['prediction_mean'] for m in cluster_metrics.values()]\n",
    "            growth_rates = [m['growth_rate'] for m in cluster_metrics.values()]\n",
    "            max_values = [m['max_value'] for m in cluster_metrics.values()]\n",
    "            min_values = [m['min_value'] for m in cluster_metrics.values()]\n",
    "            \n",
    "            uncertainty_data['horizon'].append(horizon)\n",
    "            uncertainty_data['prediction_std'].append(np.std(prediction_means))\n",
    "            uncertainty_data['growth_rate_std'].append(np.std(growth_rates))\n",
    "            uncertainty_data['max_prediction_range'].append(np.mean([max_val - min_val for max_val, min_val in zip(max_values, min_values)]))\n",
    "            \n",
    "            # Coefficient of variation\n",
    "            mean_pred = np.mean(prediction_means)\n",
    "            cv = np.std(prediction_means) / mean_pred if mean_pred > 0 else 0\n",
    "            uncertainty_data['coefficient_of_variation'].append(cv)\n",
    "        \n",
    "        # Plot uncertainty metrics\n",
    "        axes[0, 0].plot(uncertainty_data['horizon'], uncertainty_data['prediction_std'], \n",
    "                        marker='o', color='red', linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_title('Prediction Standard Deviation', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Forecast Horizon (days)')\n",
    "        axes[0, 0].set_ylabel('Standard Deviation')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[0, 1].plot(uncertainty_data['horizon'], uncertainty_data['growth_rate_std'], \n",
    "                        marker='s', color='orange', linewidth=2, markersize=8)\n",
    "        axes[0, 1].set_title('Growth Rate Standard Deviation', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Forecast Horizon (days)')\n",
    "        axes[0, 1].set_ylabel('Standard Deviation')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1, 0].plot(uncertainty_data['horizon'], uncertainty_data['max_prediction_range'], \n",
    "                        marker='^', color='green', linewidth=2, markersize=8)\n",
    "        axes[1, 0].set_title('Average Prediction Range', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Forecast Horizon (days)')\n",
    "        axes[1, 0].set_ylabel('Average Range (Max - Min)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1, 1].plot(uncertainty_data['horizon'], uncertainty_data['coefficient_of_variation'], \n",
    "                        marker='d', color='purple', linewidth=2, markersize=8)\n",
    "        axes[1, 1].set_title('Coefficient of Variation', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Forecast Horizon (days)')\n",
    "        axes[1, 1].set_ylabel('Coefficient of Variation')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_category_evolution_across_horizons(self, multi_horizon_results: Dict, horizons: List[int]) -> None:\n",
    "        \"\"\"Plot how forecast categories evolve across different horizons.\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle('Forecast Category Evolution Across Horizons', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Collect category data\n",
    "        all_categories = set()\n",
    "        category_data = {}\n",
    "        \n",
    "        for horizon in horizons:\n",
    "            category_dist = multi_horizon_results[horizon]['metrics']['category_distribution']\n",
    "            category_data[horizon] = category_dist\n",
    "            all_categories.update(category_dist.keys())\n",
    "        \n",
    "        all_categories = sorted(list(all_categories))\n",
    "        \n",
    "        # Plot 1: Stacked Bar Chart\n",
    "        ax1 = axes[0]\n",
    "        bottom_values = np.zeros(len(horizons))\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(all_categories)))\n",
    "        \n",
    "        for i, category in enumerate(all_categories):\n",
    "            values = []\n",
    "            for horizon in horizons:\n",
    "                values.append(category_data[horizon].get(category, 0))\n",
    "            \n",
    "            ax1.bar(horizons, values, bottom=bottom_values, label=category, color=colors[i])\n",
    "            bottom_values += values\n",
    "        \n",
    "        ax1.set_title('Category Distribution Across Horizons', fontweight='bold')\n",
    "        ax1.set_xlabel('Forecast Horizon (days)')\n",
    "        ax1.set_ylabel('Number of Clusters')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot 2: Line Chart showing category trends\n",
    "        ax2 = axes[1]\n",
    "        for i, category in enumerate(all_categories):\n",
    "            values = []\n",
    "            for horizon in horizons:\n",
    "                values.append(category_data[horizon].get(category, 0))\n",
    "            \n",
    "            ax2.plot(horizons, values, marker='o', linewidth=2, label=category, color=colors[i])\n",
    "        \n",
    "        ax2.set_title('Category Trends Across Horizons', fontweight='bold')\n",
    "        ax2.set_xlabel('Forecast Horizon (days)')\n",
    "        ax2.set_ylabel('Number of Clusters')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_cluster_trend_vs_growth_scatter(self, multi_horizon_results: Dict, horizons: List[int]) -> None:\n",
    "        \"\"\"Plot cluster trend strength vs growth rate scatter plots for each horizon.\"\"\"\n",
    "        \n",
    "        # Determine subplot layout based on number of horizons\n",
    "        n_horizons = len(horizons)\n",
    "        if n_horizons <= 2:\n",
    "            rows, cols = 1, n_horizons\n",
    "            figsize = (8 * n_horizons, 6)\n",
    "        elif n_horizons <= 4:\n",
    "            rows, cols = 2, 2\n",
    "            figsize = (16, 12)\n",
    "        elif n_horizons <= 6:\n",
    "            rows, cols = 2, 3\n",
    "            figsize = (18, 12)\n",
    "        else:\n",
    "            rows, cols = 3, 3\n",
    "            figsize = (18, 18)\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "        fig.suptitle('Cluster Trend Strength vs Growth Rate by Forecast Horizon', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Handle single subplot case\n",
    "        if n_horizons == 1:\n",
    "            axes = [axes]\n",
    "        elif rows == 1:\n",
    "            axes = axes if hasattr(axes, '__len__') else [axes]\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        # Color map for different prediction strength categories\n",
    "        def get_color_by_prediction(prediction_mean, all_predictions):\n",
    "            if not all_predictions:\n",
    "                return 'gray'\n",
    "            percentile_75 = np.percentile(all_predictions, 75)\n",
    "            percentile_25 = np.percentile(all_predictions, 25)\n",
    "            \n",
    "            if prediction_mean >= percentile_75:\n",
    "                return 'red'  # High prediction\n",
    "            elif prediction_mean <= percentile_25:\n",
    "                return 'blue'  # Low prediction\n",
    "            else:\n",
    "                return 'green'  # Medium prediction\n",
    "        \n",
    "        for idx, horizon in enumerate(horizons):\n",
    "            ax = axes[idx]\n",
    "            cluster_metrics = multi_horizon_results[horizon]['metrics']['cluster_metrics']\n",
    "            \n",
    "            if not cluster_metrics:\n",
    "                ax.text(0.5, 0.5, 'No data available', transform=ax.transAxes, \n",
    "                       ha='center', va='center', fontsize=12)\n",
    "                ax.set_title(f'{horizon}-Day Horizon', fontweight='bold')\n",
    "                continue\n",
    "            \n",
    "            # Extract data for this horizon\n",
    "            trend_strengths = []\n",
    "            growth_rates = []\n",
    "            prediction_means = []\n",
    "            cluster_labels = []\n",
    "            topic_words_list = []\n",
    "            \n",
    "            for cluster_id, metrics in cluster_metrics.items():\n",
    "                trend_strengths.append(metrics['trend_strength'])\n",
    "                growth_rates.append(metrics['growth_rate'])\n",
    "                prediction_means.append(metrics['prediction_mean'])\n",
    "                cluster_labels.append(f\"C{cluster_id}\")\n",
    "                topic_words_list.append(metrics.get('topic_words', [])[:2])\n",
    "            \n",
    "            # Create scatter plot with color coding\n",
    "            colors = [get_color_by_prediction(pred, prediction_means) for pred in prediction_means]\n",
    "            sizes = [50 + abs(pred) * 10 for pred in prediction_means]  # Size based on prediction magnitude\n",
    "            \n",
    "            scatter = ax.scatter(trend_strengths, growth_rates, c=colors, s=sizes, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "            \n",
    "            # Add cluster labels\n",
    "            for i, (x, y, label, topics) in enumerate(zip(trend_strengths, growth_rates, cluster_labels, topic_words_list)):\n",
    "                # Only label points that are not too crowded\n",
    "                if len(trend_strengths) <= 20:  # Only add labels if not too many clusters\n",
    "                    topic_str = ', '.join(topics) if topics else ''\n",
    "                    full_label = f\"{label}: {topic_str}\" if topic_str else label\n",
    "                    ax.annotate(full_label, (x, y), xytext=(5, 5), textcoords='offset points', \n",
    "                               fontsize=8, alpha=0.8, bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.7))\n",
    "            \n",
    "            # Customize the plot\n",
    "            ax.set_xlabel('Trend Strength')\n",
    "            ax.set_ylabel('Growth Rate')\n",
    "            ax.set_title(f'{horizon}-Day Horizon', fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "            ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "            \n",
    "            # Add quadrant labels\n",
    "            ax.text(0.02, 0.98, 'Declining\\nTrend', transform=ax.transAxes, fontsize=8, alpha=0.6, \n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round,pad=0.2', facecolor='lightblue', alpha=0.3))\n",
    "            ax.text(0.98, 0.98, 'Growing\\nTrend', transform=ax.transAxes, fontsize=8, alpha=0.6, \n",
    "                   verticalalignment='top', horizontalalignment='right', \n",
    "                   bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.3))\n",
    "            ax.text(0.02, 0.02, 'Declining\\nWeak Trend', transform=ax.transAxes, fontsize=8, alpha=0.6, \n",
    "                   bbox=dict(boxstyle='round,pad=0.2', facecolor='lightcoral', alpha=0.3))\n",
    "            ax.text(0.98, 0.02, 'Growing\\nWeak Trend', transform=ax.transAxes, fontsize=8, alpha=0.6, \n",
    "                   horizontalalignment='right', bbox=dict(boxstyle='round,pad=0.2', facecolor='lightyellow', alpha=0.3))\n",
    "        \n",
    "        # Hide extra subplots if any\n",
    "        for idx in range(n_horizons, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='red', alpha=0.7, label='High Prediction (Top 25%)'),\n",
    "            Patch(facecolor='green', alpha=0.7, label='Medium Prediction (Middle 50%)'),\n",
    "            Patch(facecolor='blue', alpha=0.7, label='Low Prediction (Bottom 25%)')\n",
    "        ]\n",
    "        \n",
    "        if n_horizons > 1:\n",
    "            fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.98, 0.95))\n",
    "        else:\n",
    "            axes[0].legend(handles=legend_elements, loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_cluster_growth_by_horizon(self, multi_horizon_results: Dict, horizons: List[int] = None, \n",
    "                                 max_clusters_per_plot: int = 15) -> None:\n",
    "        \"\"\"\n",
    "        Plot cluster growth rates across different horizons with clusters on x-axis.\n",
    "        \n",
    "        Args:\n",
    "            multi_horizon_results (Dict): Results from generate_multi_horizon_forecasts\n",
    "            horizons (List[int]): Specific horizons to plot (if None, plots all)\n",
    "            max_clusters_per_plot (int): Maximum clusters per plot to avoid overcrowding\n",
    "        \"\"\"\n",
    "        if not multi_horizon_results:\n",
    "            print(\"No multi-horizon results to visualize\")\n",
    "            return\n",
    "        \n",
    "        available_horizons = sorted(multi_horizon_results.keys())\n",
    "        if horizons is None:\n",
    "            horizons = available_horizons\n",
    "        else:\n",
    "            horizons = [h for h in horizons if h in available_horizons]\n",
    "        \n",
    "        if not horizons:\n",
    "            print(\"No valid horizons found\")\n",
    "            return\n",
    "        \n",
    "        # Collect all unique clusters across horizons\n",
    "        all_clusters = set()\n",
    "        for horizon in horizons:\n",
    "            cluster_metrics = multi_horizon_results[horizon]['metrics']['cluster_metrics']\n",
    "            all_clusters.update(cluster_metrics.keys())\n",
    "        \n",
    "        all_clusters = sorted(list(all_clusters))\n",
    "        \n",
    "        # If too many clusters, split into multiple plots\n",
    "        if len(all_clusters) > max_clusters_per_plot:\n",
    "            n_plots = (len(all_clusters) + max_clusters_per_plot - 1) // max_clusters_per_plot\n",
    "            cluster_chunks = [all_clusters[i*max_clusters_per_plot:(i+1)*max_clusters_per_plot] \n",
    "                             for i in range(n_plots)]\n",
    "        else:\n",
    "            cluster_chunks = [all_clusters]\n",
    "        \n",
    "        for chunk_idx, cluster_chunk in enumerate(cluster_chunks):\n",
    "            # Determine subplot layout\n",
    "            n_horizons = len(horizons)\n",
    "            if n_horizons == 1:\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "                axes = [ax]\n",
    "            elif n_horizons <= 2:\n",
    "                fig, axes = plt.subplots(1, n_horizons, figsize=(16, 8))\n",
    "                if n_horizons == 1:\n",
    "                    axes = [axes]\n",
    "            elif n_horizons <= 4:\n",
    "                fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "                axes = axes.flatten()\n",
    "            elif n_horizons <= 6:\n",
    "                fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "                axes = axes.flatten()\n",
    "            else:\n",
    "                fig, axes = plt.subplots(3, 3, figsize=(20, 16))\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            plot_title = f'Cluster Growth Rates by Horizon'\n",
    "            if len(cluster_chunks) > 1:\n",
    "                plot_title += f' (Part {chunk_idx + 1}/{len(cluster_chunks)})'\n",
    "            fig.suptitle(plot_title, fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # Plot each horizon\n",
    "            for idx, horizon in enumerate(horizons):\n",
    "                ax = axes[idx]\n",
    "                cluster_metrics = multi_horizon_results[horizon]['metrics']['cluster_metrics']\n",
    "                \n",
    "                # Prepare data for this horizon\n",
    "                cluster_labels = []\n",
    "                growth_rates = []\n",
    "                colors = []\n",
    "                topic_labels = []\n",
    "                \n",
    "                for cluster_id in cluster_chunk:\n",
    "                    if cluster_id in cluster_metrics:\n",
    "                        metrics = cluster_metrics[cluster_id]\n",
    "                        growth_rate = metrics['growth_rate']\n",
    "                        topic_words = metrics.get('topic_words', [])[:2]  # Take first 2 topic words\n",
    "                        \n",
    "                        # Create cluster label with topic\n",
    "                        topic_str = ', '.join(topic_words) if topic_words else 'Unknown'\n",
    "                        cluster_label = f\"C{cluster_id}\"\n",
    "                        topic_labels.append(f\"{cluster_label}\\n{topic_str}\")\n",
    "                        \n",
    "                        cluster_labels.append(cluster_label)\n",
    "                        growth_rates.append(growth_rate)\n",
    "                        \n",
    "                        # Color based on growth rate\n",
    "                        if growth_rate > 0.05:\n",
    "                            colors.append('green')\n",
    "                        elif growth_rate > 0:\n",
    "                            colors.append('lightgreen')\n",
    "                        elif growth_rate > -0.05:\n",
    "                            colors.append('orange')\n",
    "                        else:\n",
    "                            colors.append('red')\n",
    "                    else:\n",
    "                        # Cluster not available for this horizon\n",
    "                        cluster_label = f\"C{cluster_id}\"\n",
    "                        topic_labels.append(f\"{cluster_label}\\nNo Data\")\n",
    "                        cluster_labels.append(cluster_label)\n",
    "                        growth_rates.append(0)\n",
    "                        colors.append('gray')\n",
    "                \n",
    "                # Create bar plot\n",
    "                x_positions = range(len(cluster_labels))\n",
    "                bars = ax.bar(x_positions, growth_rates, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "                \n",
    "                # Customize plot\n",
    "                ax.set_xlabel('Clusters', fontweight='bold')\n",
    "                ax.set_ylabel('Growth Rate', fontweight='bold')\n",
    "                ax.set_title(f'{horizon}-Day Forecast Horizon', fontweight='bold')\n",
    "                ax.set_xticks(x_positions)\n",
    "                ax.set_xticklabels(topic_labels, rotation=45, ha='right', fontsize=9)\n",
    "                ax.grid(True, alpha=0.3, axis='y')\n",
    "                ax.axhline(y=0, color='black', linestyle='-', alpha=0.8, linewidth=1)\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar, value in zip(bars, growth_rates):\n",
    "                    height = bar.get_height()\n",
    "                    label_y = height + 0.001 if height >= 0 else height - 0.005\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2, label_y, f'{value:.3f}', \n",
    "                           ha='center', va='bottom' if height >= 0 else 'top', fontsize=8, fontweight='bold')\n",
    "                \n",
    "                # Add growth rate categories legend (only on first subplot)\n",
    "                if idx == 0:\n",
    "                    from matplotlib.patches import Patch\n",
    "                    legend_elements = [\n",
    "                        Patch(facecolor='green', alpha=0.7, label='High Growth (>5%)'),\n",
    "                        Patch(facecolor='lightgreen', alpha=0.7, label='Moderate Growth (0-5%)'),\n",
    "                        Patch(facecolor='orange', alpha=0.7, label='Slight Decline (0 to -5%)'),\n",
    "                        Patch(facecolor='red', alpha=0.7, label='Strong Decline (<-5%)'),\n",
    "                        Patch(facecolor='gray', alpha=0.7, label='No Data')\n",
    "                    ]\n",
    "                    ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0.02, 0.98), fontsize=8)\n",
    "            \n",
    "            # Hide extra subplots\n",
    "            for idx in range(len(horizons), len(axes)):\n",
    "                axes[idx].set_visible(False)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "    def plot_cluster_comparison_across_horizons(self, multi_horizon_results: Dict, \n",
    "                                              selected_clusters: List[int] = None,\n",
    "                                              top_n: int = 10) -> None:\n",
    "        \"\"\"\n",
    "        Plot growth rate comparison for selected clusters across all horizons.\n",
    "        \n",
    "        Args:\n",
    "            multi_horizon_results (Dict): Results from multi-horizon analysis\n",
    "            selected_clusters (List[int]): Specific clusters to plot (if None, uses top performers)\n",
    "            top_n (int): Number of top clusters to show if selected_clusters is None\n",
    "        \"\"\"\n",
    "        if not multi_horizon_results:\n",
    "            print(\"No multi-horizon results available\")\n",
    "            return\n",
    "        \n",
    "        horizons = sorted(multi_horizon_results.keys())\n",
    "        \n",
    "        # Determine clusters to plot\n",
    "        if selected_clusters is None:\n",
    "            # Find top performing clusters based on average growth rate\n",
    "            cluster_avg_growth = {}\n",
    "            all_clusters = set()\n",
    "            \n",
    "            for horizon in horizons:\n",
    "                cluster_metrics = multi_horizon_results[horizon]['metrics']['cluster_metrics']\n",
    "                for cluster_id, metrics in cluster_metrics.items():\n",
    "                    all_clusters.add(cluster_id)\n",
    "                    if cluster_id not in cluster_avg_growth:\n",
    "                        cluster_avg_growth[cluster_id] = []\n",
    "                    cluster_avg_growth[cluster_id].append(metrics['growth_rate'])\n",
    "            \n",
    "            # Calculate average growth rates\n",
    "            for cluster_id in cluster_avg_growth:\n",
    "                cluster_avg_growth[cluster_id] = np.mean(cluster_avg_growth[cluster_id])\n",
    "            \n",
    "            # Get top clusters\n",
    "            top_clusters = sorted(cluster_avg_growth.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "            selected_clusters = [cluster_id for cluster_id, _ in top_clusters]\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # Colors for different clusters\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(selected_clusters)))\n",
    "        \n",
    "        # Plot each cluster's growth rate across horizons\n",
    "        for i, cluster_id in enumerate(selected_clusters):\n",
    "            growth_rates = []\n",
    "            topic_words = []\n",
    "            \n",
    "            for horizon in horizons:\n",
    "                cluster_metrics = multi_horizon_results[horizon]['metrics']['cluster_metrics']\n",
    "                if cluster_id in cluster_metrics:\n",
    "                    growth_rates.append(cluster_metrics[cluster_id]['growth_rate'])\n",
    "                    if not topic_words:  # Get topic words from first available horizon\n",
    "                        topic_words = cluster_metrics[cluster_id].get('topic_words', [])[:2]\n",
    "                else:\n",
    "                    growth_rates.append(0)  # No data available\n",
    "            \n",
    "            # Create label with topic words\n",
    "            topic_str = ', '.join(topic_words) if topic_words else 'Unknown'\n",
    "            label = f\"C{cluster_id}: {topic_str}\"\n",
    "            \n",
    "            # Plot line\n",
    "            ax.plot(horizons, growth_rates, marker='o', linewidth=2, markersize=8, \n",
    "                    color=colors[i], label=label)\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Forecast Horizon (Days)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Growth Rate', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Cluster Growth Rate Comparison Across Forecast Horizons', fontweight='bold', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='black', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add legend\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    def run_enhanced_pipeline_with_forecasting(self, comment_files: List[str], video_file: str,\n",
    "                                             n_clusters: int = None, forecast_days: int = 30) -> Dict:\n",
    "        \"\"\"\n",
    "        Run the complete enhanced pipeline with ensemble forecasting.\n",
    "        \n",
    "        Args:\n",
    "            comment_files (List[str]): List of comment CSV files\n",
    "            video_file (str): Video CSV file path\n",
    "            n_clusters (int): Number of clusters (if None, will optimize)\n",
    "            forecast_days (int): Number of days to forecast\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Complete analysis results including ensemble forecasts\n",
    "        \"\"\"\n",
    "        print(\"Starting enhanced pipeline with ensemble forecasting...\")\n",
    "        \n",
    "        # Run base pipeline\n",
    "        base_results = super().run_enhanced_pipeline(comment_files, video_file, n_clusters, forecast_days)\n",
    "        \n",
    "        # Train ensemble forecasting models\n",
    "        self.train_ensemble_forecasting_models(forecast_days)\n",
    "        \n",
    "        # Generate ensemble forecasts\n",
    "        forecast_results = self.generate_ensemble_forecasts(forecast_days)\n",
    "        \n",
    "        # Visualize forecasts\n",
    "        if forecast_results:\n",
    "            self.visualize_ensemble_forecasts(forecast_results)\n",
    "        \n",
    "        # Combine results\n",
    "        enhanced_results = base_results.copy()\n",
    "        enhanced_results['ensemble_forecasts'] = forecast_results\n",
    "        enhanced_results['forecasting_summary'] = {\n",
    "            'forecast_horizon_days': forecast_days,\n",
    "            'clusters_forecasted': len(forecast_results),\n",
    "            'models_used': ['LSTM', 'ARIMA', 'Prophet'],\n",
    "            'ensemble_method': 'weighted_average',\n",
    "            'forecast_categories': {\n",
    "                category: len([r for r in forecast_results.values() if r['forecast_category'] == category])\n",
    "                for category in ['rapid_growth', 'steady_growth', 'stable', 'declining', 'volatile']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return enhanced_results\n",
    "\n",
    "    def run_multi_horizon_analysis(self, comment_files: List[str], video_file: str,\n",
    "                                  n_clusters: int = None, \n",
    "                                  forecast_horizons: List[int] = [20, 30, 40, 60, 80]) -> Dict:\n",
    "        \"\"\"\n",
    "        Run complete analysis with multiple forecast horizons.\n",
    "        \n",
    "        Args:\n",
    "            comment_files: List of comment CSV files\n",
    "            video_file: Video CSV file path\n",
    "            n_clusters: Number of clusters (if None, will optimize)\n",
    "            forecast_horizons: List of forecast horizons in days\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Complete analysis results with multi-horizon forecasts\n",
    "        \"\"\"\n",
    "        print(\"Starting multi-horizon trend analysis...\")\n",
    "        \n",
    "        # Run base pipeline first\n",
    "        base_results = self.run_enhanced_pipeline_with_forecasting(\n",
    "            comment_files, video_file, n_clusters, max(forecast_horizons)\n",
    "        )\n",
    "        \n",
    "        # Generate multi-horizon forecasts\n",
    "        multi_horizon_results = self.generate_multi_horizon_forecasts(forecast_horizons)\n",
    "        \n",
    "        # Create comprehensive visualizations\n",
    "        if multi_horizon_results:\n",
    "        # Generate comprehensive generational forecasting visualizations\n",
    "            print(\"Generating generational forecasting analysis...\")\n",
    "            self.visualize_all_generations_forecast(forecast_horizons)\n",
    "            \n",
    "            # Generate specific generation plots\n",
    "            for generation in ['gen_z', 'millennial']:\n",
    "                print(f\"Generating {generation} specific growth analysis...\")\n",
    "                self.plot_generational_growth_by_clusters(generation, forecast_horizons[:3])\n",
    "            # Combine all results\n",
    "            complete_results = base_results.copy()\n",
    "            complete_results['multi_horizon_analysis'] = multi_horizon_results\n",
    "            complete_results['forecast_horizons'] = forecast_horizons\n",
    "            \n",
    "        return complete_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T17:08:23.603394Z",
     "iopub.status.busy": "2025-09-06T17:08:23.603093Z",
     "iopub.status.idle": "2025-09-06T17:08:23.616106Z",
     "shell.execute_reply": "2025-09-06T17:08:23.615262Z",
     "shell.execute_reply.started": "2025-09-06T17:08:23.603370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "class ModelSaver:\n",
    "    \"\"\"\n",
    "    Improved model saver that handles ensemble forecasting models properly\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path='./saved_trend_models'):\n",
    "        self.base_path = base_path\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    def save_enhanced_model(self, predictor, model_name=None):\n",
    "        \"\"\"\n",
    "        Save model with proper handling of ensemble components and data\n",
    "        \"\"\"\n",
    "        if model_name is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_name = f\"enhanced_trend_ai_{timestamp}\"\n",
    "        \n",
    "        model_dir = os.path.join(self.base_path, model_name)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        saved_files = {}\n",
    "        \n",
    "        # Save essential DataFrames using pandas pickle\n",
    "        if hasattr(predictor, 'comments_df') and predictor.comments_df is not None:\n",
    "            comments_path = os.path.join(model_dir, 'comments_df.pkl')\n",
    "            predictor.comments_df.to_pickle(comments_path)\n",
    "            saved_files['comments_df'] = comments_path\n",
    "        \n",
    "        if hasattr(predictor, 'videos_df') and predictor.videos_df is not None:\n",
    "            videos_path = os.path.join(model_dir, 'videos_df.pkl')\n",
    "            predictor.videos_df.to_pickle(videos_path)\n",
    "            saved_files['videos_df'] = videos_path\n",
    "        \n",
    "        # CRITICAL: Save combined_trend_data\n",
    "        if hasattr(predictor, 'combined_trend_data') and predictor.combined_trend_data is not None:\n",
    "            trend_data_path = os.path.join(model_dir, 'combined_trend_data.pkl')\n",
    "            predictor.combined_trend_data.to_pickle(trend_data_path)\n",
    "            saved_files['combined_trend_data'] = trend_data_path\n",
    "        \n",
    "        # Save model components that can be pickled safely\n",
    "        safe_attributes = [\n",
    "            'embeddings', 'clusters', 'cluster_topics', 'cluster_tags', \n",
    "            'popular_tags', 'generational_clusters', 'scaler',\n",
    "            'video_weight', 'comment_weight', 'tag_weight'\n",
    "        ]\n",
    "        \n",
    "        model_components = {}\n",
    "        for attr in safe_attributes:\n",
    "            if hasattr(predictor, attr):\n",
    "                model_components[attr] = getattr(predictor, attr)\n",
    "        \n",
    "        components_path = os.path.join(model_dir, 'model_components.pkl')\n",
    "        with open(components_path, 'wb') as f:\n",
    "            pickle.dump(model_components, f)\n",
    "        saved_files['model_components'] = components_path\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'model_name': model_name,\n",
    "            'creation_date': datetime.now().isoformat(),\n",
    "            'model_type': 'EnhancedTrendAI',\n",
    "            'saved_components': list(model_components.keys()),\n",
    "            'has_trend_data': hasattr(predictor, 'combined_trend_data') and predictor.combined_trend_data is not None\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(model_dir, 'metadata.json')\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        saved_files['metadata'] = metadata_path\n",
    "        \n",
    "        print(f\"Enhanced model saved successfully to: {model_dir}\")\n",
    "        return {'model_name': model_name, 'saved_files': saved_files}\n",
    "    \n",
    "    def load_enhanced_model(self, model_name):\n",
    "        \"\"\"\n",
    "        Load model with proper data restoration\n",
    "        \"\"\"\n",
    "        model_dir = os.path.join(self.base_path, model_name)\n",
    "        \n",
    "        if not os.path.exists(model_dir):\n",
    "            raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
    "        \n",
    "        # Initialize fresh predictor\n",
    "        predictor = EnhancedTrendAI()\n",
    "        \n",
    "        # Load DataFrames\n",
    "        comments_path = os.path.join(model_dir, 'comments_df.pkl')\n",
    "        if os.path.exists(comments_path):\n",
    "            predictor.comments_df = pd.read_pickle(comments_path)\n",
    "            print(\"Loaded comments DataFrame\")\n",
    "        \n",
    "        videos_path = os.path.join(model_dir, 'videos_df.pkl')\n",
    "        if os.path.exists(videos_path):\n",
    "            predictor.videos_df = pd.read_pickle(videos_path)\n",
    "            print(\"Loaded videos DataFrame\")\n",
    "        \n",
    "        # CRITICAL: Load trend data\n",
    "        trend_data_path = os.path.join(model_dir, 'combined_trend_data.pkl')\n",
    "        if os.path.exists(trend_data_path):\n",
    "            predictor.combined_trend_data = pd.read_pickle(trend_data_path)\n",
    "            print(\"Loaded combined trend data\")\n",
    "        \n",
    "        # Load model components\n",
    "        components_path = os.path.join(model_dir, 'model_components.pkl')\n",
    "        if os.path.exists(components_path):\n",
    "            with open(components_path, 'rb') as f:\n",
    "                components = pickle.load(f)\n",
    "            \n",
    "            for attr, value in components.items():\n",
    "                setattr(predictor, attr, value)\n",
    "            print(f\"Loaded model components: {list(components.keys())}\")\n",
    "        \n",
    "        # Reinitialize ensemble forecaster if trend data exists\n",
    "        if hasattr(predictor, 'combined_trend_data') and predictor.combined_trend_data is not None:\n",
    "            predictor.ensemble_forecaster = EnsembleForecaster()\n",
    "            print(\"Ensemble forecaster reinitialized\")\n",
    "        \n",
    "        return predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Genrational TrendAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T17:08:30.781014Z",
     "iopub.status.busy": "2025-09-06T17:08:30.780671Z",
     "iopub.status.idle": "2025-09-06T17:08:30.800145Z",
     "shell.execute_reply": "2025-09-06T17:08:30.799473Z",
     "shell.execute_reply.started": "2025-09-06T17:08:30.780991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnhancedGenerationalTrendAI(EnhancedTrendAI):\n",
    "    \"\"\"\n",
    "    Enhanced TrendAI with improved generational analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        super().__init__(model_name)\n",
    "        # Replace the analyzer with enhanced version\n",
    "        self.generational_analyzer = EnhancedGenerationalLanguageAnalyzer()\n",
    "    \n",
    "    def analyze_generational_patterns(self) -> None:\n",
    "        \"\"\"Enhanced generational analysis with debugging.\"\"\"\n",
    "        print(\"Analyzing generational language patterns with enhanced detection...\")\n",
    "        \n",
    "        if self.comments_df is None:\n",
    "            print(\"No comment data available\")\n",
    "            return\n",
    "        \n",
    "        # Sample some comments for debugging\n",
    "        sample_comments = self.comments_df['cleaned_text'].head(100).tolist()\n",
    "        print(\"Testing generational detection on sample comments...\")\n",
    "        \n",
    "        generation_counts = {'gen_z': 0, 'millennial': 0, 'gen_x': 0, 'boomer': 0, 'neutral': 0}\n",
    "        \n",
    "        for comment in sample_comments:\n",
    "            if pd.notna(comment) and len(str(comment).strip()) > 5:\n",
    "                classification = self.generational_analyzer.classify_generation(str(comment))\n",
    "                generation_counts[classification] += 1\n",
    "        \n",
    "        print(\"Sample classification results:\")\n",
    "        for gen, count in generation_counts.items():\n",
    "            print(f\"  {gen}: {count}/100 ({count}%)\")\n",
    "        \n",
    "        # Analyze all comments\n",
    "        print(\"Analyzing all comments...\")\n",
    "        self.comments_df['generational_scores'] = self.comments_df['cleaned_text'].apply(\n",
    "            lambda x: self.generational_analyzer.analyze_generational_language(str(x)) if pd.notna(x) else {}\n",
    "        )\n",
    "        \n",
    "        # Extract individual generation scores\n",
    "        for generation in ['gen_z', 'millennial', 'gen_x', 'boomer']:\n",
    "            self.comments_df[f'{generation}_score'] = self.comments_df['generational_scores'].apply(\n",
    "                lambda x: x.get(generation, 0) if isinstance(x, dict) else 0\n",
    "            )\n",
    "        \n",
    "        # Classify predominant generation with more liberal approach\n",
    "        self.comments_df['dominant_generation'] = self.comments_df['cleaned_text'].apply(\n",
    "            lambda x: self.generational_analyzer.classify_generation(str(x)) if pd.notna(x) else 'neutral'\n",
    "        )\n",
    "        \n",
    "        # Print final distribution\n",
    "        final_distribution = self.comments_df['dominant_generation'].value_counts()\n",
    "        print(\"Final generational distribution:\")\n",
    "        total_comments = len(self.comments_df)\n",
    "        for gen, count in final_distribution.items():\n",
    "            percentage = (count / total_comments) * 100\n",
    "            print(f\"  {gen}: {count:,} comments ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Analyze videos if available\n",
    "        if self.videos_df is not None:\n",
    "            print(\"Analyzing videos for generational patterns...\")\n",
    "            combined_video_text = (\n",
    "                self.videos_df.get('cleaned_title', '').fillna('') + ' ' + \n",
    "                self.videos_df.get('cleaned_description', '').fillna('')\n",
    "            )\n",
    "            \n",
    "            self.videos_df['generational_scores'] = combined_video_text.apply(\n",
    "                lambda x: self.generational_analyzer.analyze_generational_language(str(x)) if pd.notna(x) and str(x).strip() else {}\n",
    "            )\n",
    "            \n",
    "            # Extract individual generation scores for videos\n",
    "            for generation in ['gen_z', 'millennial', 'gen_x', 'boomer']:\n",
    "                self.videos_df[f'{generation}_score'] = self.videos_df['generational_scores'].apply(\n",
    "                    lambda x: x.get(generation, 0) if isinstance(x, dict) else 0\n",
    "                )\n",
    "            \n",
    "            self.videos_df['dominant_generation'] = combined_video_text.apply(\n",
    "                lambda x: self.generational_analyzer.classify_generation(str(x)) if pd.notna(x) and str(x).strip() else 'neutral'\n",
    "            )\n",
    "            \n",
    "            # Print video distribution\n",
    "            video_distribution = self.videos_df['dominant_generation'].value_counts()\n",
    "            print(\"Video generational distribution:\")\n",
    "            total_videos = len(self.videos_df)\n",
    "            for gen, count in video_distribution.items():\n",
    "                percentage = (count / total_videos) * 100\n",
    "                print(f\"  {gen}: {count} videos ({percentage:.1f}%)\")\n",
    "    \n",
    "    def analyze_generational_trends_by_cluster(self) -> None:\n",
    "        \"\"\"Enhanced cluster analysis with debugging and validation.\"\"\"\n",
    "        print(\"Analyzing generational trends by cluster with enhanced detection...\")\n",
    "        \n",
    "        if self.comments_df is None or 'cluster' not in self.comments_df.columns:\n",
    "            print(\"Comment data or clustering results not available\")\n",
    "            return\n",
    "        \n",
    "        # Ensure generational analysis is completed\n",
    "        if 'dominant_generation' not in self.comments_df.columns:\n",
    "            self.analyze_generational_patterns()\n",
    "        \n",
    "        clusters = self.comments_df['cluster'].unique()\n",
    "        self.generational_clusters = {}\n",
    "        \n",
    "        print(f\"Analyzing {len(clusters)} clusters...\")\n",
    "        \n",
    "        for cluster_id in clusters:\n",
    "            cluster_comments = self.comments_df[self.comments_df['cluster'] == cluster_id].copy()\n",
    "            \n",
    "            if len(cluster_comments) < 5:  # Reduced minimum threshold\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Enhanced generational distribution analysis\n",
    "                generation_distribution = cluster_comments['dominant_generation'].value_counts(normalize=True)\n",
    "                \n",
    "                if len(generation_distribution) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get the most dominant generation\n",
    "                dominant_generation = generation_distribution.index[0]\n",
    "                dominant_generation_score = generation_distribution.iloc[0]\n",
    "                \n",
    "                # Calculate average generational scores for this cluster\n",
    "                avg_generational_scores = {}\n",
    "                for generation in ['gen_z', 'millennial', 'gen_x', 'boomer']:\n",
    "                    if f'{generation}_score' in cluster_comments.columns:\n",
    "                        avg_generational_scores[generation] = cluster_comments[f'{generation}_score'].mean()\n",
    "                    else:\n",
    "                        avg_generational_scores[generation] = 0.0\n",
    "                \n",
    "                # Get topic information\n",
    "                topic_words = self.cluster_topics.get(cluster_id, [])[:10]\n",
    "                topic_tags = self.cluster_tags.get(cluster_id, [])[:5]\n",
    "                \n",
    "                # Enhanced video performance analysis by generation\n",
    "                video_performance_by_generation = {}\n",
    "                if hasattr(self, 'videos_df') and self.videos_df is not None and 'cluster' in self.videos_df.columns:\n",
    "                    cluster_videos = self.videos_df[self.videos_df['cluster'] == cluster_id]\n",
    "                    \n",
    "                    if not cluster_videos.empty and 'dominant_generation' in cluster_videos.columns:\n",
    "                        for generation in ['gen_z', 'millennial', 'gen_x', 'boomer', 'neutral']:\n",
    "                            gen_videos = cluster_videos[cluster_videos['dominant_generation'] == generation]\n",
    "                            if len(gen_videos) > 0:\n",
    "                                video_performance_by_generation[generation] = {\n",
    "                                    'count': len(gen_videos),\n",
    "                                    'avg_views': gen_videos.get('viewCount', pd.Series([0])).mean(),\n",
    "                                    'avg_likes': gen_videos.get('likeCount', pd.Series([0])).mean(),\n",
    "                                    'avg_engagement': gen_videos.get('engagement_rate', pd.Series([0])).mean()\n",
    "                                }\n",
    "                \n",
    "                self.generational_clusters[cluster_id] = {\n",
    "                    'dominant_generation': dominant_generation,\n",
    "                    'dominant_generation_score': float(dominant_generation_score),\n",
    "                    'generation_distribution': generation_distribution.to_dict(),\n",
    "                    'avg_generational_scores': avg_generational_scores,\n",
    "                    'topic_words': topic_words,\n",
    "                    'topic_tags': topic_tags,\n",
    "                    'video_performance_by_generation': video_performance_by_generation,\n",
    "                    'cluster_size': len(cluster_comments)\n",
    "                }\n",
    "                \n",
    "                # Debug information for first few clusters\n",
    "                if len(self.generational_clusters) <= 3:\n",
    "                    print(f\"Cluster {cluster_id} analysis:\")\n",
    "                    print(f\"  Size: {len(cluster_comments)} comments\")\n",
    "                    print(f\"  Dominant generation: {dominant_generation} ({dominant_generation_score:.2%})\")\n",
    "                    print(f\"  Topics: {', '.join(topic_words[:3])}\")\n",
    "                    print(f\"  Generation distribution: {dict(generation_distribution.round(3))}\")\n",
    "                    print()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing cluster {cluster_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ Generational analysis completed for {len(self.generational_clusters)} clusters\")\n",
    "        \n",
    "        # Print summary by generation\n",
    "        generation_cluster_counts = {}\n",
    "        for cluster_data in self.generational_clusters.values():\n",
    "            gen = cluster_data['dominant_generation']\n",
    "            generation_cluster_counts[gen] = generation_cluster_counts.get(gen, 0) + 1\n",
    "        \n",
    "        print(\"Clusters by dominant generation:\")\n",
    "        for gen, count in sorted(generation_cluster_counts.items()):\n",
    "            print(f\"  {gen}: {count} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-06T17:08:37.112082Z",
     "iopub.status.busy": "2025-09-06T17:08:37.111735Z",
     "iopub.status.idle": "2025-09-06T17:09:13.284383Z",
     "shell.execute_reply": "2025-09-06T17:09:13.283245Z",
     "shell.execute_reply.started": "2025-09-06T17:08:37.112056Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Enhanced YouTube Trend Predictor with Tags Integration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting multi-horizon trend analysis...\n",
      "Starting enhanced pipeline with ensemble forecasting...\n",
      "Starting enhanced trend analysis pipeline with video and tag emphasis...\n",
      "Loading data...\n",
      "Loaded 1000000 comments from /kaggle/input/datathon/comments1.csv\n",
      "Loaded 999999 comments from /kaggle/input/datathon/comments2.csv\n",
      "Loaded 999999 comments from /kaggle/input/datathon/comments3.csv\n",
      "Error loading /kaggle/input/datathon/comments4.csv: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "Loaded 725015 comments from /kaggle/input/datathon/comments5.csv\n",
      "Total comments loaded: 3725013\n",
      "Error loading video file: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "Preprocessing data with tag integration...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1029851/4233601579.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# Run the complete analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         results = predictor.run_multi_horizon_analysis(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mcomment_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mvideo_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1029851/1269910200.py\u001b[0m in \u001b[0;36mrun_multi_horizon_analysis\u001b[0;34m(self, comment_files, video_file, n_clusters, forecast_horizons)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m         \u001b[0;31m# Run base pipeline first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m         base_results = self.run_enhanced_pipeline_with_forecasting(\n\u001b[0m\u001b[1;32m   1778\u001b[0m             \u001b[0mcomment_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_horizons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         )\n",
      "\u001b[0;32m/tmp/ipykernel_1029851/1269910200.py\u001b[0m in \u001b[0;36mrun_enhanced_pipeline_with_forecasting\u001b[0;34m(self, comment_files, video_file, n_clusters, forecast_days)\u001b[0m\n\u001b[1;32m   1729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0;31m# Run base pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1731\u001b[0;31m         \u001b[0mbase_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_enhanced_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast_days\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \u001b[0;31m# Train ensemble forecasting models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1029851/731723649.py\u001b[0m in \u001b[0;36mrun_enhanced_pipeline\u001b[0;34m(self, comment_files, video_file, n_clusters, forecast_days)\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0;31m# Step 1: Load and preprocess data (including tags)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0;31m# Step 2: Generate embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1029851/731723649.py\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# Clean text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'textOriginal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# Remove empty or very short comments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
      "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1029851/731723649.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# Remove excessive whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# Keep emojis and basic punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/re/__init__.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "import calendar\n",
    "\n",
    "class TrendDataExporter:\n",
    "    \"\"\"\n",
    "    Class to export comprehensive trend analysis data to JSON format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, predictor: EnhancedTrendAI):\n",
    "        self.predictor = predictor\n",
    "        self.export_timestamp = datetime.now()\n",
    "        \n",
    "    def generate_monthly_leaderboards(self, start_date: str = None, months_ahead: int = 12) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate monthly trend leaderboards for forecasted trends\n",
    "        \n",
    "        Args:\n",
    "            start_date (str): Start date in 'YYYY-MM' format (default: current month)\n",
    "            months_ahead (int): Number of months to forecast\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Monthly leaderboards with top trending topics, tags, and videos\n",
    "        \"\"\"\n",
    "        if start_date is None:\n",
    "            current_date = datetime.now()\n",
    "        else:\n",
    "            current_date = datetime.strptime(start_date + \"-01\", \"%Y-%m-%d\")\n",
    "        \n",
    "        monthly_leaderboards = {}\n",
    "        \n",
    "        # Generate forecasts for extended periods (up to 12 months)\n",
    "        extended_horizons = [30 * i for i in range(1, months_ahead + 1)]  # 30, 60, 90... days\n",
    "        multi_horizon_results = self.predictor.generate_multi_horizon_forecasts(extended_horizons)\n",
    "        \n",
    "        for month_offset in range(months_ahead):\n",
    "            target_date = current_date + timedelta(days=30 * month_offset)\n",
    "            month_key = target_date.strftime(\"%B_%Y\")  # e.g., \"July_2024\"\n",
    "            \n",
    "            horizon_days = 30 * (month_offset + 1)\n",
    "            \n",
    "            if horizon_days in multi_horizon_results:\n",
    "                horizon_data = multi_horizon_results[horizon_days]\n",
    "                cluster_metrics = horizon_data['metrics']['cluster_metrics']\n",
    "                \n",
    "                # Get top trending topics for this month\n",
    "                trending_topics = []\n",
    "                for cluster_id, metrics in cluster_metrics.items():\n",
    "                    if metrics['growth_rate'] > 0:  # Only positive growth\n",
    "                        trending_topics.append({\n",
    "                            'cluster_id': cluster_id,\n",
    "                            'topic_words': metrics.get('topic_words', [])[:3],\n",
    "                            'growth_rate': metrics['growth_rate'],\n",
    "                            'prediction_mean': metrics['prediction_mean'],\n",
    "                            'trend_strength': metrics['trend_strength']\n",
    "                        })\n",
    "                \n",
    "                # Sort by growth rate and get top 10\n",
    "                trending_topics.sort(key=lambda x: x['growth_rate'], reverse=True)\n",
    "                top_10_topics = trending_topics[:10]\n",
    "                \n",
    "                # Rank the topics\n",
    "                for rank, topic in enumerate(top_10_topics, 1):\n",
    "                    topic['ranking'] = rank\n",
    "                \n",
    "                # Get top tags for this period\n",
    "                top_tags = self._get_top_tags_for_period(cluster_metrics, limit=10)\n",
    "                \n",
    "                # Get top videos for this period\n",
    "                top_videos = self._get_top_videos_for_period(cluster_metrics, limit=2)\n",
    "                \n",
    "                monthly_leaderboards[month_key] = {\n",
    "                    'month': target_date.strftime(\"%B\"),\n",
    "                    'year': target_date.year,\n",
    "                    'forecast_date': target_date.strftime(\"%Y-%m-%d\"),\n",
    "                    'days_ahead': horizon_days,\n",
    "                    'top_trending_topics': [\n",
    "                        {\n",
    "                            'ranking': topic['ranking'],\n",
    "                            'predicted_trend': ', '.join(topic['topic_words']),\n",
    "                            'growth_rate': round(topic['growth_rate'], 4),\n",
    "                            'prediction_confidence': round(topic['prediction_mean'], 4),\n",
    "                            'trend_strength': round(topic['trend_strength'], 4)\n",
    "                        }\n",
    "                        for topic in top_10_topics\n",
    "                    ],\n",
    "                    'top_tags': top_tags,\n",
    "                    'top_videos': top_videos,\n",
    "                    'total_forecasted_clusters': len(cluster_metrics),\n",
    "                    'positive_growth_clusters': len([m for m in cluster_metrics.values() if m['growth_rate'] > 0])\n",
    "                }\n",
    "        \n",
    "        return monthly_leaderboards\n",
    "    \n",
    "    def generate_forecast_graph_data(self, days_range: int = 100, top_topics: int = 10) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate 100-day forecast data for graphing\n",
    "        \n",
    "        Args:\n",
    "            days_range (int): Number of days to forecast (default 100)\n",
    "            top_topics (int): Number of top topics to track\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Daily forecast data for top trending topics\n",
    "        \"\"\"\n",
    "        # Generate forecasts for each day from 1 to 100\n",
    "        daily_horizons = list(range(10, days_range + 1, 10))  # 10, 20, 30... 100\n",
    "        multi_horizon_results = self.predictor.generate_multi_horizon_forecasts(daily_horizons)\n",
    "        \n",
    "        # Identify top performing topics across all horizons\n",
    "        topic_performance = {}\n",
    "        for horizon, results in multi_horizon_results.items():\n",
    "            for cluster_id, metrics in results['metrics']['cluster_metrics'].items():\n",
    "                if cluster_id not in topic_performance:\n",
    "                    topic_performance[cluster_id] = {\n",
    "                        'total_growth': 0,\n",
    "                        'topic_words': metrics.get('topic_words', [])[:3],\n",
    "                        'appearances': 0\n",
    "                    }\n",
    "                topic_performance[cluster_id]['total_growth'] += metrics['growth_rate']\n",
    "                topic_performance[cluster_id]['appearances'] += 1\n",
    "        \n",
    "        # Calculate average growth and get top topics\n",
    "        for cluster_id in topic_performance:\n",
    "            avg_growth = topic_performance[cluster_id]['total_growth'] / topic_performance[cluster_id]['appearances']\n",
    "            topic_performance[cluster_id]['avg_growth'] = avg_growth\n",
    "        \n",
    "        top_topic_clusters = sorted(\n",
    "            topic_performance.items(), \n",
    "            key=lambda x: x[1]['avg_growth'], \n",
    "            reverse=True\n",
    "        )[:top_topics]\n",
    "        \n",
    "        # Generate daily data points\n",
    "        forecast_data = {\n",
    "            'metadata': {\n",
    "                'generation_date': self.export_timestamp.isoformat(),\n",
    "                'forecast_range_days': days_range,\n",
    "                'top_topics_count': len(top_topic_clusters),\n",
    "                'data_points_per_topic': len(daily_horizons)\n",
    "            },\n",
    "            'topics': {},\n",
    "            'aggregated_daily_data': []\n",
    "        }\n",
    "        \n",
    "        # Generate data for each top topic\n",
    "        for cluster_id, topic_data in top_topic_clusters:\n",
    "            topic_name = ', '.join(topic_data['topic_words'])\n",
    "            \n",
    "            daily_growth_rates = []\n",
    "            for horizon in daily_horizons:\n",
    "                if (horizon in multi_horizon_results and \n",
    "                    cluster_id in multi_horizon_results[horizon]['metrics']['cluster_metrics']):\n",
    "                    growth_rate = multi_horizon_results[horizon]['metrics']['cluster_metrics'][cluster_id]['growth_rate']\n",
    "                else:\n",
    "                    growth_rate = 0\n",
    "                \n",
    "                daily_growth_rates.append({\n",
    "                    'day': horizon,\n",
    "                    'growth_rate': round(growth_rate, 6),\n",
    "                    'date': (datetime.now() + timedelta(days=horizon)).strftime(\"%Y-%m-%d\")\n",
    "                })\n",
    "            \n",
    "            forecast_data['topics'][topic_name] = {\n",
    "                'cluster_id': cluster_id,\n",
    "                'topic_keywords': topic_data['topic_words'],\n",
    "                'average_growth_rate': round(topic_data['avg_growth'], 6),\n",
    "                'daily_forecasts': daily_growth_rates\n",
    "            }\n",
    "        \n",
    "        # Generate aggregated daily data for overall market trends\n",
    "        for horizon in daily_horizons:\n",
    "            if horizon in multi_horizon_results:\n",
    "                cluster_metrics = multi_horizon_results[horizon]['metrics']['cluster_metrics']\n",
    "                \n",
    "                total_growth = sum(m['growth_rate'] for m in cluster_metrics.values())\n",
    "                avg_growth = total_growth / len(cluster_metrics) if cluster_metrics else 0\n",
    "                positive_growth_count = sum(1 for m in cluster_metrics.values() if m['growth_rate'] > 0)\n",
    "                \n",
    "                forecast_data['aggregated_daily_data'].append({\n",
    "                    'day': horizon,\n",
    "                    'date': (datetime.now() + timedelta(days=horizon)).strftime(\"%Y-%m-%d\"),\n",
    "                    'average_market_growth': round(avg_growth, 6),\n",
    "                    'total_growth_sum': round(total_growth, 6),\n",
    "                    'positive_trends_count': positive_growth_count,\n",
    "                    'total_clusters_analyzed': len(cluster_metrics)\n",
    "                })\n",
    "        \n",
    "        return forecast_data\n",
    "    \n",
    "    def extract_raw_model_data(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract comprehensive raw data from model execution for LLM analysis\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Complete raw data from model analysis\n",
    "        \"\"\"\n",
    "        raw_data = {\n",
    "            'model_metadata': {\n",
    "                'model_type': 'EnhancedTrendAI',\n",
    "                'analysis_timestamp': self.export_timestamp.isoformat(),\n",
    "                'model_version': getattr(self.predictor, 'version', '1.0.0'),\n",
    "                'weight_factors': {\n",
    "                    'video_weight': self.predictor.video_weight,\n",
    "                    'comment_weight': self.predictor.comment_weight,\n",
    "                    'tag_weight': self.predictor.tag_weight\n",
    "                }\n",
    "            },\n",
    "            'data_summary': {\n",
    "                'total_comments_analyzed': len(self.predictor.comments_df) if self.predictor.comments_df is not None else 0,\n",
    "                'total_videos_analyzed': len(self.predictor.videos_df) if self.predictor.videos_df is not None else 0,\n",
    "                'total_clusters_identified': len(set(self.predictor.clusters)) if self.predictor.clusters is not None else 0,\n",
    "                'analysis_period': {\n",
    "                    'start_date': str(self.predictor.comments_df['publishedAt'].min().date()) if self.predictor.comments_df is not None and not self.predictor.comments_df.empty else None,\n",
    "                    'end_date': str(self.predictor.comments_df['publishedAt'].max().date()) if self.predictor.comments_df is not None and not self.predictor.comments_df.empty else None\n",
    "                }\n",
    "            },\n",
    "            'cluster_analysis': {},\n",
    "            'sentiment_analysis': {},\n",
    "            'generational_analysis': {},\n",
    "            'tag_analysis': {},\n",
    "            'video_performance_metrics': {},\n",
    "            'trending_patterns': {}\n",
    "        }\n",
    "        \n",
    "        # Extract cluster data\n",
    "        if hasattr(self.predictor, 'cluster_topics') and self.predictor.cluster_topics:\n",
    "            for cluster_id, topic_words in self.predictor.cluster_topics.items():\n",
    "                cluster_data = {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'topic_words': topic_words,\n",
    "                    'topic_tags': self.predictor.cluster_tags.get(cluster_id, []),\n",
    "                    'cluster_size': 0\n",
    "                }\n",
    "                \n",
    "                if self.predictor.comments_df is not None:\n",
    "                    cluster_comments = self.predictor.comments_df[self.predictor.comments_df['cluster'] == cluster_id]\n",
    "                    cluster_data.update({\n",
    "                        'cluster_size': len(cluster_comments),\n",
    "                        'avg_sentiment': cluster_comments['compound'].mean() if 'compound' in cluster_comments.columns else 0,\n",
    "                        'sentiment_distribution': {\n",
    "                            'positive': len(cluster_comments[cluster_comments['compound'] > 0.1]) if 'compound' in cluster_comments.columns else 0,\n",
    "                            'neutral': len(cluster_comments[(cluster_comments['compound'] >= -0.1) & (cluster_comments['compound'] <= 0.1)]) if 'compound' in cluster_comments.columns else 0,\n",
    "                            'negative': len(cluster_comments[cluster_comments['compound'] < -0.1]) if 'compound' in cluster_comments.columns else 0\n",
    "                        }\n",
    "                    })\n",
    "                \n",
    "                raw_data['cluster_analysis'][str(cluster_id)] = cluster_data\n",
    "        \n",
    "        # Extract sentiment analysis\n",
    "        if self.predictor.comments_df is not None and 'compound' in self.predictor.comments_df.columns:\n",
    "            sentiment_stats = {\n",
    "                'overall_sentiment_mean': self.predictor.comments_df['compound'].mean(),\n",
    "                'overall_sentiment_std': self.predictor.comments_df['compound'].std(),\n",
    "                'sentiment_distribution': {\n",
    "                    'very_positive': len(self.predictor.comments_df[self.predictor.comments_df['compound'] > 0.5]),\n",
    "                    'positive': len(self.predictor.comments_df[(self.predictor.comments_df['compound'] > 0.1) & (self.predictor.comments_df['compound'] <= 0.5)]),\n",
    "                    'neutral': len(self.predictor.comments_df[(self.predictor.comments_df['compound'] >= -0.1) & (self.predictor.comments_df['compound'] <= 0.1)]),\n",
    "                    'negative': len(self.predictor.comments_df[(self.predictor.comments_df['compound'] >= -0.5) & (self.predictor.comments_df['compound'] < -0.1)]),\n",
    "                    'very_negative': len(self.predictor.comments_df[self.predictor.comments_df['compound'] < -0.5])\n",
    "                }\n",
    "            }\n",
    "            raw_data['sentiment_analysis'] = sentiment_stats\n",
    "        \n",
    "        # Extract generational analysis\n",
    "        if hasattr(self.predictor, 'generational_clusters') and self.predictor.generational_clusters:\n",
    "            generational_data = {}\n",
    "            generation_distribution = {}\n",
    "            \n",
    "            for cluster_id, gen_data in self.predictor.generational_clusters.items():\n",
    "                generational_data[str(cluster_id)] = {\n",
    "                    'dominant_generation': gen_data['dominant_generation'],\n",
    "                    'confidence_score': gen_data['dominant_generation_score'],\n",
    "                    'generation_distribution': gen_data['generation_distribution'],\n",
    "                    'avg_generational_scores': gen_data['avg_generational_scores']\n",
    "                }\n",
    "                \n",
    "                # Aggregate generation distribution\n",
    "                dominant_gen = gen_data['dominant_generation']\n",
    "                if dominant_gen not in generation_distribution:\n",
    "                    generation_distribution[dominant_gen] = 0\n",
    "                generation_distribution[dominant_gen] += 1\n",
    "            \n",
    "            raw_data['generational_analysis'] = {\n",
    "                'cluster_level_analysis': generational_data,\n",
    "                'overall_generation_distribution': generation_distribution\n",
    "            }\n",
    "        \n",
    "        # Extract tag analysis\n",
    "        if hasattr(self.predictor, 'popular_tags') and self.predictor.popular_tags is not None:\n",
    "            tag_data = {}\n",
    "            for tag, data in self.predictor.popular_tags.head(50).iterrows():  # Top 50 tags\n",
    "                tag_data[tag] = {\n",
    "                    'frequency': int(data['frequency']),\n",
    "                    'total_views': int(data['total_views']),\n",
    "                    'total_likes': int(data['total_likes']),\n",
    "                    'popularity_score': float(data['tag_popularity_score']),\n",
    "                    'avg_views_per_video': float(data['avg_views_per_video']),\n",
    "                    'avg_likes_per_video': float(data['avg_likes_per_video'])\n",
    "                }\n",
    "            \n",
    "            raw_data['tag_analysis'] = {\n",
    "                'top_tags': tag_data,\n",
    "                'total_unique_tags': len(self.predictor.popular_tags)\n",
    "            }\n",
    "        \n",
    "        # Extract video performance metrics\n",
    "        if self.predictor.videos_df is not None:\n",
    "            video_stats = {\n",
    "                'total_views': int(self.predictor.videos_df['viewCount'].sum()),\n",
    "                'total_likes': int(self.predictor.videos_df['likeCount'].sum()),\n",
    "                'total_comments': int(self.predictor.videos_df['commentCount'].sum()),\n",
    "                'avg_engagement_rate': float(self.predictor.videos_df['engagement_rate'].mean()),\n",
    "                'avg_trending_score': float(self.predictor.videos_df['trending_score'].mean()),\n",
    "                'top_performing_videos': []\n",
    "            }\n",
    "            \n",
    "            # Get top 10 performing videos\n",
    "            top_videos = self.predictor.videos_df.nlargest(10, 'trending_score')\n",
    "            for _, video in top_videos.iterrows():\n",
    "                video_stats['top_performing_videos'].append({\n",
    "                    'title': video['title'],\n",
    "                    'views': int(video['viewCount']),\n",
    "                    'likes': int(video['likeCount']),\n",
    "                    'comments': int(video['commentCount']),\n",
    "                    'trending_score': float(video['trending_score']),\n",
    "                    'engagement_rate': float(video['engagement_rate']),\n",
    "                    'published_date': str(video['publishedAt'].date()) if pd.notna(video['publishedAt']) else None\n",
    "                })\n",
    "            \n",
    "            raw_data['video_performance_metrics'] = video_stats\n",
    "        \n",
    "        # Extract trending patterns\n",
    "        if hasattr(self.predictor, 'combined_trend_data') and not self.predictor.combined_trend_data.empty:\n",
    "            trending_patterns = {\n",
    "                'temporal_trends': [],\n",
    "                'cluster_performance_over_time': {}\n",
    "            }\n",
    "            \n",
    "            # Aggregate temporal trends\n",
    "            temporal_agg = self.predictor.combined_trend_data.groupby('date').agg({\n",
    "                'combined_trending_score': 'mean',\n",
    "                'total_views': 'sum',\n",
    "                'video_likes': 'sum',\n",
    "                'comment_count': 'sum'\n",
    "            }).reset_index()\n",
    "            \n",
    "            for _, row in temporal_agg.iterrows():\n",
    "                trending_patterns['temporal_trends'].append({\n",
    "                    'date': str(row['date'].date()),\n",
    "                    'avg_trending_score': float(row['combined_trending_score']),\n",
    "                    'total_daily_views': int(row['total_views']),\n",
    "                    'total_daily_likes': int(row['video_likes']),\n",
    "                    'total_daily_comments': int(row['comment_count'])\n",
    "                })\n",
    "            \n",
    "            raw_data['trending_patterns'] = trending_patterns\n",
    "        \n",
    "        return raw_data\n",
    "    \n",
    "    def _get_top_tags_for_period(self, cluster_metrics: Dict, limit: int = 10) -> List[Dict]:\n",
    "        \"\"\"Extract top tags for a specific forecast period\"\"\"\n",
    "        tag_performance = {}\n",
    "        \n",
    "        for cluster_id, metrics in cluster_metrics.items():\n",
    "            if hasattr(self.predictor, 'cluster_tags') and cluster_id in self.predictor.cluster_tags:\n",
    "                cluster_tags = self.predictor.cluster_tags[cluster_id]\n",
    "                growth_rate = metrics['growth_rate']\n",
    "                \n",
    "                for tag in cluster_tags:\n",
    "                    if tag not in tag_performance:\n",
    "                        tag_performance[tag] = {'total_growth': 0, 'cluster_count': 0}\n",
    "                    tag_performance[tag]['total_growth'] += growth_rate\n",
    "                    tag_performance[tag]['cluster_count'] += 1\n",
    "        \n",
    "        # Calculate average growth for each tag\n",
    "        for tag in tag_performance:\n",
    "            avg_growth = tag_performance[tag]['total_growth'] / tag_performance[tag]['cluster_count']\n",
    "            tag_performance[tag]['avg_growth'] = avg_growth\n",
    "        \n",
    "        # Sort and get top tags\n",
    "        top_tags = sorted(tag_performance.items(), key=lambda x: x[1]['avg_growth'], reverse=True)[:limit]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                'ranking': i + 1,\n",
    "                'tag': tag,\n",
    "                'growth_rate': round(data['avg_growth'], 4),\n",
    "                'cluster_count': data['cluster_count']\n",
    "            }\n",
    "            for i, (tag, data) in enumerate(top_tags)\n",
    "        ]\n",
    "    \n",
    "    def _get_top_videos_for_period(self, cluster_metrics: Dict, limit: int = 2) -> List[Dict]:\n",
    "        \"\"\"Extract top videos for a specific forecast period\"\"\"\n",
    "        if self.predictor.videos_df is None:\n",
    "            return []\n",
    "        \n",
    "        # Get clusters with highest growth rates\n",
    "        top_growth_clusters = sorted(\n",
    "            cluster_metrics.items(), \n",
    "            key=lambda x: x[1]['growth_rate'], \n",
    "            reverse=True\n",
    "        )[:5]  # Look at top 5 growing clusters\n",
    "        \n",
    "        top_videos = []\n",
    "        for cluster_id, metrics in top_growth_clusters:\n",
    "            if 'cluster' in self.predictor.videos_df.columns:\n",
    "                cluster_videos = self.predictor.videos_df[\n",
    "                    self.predictor.videos_df['cluster'] == cluster_id\n",
    "                ].nlargest(1, 'trending_score')  # Get top video from this cluster\n",
    "                \n",
    "                for _, video in cluster_videos.iterrows():\n",
    "                    top_videos.append({\n",
    "                        'title': video['title'],\n",
    "                        'views': int(video['viewCount']),\n",
    "                        'likes': int(video['likeCount']),\n",
    "                        'trending_score': float(video['trending_score']),\n",
    "                        'predicted_growth_rate': round(metrics['growth_rate'], 4),\n",
    "                        'cluster_topics': ', '.join(metrics.get('topic_words', [])[:3])\n",
    "                    })\n",
    "                    \n",
    "                    if len(top_videos) >= limit:\n",
    "                        break\n",
    "            \n",
    "            if len(top_videos) >= limit:\n",
    "                break\n",
    "        \n",
    "        # Add ranking\n",
    "        for i, video in enumerate(top_videos[:limit], 1):\n",
    "            video['ranking'] = i\n",
    "        \n",
    "        return top_videos[:limit]\n",
    "    \n",
    "    def export_complete_analysis(self, output_file: str = None, months_ahead: int = 12) -> Dict:\n",
    "        \"\"\"\n",
    "        Export complete trend analysis to JSON\n",
    "        \n",
    "        Args:\n",
    "            output_file (str): Output JSON file path\n",
    "            months_ahead (int): Number of months to forecast for leaderboards\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Complete analysis data\n",
    "        \"\"\"\n",
    "        print(\"Generating comprehensive trend analysis export...\")\n",
    "        \n",
    "        complete_data = {\n",
    "            'export_info': {\n",
    "                'generation_timestamp': self.export_timestamp.isoformat(),\n",
    "                'export_version': '1.0.0',\n",
    "                'data_types_included': [\n",
    "                    'monthly_leaderboards',\n",
    "                    'forecast_graph_data', \n",
    "                    'raw_model_data'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Generate monthly leaderboards\n",
    "            print(\"Generating monthly trend leaderboards...\")\n",
    "            complete_data['monthly_leaderboards'] = self.generate_monthly_leaderboards(\n",
    "                months_ahead=months_ahead\n",
    "            )\n",
    "            \n",
    "            # Generate forecast graph data\n",
    "            print(\"Generating 100-day forecast graph data...\")\n",
    "            complete_data['forecast_graph_data'] = self.generate_forecast_graph_data()\n",
    "            \n",
    "            # Extract raw model data\n",
    "            print(\"Extracting comprehensive raw model data...\")\n",
    "            complete_data['raw_model_data'] = self.extract_raw_model_data()\n",
    "            \n",
    "            # Add summary statistics\n",
    "            complete_data['summary_statistics'] = {\n",
    "                'total_months_forecasted': len(complete_data['monthly_leaderboards']),\n",
    "                'total_topics_tracked': len(complete_data['forecast_graph_data']['topics']),\n",
    "                'total_clusters_analyzed': complete_data['raw_model_data']['data_summary']['total_clusters_identified'],\n",
    "                'forecast_data_points': len(complete_data['forecast_graph_data']['aggregated_daily_data']),\n",
    "                'top_growth_rate': max([\n",
    "                    topic_data['average_growth_rate'] \n",
    "                    for topic_data in complete_data['forecast_graph_data']['topics'].values()\n",
    "                ], default=0)\n",
    "            }\n",
    "            \n",
    "            # Save to file if specified\n",
    "            if output_file:\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(complete_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "                print(f\"Complete analysis exported to: {output_file}\")\n",
    "            \n",
    "            print(\"Export completed successfully!\")\n",
    "            return complete_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during export: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Modified main execution with JSON export\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the enhanced predictor with ensemble forecasting\n",
    "    predictor = EnhancedGenerationalTrendAI()\n",
    "    \n",
    "    # Define your file paths here\n",
    "    comment_files = [\n",
    "        '/kaggle/input/datathon/comments1.csv',\n",
    "        '/kaggle/input/datathon/comments2.csv',\n",
    "        '/kaggle/input/datathon/comments3.csv',\n",
    "        '/kaggle/input/datathon/comments4.csv',\n",
    "        '/kaggle/input/datathon/comments5.csv'\n",
    "    ]\n",
    "    video_file = '/kaggle/input/datathon/videos.csv'\n",
    "    forecast_horizons = [20, 30, 40, 60, 80]\n",
    "    \n",
    "    try:\n",
    "        # Run the complete analysis\n",
    "        results = predictor.run_multi_horizon_analysis(\n",
    "            comment_files, \n",
    "            video_file, \n",
    "            n_clusters=25,\n",
    "            forecast_horizons=forecast_horizons\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"GENERATING COMPREHENSIVE JSON EXPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Initialize the data exporter\n",
    "        exporter = TrendDataExporter(predictor)\n",
    "        \n",
    "        # Generate complete analysis export\n",
    "        json_output_file = f\"trend_analysis_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        complete_export_data = exporter.export_complete_analysis(\n",
    "            output_file=json_output_file,\n",
    "            months_ahead=12  # 12 months of forecasts\n",
    "        )\n",
    "        \n",
    "        # Display summary of exported data\n",
    "        print(\"\\n📊 EXPORT SUMMARY:\")\n",
    "        print(f\"📁 File saved as: {json_output_file}\")\n",
    "        print(f\"📅 Months forecasted: {complete_export_data['summary_statistics']['total_months_forecasted']}\")\n",
    "        print(f\"🎯 Topics tracked: {complete_export_data['summary_statistics']['total_topics_tracked']}\")\n",
    "        print(f\"📈 Forecast data points: {complete_export_data['summary_statistics']['forecast_data_points']}\")\n",
    "        print(f\"🏆 Top growth rate: {complete_export_data['summary_statistics']['top_growth_rate']:.4f}\")\n",
    "        \n",
    "        # Show sample of monthly leaderboard\n",
    "        print(\"\\n🏆 SAMPLE MONTHLY LEADERBOARD (First Month):\")\n",
    "        first_month = list(complete_export_data['monthly_leaderboards'].keys())[0]\n",
    "        leaderboard = complete_export_data['monthly_leaderboards'][first_month]\n",
    "        \n",
    "        print(f\"Month: {leaderboard['month']} {leaderboard['year']}\")\n",
    "        print(\"Top 5 Predicted Trends:\")\n",
    "        for topic in leaderboard['top_trending_topics'][:5]:\n",
    "            print(f\"  {topic['ranking']}. {topic['predicted_trend']} (Growth: {topic['growth_rate']:.2%})\")\n",
    "        \n",
    "        print(f\"\\nTop 5 Tags:\")\n",
    "        for tag in leaderboard['top_tags'][:5]:\n",
    "            print(f\"  {tag['ranking']}. {tag['tag']} (Growth: {tag['growth_rate']:.2%})\")\n",
    "        \n",
    "        if leaderboard['top_videos']:\n",
    "            print(f\"\\nTop Videos:\")\n",
    "            for video in leaderboard['top_videos']:\n",
    "                print(f\"  {video['ranking']}. {video['title'][:50]}...\")\n",
    "        \n",
    "        # Show sample forecast data structure\n",
    "        print(\"\\n📈 FORECAST GRAPH DATA STRUCTURE:\")\n",
    "        forecast_data = complete_export_data['forecast_graph_data']\n",
    "        print(f\"Data points per topic: {forecast_data['metadata']['data_points_per_topic']}\")\n",
    "        print(f\"First topic: {list(forecast_data['topics'].keys())[0]}\")\n",
    "        \n",
    "        sample_topic = list(forecast_data['topics'].values())[0]\n",
    "        print(f\"Sample data points (first 3 days): {sample_topic['daily_forecasts'][:3]}\")\n",
    "        \n",
    "        print(\"\\n✅ JSON export completed successfully!\")\n",
    "        print(f\"📄 The file '{json_output_file}' contains all the data you requested:\")\n",
    "        print(\"  - Monthly trend leaderboards with rankings and growth rates\")\n",
    "        print(\"  - Top 10 tags per month\")\n",
    "        print(\"  - Top 2 videos per month\")\n",
    "        print(\"  - 100-day forecast data for graphing (10-day intervals)\")\n",
    "        print(\"  - Complete raw model data for LLM analysis\")\n",
    "        \n",
    "        # Additional file with just the forecast data for easy graphing\n",
    "        forecast_only_file = f\"forecast_data_only_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(forecast_only_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(complete_export_data['forecast_graph_data'], f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        print(f\"📊 Separate forecast data file: {forecast_only_file}\")\n",
    "        \n",
    "        # Save the model as well\n",
    "        saver = ModelSaver(base_path='./saved_trend_models')\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_name = f\"enhanced_trend_ai_with_export_{timestamp}\"\n",
    "        \n",
    "        save_info = saver.save_enhanced_model(\n",
    "            predictor=predictor,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        \n",
    "        print(f\"💾 Model saved as: {model_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during analysis and export: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T17:09:28.143465Z",
     "iopub.status.busy": "2025-09-06T17:09:28.143085Z",
     "iopub.status.idle": "2025-09-06T17:09:28.158097Z",
     "shell.execute_reply": "2025-09-06T17:09:28.157171Z",
     "shell.execute_reply.started": "2025-09-06T17:09:28.143440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DateSpecificPredictor:\n",
    "    \"\"\"\n",
    "    Fixed version that works with properly loaded models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loaded_model):\n",
    "        self.predictor = loaded_model\n",
    "        \n",
    "    def predict_growth_rate_for_date(self, target_date: Union[str, datetime], \n",
    "                                   topic_keywords: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Predict growth rate for a specific date using loaded model data\n",
    "        \"\"\"\n",
    "        if isinstance(target_date, str):\n",
    "            target_date = pd.to_datetime(target_date)\n",
    "        \n",
    "        days_ahead = (target_date - pd.Timestamp.now()).days\n",
    "        \n",
    "        if days_ahead <= 0:\n",
    "            raise ValueError(\"Target date must be in the future\")\n",
    "        \n",
    "        print(f\"Predicting for {target_date.strftime('%Y-%m-%d')} ({days_ahead} days ahead)\")\n",
    "        \n",
    "        # Check if we have trend data\n",
    "        if not hasattr(self.predictor, 'combined_trend_data') or self.predictor.combined_trend_data is None:\n",
    "            return self._fallback_prediction(target_date, topic_keywords)\n",
    "        \n",
    "        # Use trend data to make predictions\n",
    "        return self._trend_based_prediction(target_date, days_ahead, topic_keywords)\n",
    "    \n",
    "    def _trend_based_prediction(self, target_date, days_ahead, topic_keywords=None):\n",
    "        \"\"\"\n",
    "        Make predictions based on historical trend data\n",
    "        \"\"\"\n",
    "        trend_data = self.predictor.combined_trend_data\n",
    "        \n",
    "        results = {\n",
    "            'target_date': target_date.strftime('%Y-%m-%d'),\n",
    "            'days_ahead': days_ahead,\n",
    "            'predictions': {},\n",
    "            'method': 'trend_extrapolation'\n",
    "        }\n",
    "        \n",
    "        # Get unique clusters\n",
    "        clusters = trend_data['cluster'].unique()\n",
    "        \n",
    "        for cluster_id in clusters:\n",
    "            cluster_data = trend_data[trend_data['cluster'] == cluster_id].copy()\n",
    "            \n",
    "            if len(cluster_data) < 3:  # Need minimum data points\n",
    "                continue\n",
    "            \n",
    "            # Sort by date\n",
    "            cluster_data = cluster_data.sort_values('date')\n",
    "            \n",
    "            # Calculate trend\n",
    "            recent_scores = cluster_data['combined_trending_score'].tail(5).values\n",
    "            if len(recent_scores) >= 2:\n",
    "                # Simple linear projection\n",
    "                trend_slope = np.polyfit(range(len(recent_scores)), recent_scores, 1)[0]\n",
    "                current_score = recent_scores[-1]\n",
    "                predicted_score = current_score + (trend_slope * (days_ahead / 7))  # Weekly projection\n",
    "                \n",
    "                # Calculate growth rate\n",
    "                growth_rate = trend_slope / max(current_score, 0.001)  # Avoid division by zero\n",
    "                \n",
    "                # Get topic information\n",
    "                topic_words = self.predictor.cluster_topics.get(cluster_id, [])\n",
    "                topic_tags = self.predictor.cluster_tags.get(cluster_id, [])\n",
    "                \n",
    "                # Filter by keywords if provided\n",
    "                if topic_keywords:\n",
    "                    topic_text = ' '.join(topic_words + topic_tags).lower()\n",
    "                    if not any(keyword.lower() in topic_text for keyword in topic_keywords):\n",
    "                        continue\n",
    "                \n",
    "                topic_name = ', '.join(topic_words[:3])\n",
    "                \n",
    "                results['predictions'][topic_name] = {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'predicted_score': float(predicted_score),\n",
    "                    'growth_rate': float(growth_rate),\n",
    "                    'trend_slope': float(trend_slope),\n",
    "                    'current_score': float(current_score),\n",
    "                    'topic_words': topic_words[:5],\n",
    "                    'topic_tags': topic_tags[:5],\n",
    "                    'confidence': 'medium'\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _fallback_prediction(self, target_date, topic_keywords=None):\n",
    "        \"\"\"\n",
    "        Fallback prediction using cluster performance data\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'target_date': target_date.strftime('%Y-%m-%d'),\n",
    "            'predictions': {},\n",
    "            'method': 'cluster_analysis_fallback',\n",
    "            'warning': 'Limited data - using cluster-based estimation'\n",
    "        }\n",
    "        \n",
    "        if not hasattr(self.predictor, 'cluster_topics'):\n",
    "            results['error'] = 'No cluster data available'\n",
    "            return results\n",
    "        \n",
    "        # Use cluster popularity and tag performance as proxy\n",
    "        for cluster_id, topic_words in self.predictor.cluster_topics.items():\n",
    "            if topic_keywords:\n",
    "                topic_text = ' '.join(topic_words).lower()\n",
    "                if not any(keyword.lower() in topic_text for keyword in topic_keywords):\n",
    "                    continue\n",
    "            \n",
    "            # Estimate growth based on cluster size and tag popularity\n",
    "            cluster_size = 0\n",
    "            if hasattr(self.predictor, 'comments_df') and self.predictor.comments_df is not None:\n",
    "                cluster_size = len(self.predictor.comments_df[self.predictor.comments_df['cluster'] == cluster_id])\n",
    "            \n",
    "            # Simple heuristic: larger clusters with popular tags have higher growth potential\n",
    "            base_growth = min(cluster_size / 1000, 0.1)  # Cap at 10%\n",
    "            \n",
    "            topic_name = ', '.join(topic_words[:3])\n",
    "            results['predictions'][topic_name] = {\n",
    "                'cluster_id': cluster_id,\n",
    "                'estimated_growth_rate': float(base_growth),\n",
    "                'topic_words': topic_words[:5],\n",
    "                'cluster_size': cluster_size,\n",
    "                'confidence': 'low'\n",
    "            }\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T17:09:29.448581Z",
     "iopub.status.busy": "2025-09-06T17:09:29.447924Z",
     "iopub.status.idle": "2025-09-06T17:09:31.652282Z",
     "shell.execute_reply": "2025-09-06T17:09:31.651338Z",
     "shell.execute_reply.started": "2025-09-06T17:09:29.448554Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced model saved successfully to: ./saved_trend_models/enhanced_trend_ai_with_export_20250906_170929\n"
     ]
    }
   ],
   "source": [
    "saver = ModelSaver(base_path='./saved_trend_models')\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"enhanced_trend_ai_with_export_{timestamp}\"\n",
    "\n",
    "save_info = saver.save_enhanced_model(\n",
    "    predictor=predictor,\n",
    "    model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T17:12:01.814093Z",
     "iopub.status.busy": "2025-09-06T17:12:01.813771Z",
     "iopub.status.idle": "2025-09-06T17:12:15.626353Z",
     "shell.execute_reply": "2025-09-06T17:12:15.625471Z",
     "shell.execute_reply.started": "2025-09-06T17:12:01.814072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Enhanced YouTube Trend Predictor with Tags Integration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded comments DataFrame\n",
      "Loaded videos DataFrame\n",
      "Loaded combined trend data\n",
      "Loaded model components: ['embeddings', 'clusters', 'cluster_topics', 'cluster_tags', 'popular_tags', 'generational_clusters', 'scaler', 'video_weight', 'comment_weight', 'tag_weight']\n",
      "Ensemble forecaster reinitialized\n",
      "Fixing loaded model by regenerating trend data...\n",
      "Predicting for 2025-12-25 (109 days ahead)\n"
     ]
    }
   ],
   "source": [
    "# Load your saved model\n",
    "saver = ModelSaver('./saved_trend_models')\n",
    "loaded_predictor = saver.load_enhanced_model('/kaggle/working/saved_trend_models/enhanced_trend_ai_with_export_20250906_165800')\n",
    "\n",
    "# To fix your current loaded model and enable predictions:\n",
    "def fix_loaded_model(loaded_predictor, original_data_files):\n",
    "    \"\"\"\n",
    "    Fix a loaded model by regenerating missing trend data\n",
    "    \"\"\"\n",
    "    print(\"Fixing loaded model by regenerating trend data...\")\n",
    "    \n",
    "    # If trend data is missing, regenerate it\n",
    "    if not hasattr(loaded_predictor, 'combined_trend_data') or loaded_predictor.combined_trend_data is None:\n",
    "        if hasattr(loaded_predictor, 'comments_df') and hasattr(loaded_predictor, 'videos_df'):\n",
    "            print(\"Regenerating trend data from existing DataFrames...\")\n",
    "            loaded_predictor.prepare_time_series_data()\n",
    "        else:\n",
    "            print(\"Need to reload original data...\")\n",
    "            # Reload original data if needed\n",
    "            comment_files, video_file = original_data_files\n",
    "            loaded_predictor.load_data(comment_files, video_file)\n",
    "            loaded_predictor.preprocess_data()\n",
    "            loaded_predictor.prepare_time_series_data()\n",
    "    \n",
    "    # Reinitialize ensemble forecaster\n",
    "    loaded_predictor.ensemble_forecaster = EnsembleForecaster()\n",
    "    \n",
    "    return loaded_predictor\n",
    "\n",
    "# Usage:\n",
    "comment_files = [\n",
    "    '/kaggle/input/datathon/comments1.csv',\n",
    "    '/kaggle/input/datathon/comments2.csv',\n",
    "    '/kaggle/input/datathon/comments3.csv',\n",
    "    '/kaggle/input/datathon/comments4.csv',\n",
    "    '/kaggle/input/datathon/comments5.csv'\n",
    "]\n",
    "video_file = '/kaggle/input/datathon/videos.csv'\n",
    "original_files = (comment_files, video_file)  # Your original data files\n",
    "fixed_predictor = fix_loaded_model(loaded_predictor, original_files)\n",
    "\n",
    "# Now you can use date-specific predictions\n",
    "date_predictor = DateSpecificPredictor(fixed_predictor)\n",
    "christmas_prediction = date_predictor.predict_growth_rate_for_date('2025-12-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T17:12:28.921586Z",
     "iopub.status.busy": "2025-09-06T17:12:28.920969Z",
     "iopub.status.idle": "2025-09-06T17:12:28.926418Z",
     "shell.execute_reply": "2025-09-06T17:12:28.925555Z",
     "shell.execute_reply.started": "2025-09-06T17:12:28.921561Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_date': '2025-12-25', 'days_ahead': 109, 'predictions': {'hair, hairstyle, wig': {'cluster_id': 4, 'predicted_score': -0.047014411231051864, 'growth_rate': -3.0192741157556244, 'trend_slope': -0.0030192741157556244, 'current_score': 0.0, 'topic_words': ['hair', 'hairstyle', 'wig', 'beautiful', 'bald'], 'topic_tags': ['hair', 'hair transformation', 'hairstyle', 'haircut', 'shorts'], 'confidence': 'medium'}, 'face, try, why': {'cluster_id': 10, 'predicted_score': -0.08283105132810757, 'growth_rate': -5.31942531464911, 'trend_slope': -0.005319425314649111, 'current_score': 0.0, 'topic_words': ['face', 'try', 'why', 'products', 'dont'], 'topic_tags': ['shorts', 'womens health', 'discharge', 'obgyn', 'menopause'], 'confidence': 'medium'}, 'makeup, beauty, beautiful': {'cluster_id': 16, 'predicted_score': 0.3409462411575563, 'growth_rate': 0.2670161583455357, 'trend_slope': 0.017650500000000003, 'current_score': 0.06610274115755627, 'topic_words': ['makeup', 'beauty', 'beautiful', 'without', 'tutorial'], 'topic_tags': ['makeup', 'makeup tutorial', 'shorts', 'beauty', 'lipstick'], 'confidence': 'medium'}, 'skin, face, skincare': {'cluster_id': 18, 'predicted_score': -0.044224359209921876, 'growth_rate': -2.840096463022506, 'trend_slope': -0.002840096463022506, 'current_score': 0.0, 'topic_words': ['skin', 'face', 'skincare', 'beauty', 'makeup'], 'topic_tags': ['skincare', 'shorts', 'beauty', 'makeup', 'skin care'], 'confidence': 'medium'}, 'hai, makeup, aap': {'cluster_id': 3, 'predicted_score': 9.632258416300527e-21, 'growth_rate': 6.185854028816852e-19, 'trend_slope': 6.185854028816853e-22, 'current_score': 0.0, 'topic_words': ['hai', 'makeup', 'aap', 'bhi', 'kya'], 'topic_tags': ['shorts', 'makeup', 'vlog', 'tips', 'fyp'], 'confidence': 'medium'}, 'ich, cantik, makeup': {'cluster_id': 0, 'predicted_score': 0.13331565342214052, 'growth_rate': 0.21934319619103096, 'trend_slope': 0.0066225723472668795, 'current_score': 0.030192741157556265, 'topic_words': ['ich', 'cantik', 'makeup', 'ist', 'das'], 'topic_tags': ['shorts', 'makeup', 'tiktok', 'short', 'trending'], 'confidence': 'medium'}, 'shorts, makeup, beauty': {'cluster_id': 1, 'predicted_score': -7.823265962332504e-05, 'growth_rate': -0.005024115755626378, 'trend_slope': -5.024115755626379e-06, 'current_score': 0.0, 'topic_words': ['shorts', 'makeup', 'beauty', 'hair', 'wear'], 'topic_tags': ['shorts', 'makeup', 'viral', 'beauty', 'youtube shorts'], 'confidence': 'medium'}, 'pls, link, plz': {'cluster_id': 5, 'predicted_score': -0.05871076167375444, 'growth_rate': -3.7704158873053313, 'trend_slope': -0.0037704158873053313, 'current_score': 0.0, 'topic_words': ['pls', 'link', 'plz', 'need', 'name'], 'topic_tags': ['shorts', 'beauty', 'makeup', 'fashion', 'style'], 'confidence': 'medium'}, 'people, why, dont': {'cluster_id': 9, 'predicted_score': -0.0004693959577400091, 'growth_rate': -0.03014469453376205, 'trend_slope': -3.0144694533762053e-05, 'current_score': 0.0, 'topic_words': ['people', 'why', 'dont', 'don', 'about'], 'topic_tags': ['shorts', 'viral', 'motivation', 'makeup', 'short'], 'confidence': 'medium'}, 'videos, share, amazing': {'cluster_id': 15, 'predicted_score': 0.03869899381861199, 'growth_rate': -0.019043338853746632, 'trend_slope': -0.0010476070594082917, 'current_score': 0.055011732315112534, 'topic_words': ['videos', 'share', 'amazing', 'beautiful', 'dear'], 'topic_tags': ['beauty', 'makeup', 'inshot', 'fashion', 'trending'], 'confidence': 'medium'}, 'color, hair, brown': {'cluster_id': 17, 'predicted_score': 0.02997100746440054, 'growth_rate': -0.023196392673981145, 'trend_slope': -0.001088322347266882, 'current_score': 0.04691774115755627, 'topic_words': ['color', 'hair', 'brown', 'black', 'colour'], 'topic_tags': ['hair color', 'hair', 'hair dye', 'shorts', 'makeup'], 'confidence': 'medium'}, 'girl, wow, nice': {'cluster_id': 23, 'predicted_score': -0.07388917581534217, 'growth_rate': -4.751629421221862, 'trend_slope': -0.004751629421221862, 'current_score': 0.00010048231511254019, 'topic_words': ['girl', 'wow', 'nice', 'bro', 'cute'], 'topic_tags': ['shorts', 'makeup', 'beauty', 'lovers', 'beauty lovers'], 'confidence': 'medium'}, 'name, girl, cute': {'cluster_id': 13, 'predicted_score': -0.06865668167202572, 'growth_rate': -4.4091446945337625, 'trend_slope': -0.004409144694533762, 'current_score': 0.0, 'topic_words': ['name', 'girl', 'cute', 'omg', 'bro'], 'topic_tags': ['shorts', 'tiktok', 'makeup', 'tik tok', 'viral'], 'confidence': 'medium'}, 'beautiful, pretty, gorgeous': {'cluster_id': 21, 'predicted_score': 0.12816357889297203, 'growth_rate': 0.09970431404669185, 'trend_slope': 0.0050061776527331224, 'current_score': 0.05021024115755627, 'topic_words': ['beautiful', 'pretty', 'gorgeous', 'cute', 'wow'], 'topic_tags': [], 'confidence': 'medium'}, 'beautiful, looks, night': {'cluster_id': 2, 'predicted_score': 7.823265962335957e-05, 'growth_rate': 0.005024115755628596, 'trend_slope': 5.024115755628596e-06, 'current_score': 0.0, 'topic_words': ['beautiful', 'looks', 'night', 'pretty', 'nice'], 'topic_tags': ['inshot', 'fragrance', 'perfume', 'makeup', 'beauty'], 'confidence': 'medium'}, 'india, beautiful, pakistan': {'cluster_id': 7, 'predicted_score': 9.632258416300527e-21, 'growth_rate': 6.185854028816852e-19, 'trend_slope': 6.185854028816853e-22, 'current_score': 0.0, 'topic_words': ['india', 'beautiful', 'pakistan', 'bts', 'wow'], 'topic_tags': [], 'confidence': 'medium'}, 'song, best, music': {'cluster_id': 11, 'predicted_score': 0.018646034680753328, 'growth_rate': 1.197451768488746, 'trend_slope': 0.001197451768488746, 'current_score': 0.0, 'topic_words': ['song', 'best', 'music', 'favorite', 'name'], 'topic_tags': ['shorts', 'short', 'youtubeshorts', 'love', 'trending'], 'confidence': 'medium'}, 'russian, russia, makeup': {'cluster_id': 12, 'predicted_score': 0.00020670647680293982, 'growth_rate': 0.010048231511254018, 'trend_slope': 1.0048231511254018e-05, 'current_score': 5.0241157556270096e-05, 'topic_words': ['russian', 'russia', 'makeup', 'arabic', 'beautiful'], 'topic_tags': ['косметика', 'уход за волосами', 'уход', 'mone professional', 'средства для волос'], 'confidence': 'medium'}, 'women, men, girl': {'cluster_id': 14, 'predicted_score': -0.00015646531924667968, 'growth_rate': -0.010048231511254658, 'trend_slope': -1.0048231511254658e-05, 'current_score': 0.0, 'topic_words': ['women', 'men', 'girl', 'girls', 'woman'], 'topic_tags': ['dating', 'women', 'dating tips', 'dating advice for men', 'dating advice'], 'confidence': 'medium'}, 'india, indian, best': {'cluster_id': 20, 'predicted_score': 0.0, 'growth_rate': 0.0, 'trend_slope': 0.0, 'current_score': 0.0, 'topic_words': ['india', 'indian', 'best', 'pakistan', 'beautiful'], 'topic_tags': ['makeup', 'makeup challenge', 'makeup shorts', 'shorts', 'makeup tutorial'], 'confidence': 'medium'}, 'beautiful, pretty, dont': {'cluster_id': 22, 'predicted_score': -0.09871470501953465, 'growth_rate': -6.339476469144428, 'trend_slope': -0.0063394764691444274, 'current_score': 0.0, 'topic_words': ['beautiful', 'pretty', 'dont', 'girl', 'face'], 'topic_tags': ['beauty', 'how to be more attractive', 'how to look more attractive', 'how to be attractive', 'skincare'], 'confidence': 'medium'}, 'girl, looks, shes': {'cluster_id': 24, 'predicted_score': 5.0076156691923515e-05, 'growth_rate': -1.059638578372552e-05, 'trend_slope': -1.0596385783725521e-08, 'current_score': 5.0241157556270096e-05, 'topic_words': ['girl', 'looks', 'shes', 'why', 'hair'], 'topic_tags': ['shorts', 'rihanna', 'makeup', 'funny', 'beauty'], 'confidence': 'medium'}, 'que, linda, para': {'cluster_id': 6, 'predicted_score': -7.823265962333481e-05, 'growth_rate': -0.005024115755627006, 'trend_slope': -5.024115755627006e-06, 'current_score': 0.0, 'topic_words': ['que', 'linda', 'para', 'por', 'con'], 'topic_tags': ['beleza', 'maquillaje', 'makeup', 'avon', 'tutorial de maquillaje'], 'confidence': 'medium'}, 'beautiful, looks, pretty': {'cluster_id': 8, 'predicted_score': -0.06834651734037664, 'growth_rate': -4.389225884244372, 'trend_slope': -0.004389225884244371, 'current_score': 0.0, 'topic_words': ['beautiful', 'looks', 'pretty', 'shes', 'gorgeous'], 'topic_tags': ['selena gomez', 'rare beauty', 'shorts', 'beauty', 'anushkasen'], 'confidence': 'medium'}, 'hair, curly, curls': {'cluster_id': 19, 'predicted_score': 0.0, 'growth_rate': 0.0, 'trend_slope': 0.0, 'current_score': 0.0, 'topic_words': ['hair', 'curly', 'curls', 'straight', 'wavy'], 'topic_tags': ['curly hair', 'hair', 'natural hair', 'curls', 'shorts'], 'confidence': 'medium'}}, 'method': 'trend_extrapolation'}\n"
     ]
    }
   ],
   "source": [
    "print(christmas_prediction)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8205725,
     "sourceId": 12965486,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
